{"text": "<fim_prefix>        final LogContext logContext = new LogContext(logPrefix);\n        this.log = logContext.logger(getClass());\n        this.adminClient = adminClient;\n    }\n    void createTasks(final Collection<TopicPartition> assignment) {\n        if (consumer == null) {\n            throw new IllegalStateException(logPrefix + \"consumer has not been initialized while adding stream tasks. This should not happen.\");\n        }\n        // do this first as we may have suspended standby tasks that\n        // will become active or vice versa\n        standby.closeNonAssignedSuspendedTasks(assignedStandbyTasks);\n        active.closeNonAssignedSuspendedTasks(assignedActiveTasks);\n        addStreamTasks(assignment);\n        addStandbyTasks();\n        // Pause all the partitions until the underlying state store is ready for all the active tasks.\n        log.trace(\"Pausing partitions: {}\", assignment);\n        consumer.pause(assignment);\n    }\n    private void addStreamTasks(final Collection<TopicPartition> assignment) {\n        if (assignedActiveTasks.isEmpty()) {\n            return;\n        }\n        final Map<TaskId, Set<TopicPartition>> newTasks = new HashMap<>();\n        // collect newly assigned tasks and reopen re-assigned tasks\n        log.debug(\"Adding assigned tasks as active: {}\", assignedActiveTasks);\n        for (final Map.Entry<TaskId, Set<TopicPartition>> entry : assignedActiveTasks.entrySet()) {\n            final TaskId taskId = entry.getKey();\n            final Set<TopicPartition> partitions = entry.getValue();\n            if (assignment.containsAll(partitions)) {\n                try {\n                    if (!active.maybeResumeSuspendedTask(taskId, partitions)) {\n                        newTasks.put(taskId, partitions);\n                    }\n                } catch (final StreamsException e) {\n                    log.error(\"Failed to resume an active task {} due to the following error:\", taskId, e);\n                    throw e;\n                }\n            } else {\n                log.warn(\"Task {} owned partitions {} are not contained in the assignment {}\", taskId, partitions, assignment);\n            }\n        }\n        if (newTasks.isEmpty()) {\n            return;\n        }\n        // CANNOT FIND RETRY AND BACKOFF LOGIC\n        // create all newly assigned tasks (guard against race condition with other thread via backoff and retry)\n        // -> other thread will call removeSuspendedTasks(); eventually\n        log.trace(\"New active tasks to be created: {}\", newTasks);\n        for (final StreamTask task : taskCreator.createTasks(consumer, newTasks)) {\n            active.addNewTask(task);\n        }\n    }\n    private void addStandbyTasks() {\n        final Map<TaskId, Set<TopicPartition>> assignedStandbyTasks = this.assignedStandbyTasks;\n        if (assignedStandbyTasks.isEmpty()) {\n            return;\n        }\n        log.debug(\"Adding assigned standby tasks {}\", assignedStandbyTasks);\n        final Map<TaskId, Set<TopicPartition>> newStandbyTasks = new HashMap<>();\n        // collect newly assigned standby tasks and reopen re-assigned standby tasks\n        for (final Map.Entry<TaskId, Set<TopicPartition>> entry : assignedStandbyTasks.entrySet()) {\n            final TaskId taskId = entry.getKey();\n            final Set<TopicPartition> partitions = entry.getValue();\n            if (!standby.maybeResumeSuspendedTask(taskId, partitions)) {\n                newStandbyTasks.put(taskId, partitions);\n            }\n        }\n        if (newStandbyTasks.isEmpty()) {\n            return;\n        }\n        // create all newly assigned standby tasks (guard against race condition with other thread via backoff and retry)\n        // -> other thread will call removeSuspendedStandbyTasks(); eventually\n        log.trace(\"New standby tasks to be created: {}\", newStandbyTasks);\n        for (final StandbyTask task : standbyTaskCreator.createTasks(consumer, newStandbyTasks)) {\n            standby.addNewTask(task);\n        }\n    }\n    Set<TaskId> activeTaskIds() {\n        return active.allAssignedTaskIds();\n    }\n    Set<TaskId> standbyTaskIds() {\n        return standby.allAssignedTaskIds();\n    }\n    public Set<TaskId> prevActiveTaskIds() {\n        return active.previousTaskIds();\n    }\n    /**\n     * Returns ids of tasks whose states are kept on the local storage.\n     */\n    public Set<TaskId> cachedTasksIds() {\n        // A client could contain some inactive tasks whose states are still kept on the local storage in the following scenarios:\n        // 1) the client is actively maintaining standby tasks by maintaining their states from the change log.\n        // 2) the client has just got some tasks migrated out of itself to other clients while these task states\n        //    have not been cleaned up yet (this can happen in a rolling bounce upgrade, for example).\n        final HashSet<TaskId> tasks = new HashSet<>();\n        final File[] stateDirs = taskCreator.stateDirectory().listTaskDirectories();\n        if (stateDirs != null) {\n            for (final File dir : stateDirs) {\n                try {\n                    final TaskId id = TaskId.parse(dir.getName());\n                    // if the checkpoint file exists, the state is valid.\n                    if (new File(dir, ProcessorStateManager.CHECKPOINT_FILE_NAME).exists()) {\n                        tasks.add(id);\n                    }\n                } catch (final TaskIdFormatException e) {\n                    // there may be some unknown files that sits in the same directory,\n                    // we should ignore these files instead trying to delete them as well\n                }\n            }\n        }\n        return tasks;\n    }\n    public UUID processId() {\n        return processId;\n    }\n    InternalTopologyBuilder builder() {\n        return taskCreator.builder();\n    }\n    /**\n     * Similar to shutdownTasksAndState, however does not close the task managers, in the hope that\n     * soon the tasks will be assigned again\n     * @throws TaskMigratedException if the task producer got fenced (EOS only)\n     */\n    void suspendTasksAndState()  {\n        log.debug(\"Suspending all active tasks {} and standby tasks {}\", active.runningTaskIds(), standby.runningTaskIds());\n        final AtomicReference<RuntimeException> firstException = new AtomicReference<>(null);\n        firstException.compareAndSet(null, active.suspend());\n        // close all restoring tasks as well and then reset changelog reader;\n        // for those restoring and still assigned tasks, they will be re-created\n        // in addStreamTasks.\n        firstException.compareAndSet(null, active.closeAllRestoringTasks());\n        changelogReader.reset();\n        firstException.compareAndSet(null, standby.suspend());\n        // remove the changelog partitions from restore consumer\n        restoreConsumer.unsubscribe();\n        final Exception exception = firstException.get();\n        if (exception != null) {\n            throw new StreamsException(logPrefix + \"failed to suspend stream tasks\", exception);\n        }\n    }\n    void shutdown(final boolean clean) {\n        final AtomicReference<RuntimeException> firstException = new AtomicReference<>(null);\n        log.debug(\"Shutting down all active tasks {}, standby tasks {}, suspended tasks {}, and suspended standby tasks {}\", active.runningTaskIds(), standby.runningTaskIds(),\n                  active.previousTaskIds(), standby.previousTaskIds());\n        try {\n            active.close(clean);\n        } catch (final RuntimeException fatalException) {\n            firstException.compareAndSet(null, fatalException);\n        }\n        standby.close(clean);\n        // remove the changelog partitions from restore consumer\n        try {\n            restoreConsumer.unsubscribe();\n        } catch (final RuntimeException fatalException) {\n            firstException.compareAndSet(null, fatalException);\n        }\n        taskCreator.close();\n        standbyTaskCreator.close();\n        final RuntimeException fatalException = firstException.get();\n        if (fatalException != null) {\n            throw fatalException;\n        }\n    }\n    AdminClient getAdminClient() {\n        return adminClient;\n    }\n    Set<TaskId> suspendedActiveTaskIds() {\n        return active.previousTaskIds();\n    }\n    Set<TaskId> suspendedStandbyTaskIds() {\n        return standby.previousTaskIds();\n    }\n    StreamTask activeTask(final TopicPartition partition) {\n        return active.runningTaskFor(partition);\n    }\n    StandbyTask standbyTask(final TopicPartition partition) {\n        return standby.runningTaskFor(partition);\n    }\n    Map<TaskId, StreamTask> activeTasks() {\n        return active.runningTaskMap();\n    }\n    Map<TaskId, StandbyTask> standbyTasks() {\n        return standby.runningTaskMap();\n    }\n    void setConsumer(final Consumer<byte[], byte[]> consumer) {\n        this.consumer = consumer;\n    }\n    /**\n     * @throws IllegalStateException If store gets registered after initialized is already finished\n     * @throws StreamsException if the store's change log does not contain the partition\n     */\n<fim_suffix>    boolean updateNewAndRestoringTasks() {\n        active.initializeNewTasks();\n        standby.initializeNewTasks();\n        final Collection<TopicPartition> restored = changelogReader.restore(active);\n        active.updateRestored(restored);\n        if (active.allTasksRunning()) {\n            final Set<TopicPartition> assignment = consumer.assignment();\n            log.trace(\"Resuming partitions {}\", assignment);\n            consumer.resume(assignment);\n            assignStandbyPartitions();\n            return true;\n        }\n        return false;\n    }<fim_middle>// function below has no smell\n"}