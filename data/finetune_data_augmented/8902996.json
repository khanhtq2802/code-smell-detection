{"text": "<fim_prefix>/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.hadoop.hive.ql.optimizer.spark;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.hive.conf.HiveConf;\nimport org.apache.hadoop.hive.ql.Context;\nimport org.apache.hadoop.hive.ql.exec.CommonJoinOperator;\nimport org.apache.hadoop.hive.ql.exec.JoinOperator;\nimport org.apache.hadoop.hive.ql.exec.MapJoinOperator;\nimport org.apache.hadoop.hive.ql.exec.Operator;\nimport org.apache.hadoop.hive.ql.exec.OperatorUtils;\nimport org.apache.hadoop.hive.ql.exec.ReduceSinkOperator;\nimport org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator;\nimport org.apache.hadoop.hive.ql.exec.TableScanOperator;\nimport org.apache.hadoop.hive.ql.exec.Task;\nimport org.apache.hadoop.hive.ql.exec.TaskFactory;\nimport org.apache.hadoop.hive.ql.exec.spark.SparkTask;\nimport org.apache.hadoop.hive.ql.lib.Node;\nimport org.apache.hadoop.hive.ql.lib.NodeProcessor;\nimport org.apache.hadoop.hive.ql.lib.NodeProcessorCtx;\nimport org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils;\nimport org.apache.hadoop.hive.ql.optimizer.physical.GenMRSkewJoinProcessor;\nimport org.apache.hadoop.hive.ql.optimizer.physical.GenSparkSkewJoinProcessor;\nimport org.apache.hadoop.hive.ql.optimizer.physical.SkewJoinProcFactory;\nimport org.apache.hadoop.hive.ql.parse.ParseContext;\nimport org.apache.hadoop.hive.ql.parse.SemanticException;\nimport org.apache.hadoop.hive.ql.parse.spark.GenSparkUtils;\nimport org.apache.hadoop.hive.ql.plan.BaseWork;\nimport org.apache.hadoop.hive.ql.plan.MapWork;\nimport org.apache.hadoop.hive.ql.plan.OperatorDesc;\nimport org.apache.hadoop.hive.ql.plan.PlanUtils;\nimport org.apache.hadoop.hive.ql.plan.ReduceWork;\nimport org.apache.hadoop.hive.ql.plan.SparkEdgeProperty;\nimport org.apache.hadoop.hive.ql.plan.SparkWork;\nimport org.apache.hadoop.hive.ql.plan.TableDesc;\nimport java.io.Serializable;\nimport java.util.HashSet;\nimport java.util.List;\nimport java.util.Set;\nimport java.util.Stack;\n/**\n * Spark-version of SkewJoinProcFactory.\n */\npublic class SparkSkewJoinProcFactory {\n  // let's remember the join operators we have processed\n  private static final Set<JoinOperator> visitedJoinOp = new HashSet<JoinOperator>();\n<fim_suffix>  private SparkSkewJoinProcFactory() {\n    // prevent instantiation\n  }\n  public static NodeProcessor getDefaultProc() {\n    return SkewJoinProcFactory.getDefaultProc();\n  }\n  public static NodeProcessor getJoinProc() {\n    return new SparkSkewJoinJoinProcessor();\n  }\n  public static class SparkSkewJoinJoinProcessor implements NodeProcessor {\n    @Override\n    public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n        Object... nodeOutputs) throws SemanticException {\n      SparkSkewJoinResolver.SparkSkewJoinProcCtx context =\n          (SparkSkewJoinResolver.SparkSkewJoinProcCtx) procCtx;\n      Task<? extends Serializable> currentTsk = context.getCurrentTask();\n      JoinOperator op = (JoinOperator) nd;\n      ReduceWork reduceWork = context.getReducerToReduceWork().get(op);\n      ParseContext parseContext = context.getParseCtx();\n      if (reduceWork != null && !visitedJoinOp.contains(op) &&\n          supportRuntimeSkewJoin(op, reduceWork, currentTsk, parseContext.getConf())) {\n        // first we try to split the task\n        splitTask((SparkTask) currentTsk, reduceWork, parseContext);\n        GenSparkSkewJoinProcessor.processSkewJoin(op, currentTsk, reduceWork, parseContext);\n        visitedJoinOp.add(op);\n      }\n      return null;\n    }\n  }\n  /**\n   * If the join is not in a leaf ReduceWork, the spark task has to be split into 2 tasks.\n   */\n  private static void splitTask(SparkTask currentTask, ReduceWork reduceWork,\n      ParseContext parseContext) throws SemanticException {\n    SparkWork currentWork = currentTask.getWork();\n    Set<Operator<?>> reduceSinkSet =\n        OperatorUtils.getOp(reduceWork, ReduceSinkOperator.class);\n    if (currentWork.getChildren(reduceWork).size() == 1 && canSplit(currentWork)\n      && reduceSinkSet.size() == 1) {\n      ReduceSinkOperator reduceSink = (ReduceSinkOperator) reduceSinkSet.iterator().next();\n      BaseWork childWork = currentWork.getChildren(reduceWork).get(0);\n      SparkEdgeProperty originEdge = currentWork.getEdgeProperty(reduceWork, childWork);\n      // disconnect the reduce work from its child. this should produce two isolated sub graphs\n      currentWork.disconnect(reduceWork, childWork);\n      // move works following the current reduce work into a new spark work\n      SparkWork newWork =\n          new SparkWork(parseContext.getConf().getVar(HiveConf.ConfVars.HIVEQUERYID));\n      newWork.add(childWork);\n      copyWorkGraph(currentWork, newWork, childWork);\n      // remove them from current spark work\n      for (BaseWork baseWork : newWork.getAllWorkUnsorted()) {\n        currentWork.remove(baseWork);\n        currentWork.getCloneToWork().remove(baseWork);\n      }\n      // create TS to read intermediate data\n      Context baseCtx = parseContext.getContext();\n      Path taskTmpDir = baseCtx.getMRTmpPath();\n      Operator<? extends OperatorDesc> rsParent = reduceSink.getParentOperators().get(0);\n      TableDesc tableDesc = PlanUtils.getIntermediateFileTableDesc(PlanUtils\n          .getFieldSchemasFromRowSchema(rsParent.getSchema(), \"temporarycol\"));\n      // this will insert FS and TS between the RS and its parent\n      TableScanOperator tableScanOp = GenMapRedUtils.createTemporaryFile(\n          rsParent, reduceSink, taskTmpDir, tableDesc, parseContext);\n      // create new MapWork\n      MapWork mapWork = PlanUtils.getMapRedWork().getMapWork();\n      mapWork.setName(\"Map \" + GenSparkUtils.getUtils().getNextSeqNumber());\n      newWork.add(mapWork);\n      newWork.connect(mapWork, childWork, originEdge);\n      // setup the new map work\n      String streamDesc = taskTmpDir.toUri().toString();\n      if (GenMapRedUtils.needsTagging((ReduceWork) childWork)) {\n        Operator<? extends OperatorDesc> childReducer = ((ReduceWork) childWork).getReducer();\n        String id = null;\n        if (childReducer instanceof JoinOperator) {\n          if (parseContext.getJoinOps().contains(childReducer)) {\n            id = ((JoinOperator)childReducer).getConf().getId();\n          }\n        } else if (childReducer instanceof MapJoinOperator) {\n          if (parseContext.getMapJoinOps().contains(childReducer)) {\n            id = ((MapJoinOperator)childReducer).getConf().getId();\n          }\n        } else if (childReducer instanceof SMBMapJoinOperator) {\n          if (parseContext.getSmbMapJoinOps().contains(childReducer)) {\n            id = ((SMBMapJoinOperator)childReducer).getConf().getId();\n          }\n        }\n        if (id != null) {\n          streamDesc = id + \":$INTNAME\";\n        } else {\n          streamDesc = \"$INTNAME\";\n        }\n        String origStreamDesc = streamDesc;\n        int pos = 0;\n        while (mapWork.getAliasToWork().get(streamDesc) != null) {\n          streamDesc = origStreamDesc.concat(String.valueOf(++pos));\n        }\n      }\n      GenMapRedUtils.setTaskPlan(taskTmpDir, streamDesc,\n          tableScanOp, mapWork, false, tableDesc);<fim_middle>// function below has no smell\n"}