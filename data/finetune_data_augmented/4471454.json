{"text": "<fim_prefix>/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.drill.exec.physical.impl.spill;\nimport java.io.BufferedInputStream;\nimport java.io.File;\nimport java.io.FileInputStream;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.io.OutputStream;\nimport java.nio.ByteBuffer;\nimport java.nio.channels.FileChannel;\nimport java.nio.channels.WritableByteChannel;\nimport java.nio.file.StandardOpenOption;\nimport java.util.Iterator;\nimport java.util.List;\nimport java.util.Set;\nimport org.apache.drill.common.config.DrillConfig;\nimport org.apache.drill.common.exceptions.UserException;\nimport org.apache.drill.exec.ExecConstants;\nimport org.apache.drill.exec.cache.VectorSerializer;\nimport org.apache.drill.exec.ops.FragmentContext;\nimport org.apache.drill.exec.physical.base.PhysicalOperator;\nimport org.apache.drill.exec.physical.config.HashAggregate;\nimport org.apache.drill.exec.physical.config.HashJoinPOP;\nimport org.apache.drill.exec.physical.config.Sort;\nimport org.apache.drill.exec.proto.ExecProtos.FragmentHandle;\nimport org.apache.drill.exec.proto.helper.QueryIdHelper;\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.FSDataInputStream;\nimport org.apache.hadoop.fs.FSDataOutputStream;\nimport org.apache.hadoop.fs.FileSystem;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.drill.shaded.guava.com.google.common.base.Joiner;\nimport org.apache.drill.shaded.guava.com.google.common.collect.Iterators;\nimport org.apache.drill.shaded.guava.com.google.common.collect.Sets;\n/**\n * Generates the set of spill files for this sort session.\n */\npublic class SpillSet {\n  private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(SpillSet.class);\n  /**\n   * Spilling on the Mac using the HDFS file system is very inefficient,\n   * affects performance numbers. This interface allows using HDFS in\n   * production, but to bypass the HDFS file system when needed.\n   */\n  private interface FileManager {\n    void deleteOnExit(String fragmentSpillDir) throws IOException;\n    WritableByteChannel createForWrite(String fileName) throws IOException;\n    InputStream openForInput(String fileName) throws IOException;\n    void deleteFile(String fileName) throws IOException;\n    void deleteDir(String fragmentSpillDir) throws IOException;\n    /**\n     * Given a manager-specific output stream, return the current write position.\n     * Used to report total write bytes.\n     *\n     * @param channel created by the file manager\n     * @return\n     */\n    long getWriteBytes(WritableByteChannel channel);\n    /**\n     * Given a manager-specific input stream, return the current read position.\n     * Used to report total read bytes.\n     *\n     * @param inputStream input stream created by the file manager\n     * @return\n     */\n    long getReadBytes(InputStream inputStream);\n  }\n  /**\n   * Normal implementation of spill files using the HDFS file system.\n   */\n  private static class HadoopFileManager implements FileManager{\n    /**\n     * The HDFS file system (for local directories, HDFS storage, etc.) used to\n     * create the temporary spill files. Allows spill files to be either on local\n     * disk, or in a DFS. (The admin can choose to put spill files in DFS when\n     * nodes provide insufficient local disk space)\n     */\n    // The buffer size is calculated as LCM of the Hadoop internal checksum buffer (9 * checksum length), where\n    // checksum length is 512 by default, and MapRFS page size that equals to 8 * 1024. The length of the transfer\n    // buffer does not affect performance of the write to hdfs or maprfs significantly once buffer length is more\n    // than 32 bytes.\n    private static final int TRANSFER_SIZE = 9 * 8 * 1024;\n    private final byte buffer[];\n    private FileSystem fs;\n    protected HadoopFileManager(String fsName) {\n      buffer = new byte[TRANSFER_SIZE];\n      Configuration conf = new Configuration();\n      conf.set(FileSystem.FS_DEFAULT_NAME_KEY, fsName);\n      try {\n        fs = FileSystem.get(conf);\n      } catch (IOException e) {\n        throw UserException.resourceError(e)\n              .message(\"Failed to get the File System for external sort\")\n              .build(logger);\n      }\n    }\n    @Override\n    public void deleteOnExit(String fragmentSpillDir) throws IOException {\n      fs.deleteOnExit(new Path(fragmentSpillDir));\n    }\n    @Override\n    public WritableByteChannel createForWrite(String fileName) throws IOException {\n      return new WritableByteChannelImpl(buffer, fs.create(new Path(fileName)));\n    }\n    @Override\n    public InputStream openForInput(String fileName) throws IOException {\n      return fs.open(new Path(fileName));\n    }\n    @Override\n    public void deleteFile(String fileName) throws IOException {\n      Path path = new Path(fileName);\n      if (fs.exists(path)) {\n        fs.delete(path, false);\n      }\n    }\n    @Override\n    public void deleteDir(String fragmentSpillDir) throws IOException {\n      Path path = new Path(fragmentSpillDir);\n      if (path != null && fs.exists(path)) {\n        if (fs.delete(path, true)) {\n            fs.cancelDeleteOnExit(path);\n        }\n      }\n    }\n    @Override\n    public long getWriteBytes(WritableByteChannel channel) {\n      try {\n        return ((FSDataOutputStream)((WritableByteChannelImpl)channel).out).getPos();\n      } catch (Exception e) {\n        // Just used for logging, not worth dealing with the exception.\n        return 0;\n      }\n    }\n    @Override\n    public long getReadBytes(InputStream inputStream) {\n      try {\n        return ((FSDataInputStream) inputStream).getPos();\n      } catch (IOException e) {\n        // Just used for logging, not worth dealing with the exception.\n        return 0;\n      }\n    }\n  }\n  /**\n   * Wrapper around an input stream to collect the total bytes\n   * read through the stream for use in reporting performance\n   * metrics.\n   */\n  public static class CountingInputStream extends InputStream\n  {\n    private InputStream in;\n    private long count;\n    public CountingInputStream(InputStream in) {\n      this.in = in;\n    }\n    @Override\n    public int read() throws IOException {\n      int b = in.read();\n      if (b != -1) {\n        count++;\n      }\n      return b;\n    }\n    @Override\n    public int read(byte b[]) throws IOException {\n      int n = in.read(b);\n      if (n != -1) {\n        count += n;\n      }\n      return n;\n    }\n    @Override\n    public int read(byte b[], int off, int len) throws IOException {\n      int n = in.read(b, off, len);\n      if (n != -1) {\n        count += n;\n      }\n      return n;\n    }\n    @Override\n    public long skip(long n) throws IOException {\n      return in.skip(n);\n    }\n    @Override\n    public void close() throws IOException {\n      in.close();\n    }\n    public long getCount() { return count; }\n  }\n  /**\n   * Wrapper around an output stream to collect the total bytes\n   * written through the stream for use in reporting performance\n   * metrics.\n   */\n  public static class CountingOutputStream extends OutputStream {\n    private OutputStream out;\n    private long count;\n    public CountingOutputStream(OutputStream out) {\n      this.out = out;\n    }\n    @Override\n    public void write(int b) throws IOException {\n      count++;\n      out.write(b);\n    }\n<fim_suffix>    @Override\n    public void write(byte[] b) throws IOException {\n      count += b.length;\n      out.write(b);\n    }\n    @Override\n    public void write(byte[] b, int off, int len) throws IOException {\n      count += len;<fim_middle>// function below has no smell\n"}