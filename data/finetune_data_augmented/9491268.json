{"text": "<fim_prefix>import org.apache.hadoop.mapreduce.v2.app.AppContext;\nimport org.apache.hadoop.mapreduce.v2.app.job.event.JobCounterUpdateEvent;\nimport org.apache.hadoop.yarn.api.records.Container;\nimport org.apache.hadoop.yarn.api.records.ContainerId;\nimport org.apache.hadoop.yarn.api.records.PreemptionContainer;\nimport org.apache.hadoop.yarn.api.records.PreemptionContract;\nimport org.apache.hadoop.yarn.api.records.PreemptionMessage;\nimport org.apache.hadoop.yarn.api.records.PreemptionResourceRequest;\nimport org.apache.hadoop.yarn.api.records.Resource;\nimport org.apache.hadoop.yarn.api.records.ResourceRequest;\nimport org.apache.hadoop.yarn.api.records.StrictPreemptionContract;\nimport org.apache.hadoop.yarn.event.EventHandler;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n/**\n * This policy works in combination with an implementation of task\n * checkpointing. It computes the tasks to be preempted in response to the RM\n * request for preemption. For strict requests, it maps containers to\n * corresponding tasks; for fungible requests, it attempts to pick the best\n * containers to preempt (reducers in reverse allocation order). The\n * TaskAttemptListener will interrogate this policy when handling a task\n * heartbeat to check whether the task should be preempted or not. When handling\n * fungible requests, the policy discount the RM ask by the amount of currently\n * in-flight preemptions (i.e., tasks that are checkpointing).\n *\n * This class it is also used to maintain the list of checkpoints for existing\n * tasks. Centralizing this functionality here, allows us to have visibility on\n * preemption and checkpoints in a single location, thus coordinating preemption\n * and checkpoint management decisions in a single policy.\n */\npublic class CheckpointAMPreemptionPolicy implements AMPreemptionPolicy {\n  // task attempts flagged for preemption\n  private final Set<TaskAttemptId> toBePreempted;\n  private final Set<TaskAttemptId> countedPreemptions;\n  private final Map<TaskId,TaskCheckpointID> checkpoints;\n  private final Map<TaskAttemptId,Resource> pendingFlexiblePreemptions;\n  @SuppressWarnings(\"rawtypes\")\n  private EventHandler eventHandler;\n  static final Logger LOG = LoggerFactory\n      .getLogger(CheckpointAMPreemptionPolicy.class);\n  public CheckpointAMPreemptionPolicy() {\n    this(Collections.synchronizedSet(new HashSet<TaskAttemptId>()),\n         Collections.synchronizedSet(new HashSet<TaskAttemptId>()),\n         Collections.synchronizedMap(new HashMap<TaskId,TaskCheckpointID>()),\n         Collections.synchronizedMap(new HashMap<TaskAttemptId,Resource>()));\n  }\n  CheckpointAMPreemptionPolicy(Set<TaskAttemptId> toBePreempted,\n      Set<TaskAttemptId> countedPreemptions,\n      Map<TaskId,TaskCheckpointID> checkpoints,\n      Map<TaskAttemptId,Resource> pendingFlexiblePreemptions) {\n    this.toBePreempted = toBePreempted;\n    this.countedPreemptions = countedPreemptions;\n    this.checkpoints = checkpoints;\n    this.pendingFlexiblePreemptions = pendingFlexiblePreemptions;\n  }\n  @Override\n  public void init(AppContext context) {\n    this.eventHandler = context.getEventHandler();\n  }\n  @Override\n  public void preempt(Context ctxt, PreemptionMessage preemptionRequests) {\n    if (preemptionRequests != null) {\n      // handling non-negotiable preemption\n      StrictPreemptionContract cStrict = preemptionRequests.getStrictContract();\n      if (cStrict != null\n          && cStrict.getContainers() != null\n          && cStrict.getContainers().size() > 0) {\n        LOG.info(\"strict preemption :\" +\n            preemptionRequests.getStrictContract().getContainers().size() +\n            \" containers to kill\");\n        // handle strict preemptions. These containers are non-negotiable\n        for (PreemptionContainer c :\n            preemptionRequests.getStrictContract().getContainers()) {\n          ContainerId reqCont = c.getId();\n          TaskAttemptId reqTask = ctxt.getTaskAttempt(reqCont);\n          if (reqTask != null) {\n            // ignore requests for preempting containers running maps\n            if (org.apache.hadoop.mapreduce.v2.api.records.TaskType.REDUCE\n                .equals(reqTask.getTaskId().getTaskType())) {\n              toBePreempted.add(reqTask);\n              LOG.info(\"preempting \" + reqCont + \" running task:\" + reqTask);\n            } else {\n              LOG.info(\"NOT preempting \" + reqCont + \" running task:\" + reqTask);\n            }\n          }\n        }\n      }\n      // handling negotiable preemption\n      PreemptionContract cNegot = preemptionRequests.getContract();\n      if (cNegot != null\n          && cNegot.getResourceRequest() != null\n          && cNegot.getResourceRequest().size() > 0\n          && cNegot.getContainers() != null\n          && cNegot.getContainers().size() > 0) {\n        LOG.info(\"negotiable preemption :\" +\n            preemptionRequests.getContract().getResourceRequest().size() +\n            \" resourceReq, \" +\n            preemptionRequests.getContract().getContainers().size() +\n            \" containers\");\n        // handle fungible preemption. Here we only look at the total amount of\n        // resources to be preempted and pick enough of our containers to\n        // satisfy that. We only support checkpointing for reducers for now.\n        List<PreemptionResourceRequest> reqResources =\n          preemptionRequests.getContract().getResourceRequest();\n        // compute the total amount of pending preemptions (to be discounted\n        // from current request)\n        int pendingPreemptionRam = 0;\n        int pendingPreemptionCores = 0;\n        for (Resource r : pendingFlexiblePreemptions.values()) {\n          pendingPreemptionRam += r.getMemorySize();\n          pendingPreemptionCores += r.getVirtualCores();\n        }\n        // discount preemption request based on currently pending preemption\n        for (PreemptionResourceRequest rr : reqResources) {\n          ResourceRequest reqRsrc = rr.getResourceRequest();\n          if (!ResourceRequest.ANY.equals(reqRsrc.getResourceName())) {\n            // For now, only respond to aggregate requests and ignore locality\n            continue;\n          }\n          LOG.info(\"ResourceRequest:\" + reqRsrc);\n          int reqCont = reqRsrc.getNumContainers();\n          long reqMem = reqRsrc.getCapability().getMemorySize();\n          long totalMemoryToRelease = reqCont * reqMem;\n          int reqCores = reqRsrc.getCapability().getVirtualCores();\n          int totalCoresToRelease = reqCont * reqCores;\n          // remove\n          if (pendingPreemptionRam > 0) {\n            // if goes negative we simply exit\n            totalMemoryToRelease -= pendingPreemptionRam;\n            // decrement pending resources if zero or negatve we will\n            // ignore it while processing next PreemptionResourceRequest\n            pendingPreemptionRam -= totalMemoryToRelease;\n          }\n          if (pendingPreemptionCores > 0) {\n            totalCoresToRelease -= pendingPreemptionCores;\n            pendingPreemptionCores -= totalCoresToRelease;\n          }\n          // reverse order of allocation (for now)\n          List<Container> listOfCont = ctxt.getContainers(TaskType.REDUCE);\n          Collections.sort(listOfCont, new Comparator<Container>() {\n            @Override\n            public int compare(final Container o1, final Container o2) {\n              return o2.getId().compareTo(o1.getId());\n            }\n          });\n          // preempt reducers first\n          for (Container cont : listOfCont) {\n            if (totalMemoryToRelease <= 0 && totalCoresToRelease<=0) {\n              break;\n            }\n            TaskAttemptId reduceId = ctxt.getTaskAttempt(cont.getId());\n            int cMem = (int) cont.getResource().getMemorySize();\n            int cCores = cont.getResource().getVirtualCores();\n            if (!toBePreempted.contains(reduceId)) {\n              totalMemoryToRelease -= cMem;\n              totalCoresToRelease -= cCores;\n                toBePreempted.add(reduceId);\n                pendingFlexiblePreemptions.put(reduceId, cont.getResource());\n            }\n            LOG.info(\"ResourceRequest:\" + reqRsrc + \" satisfied preempting \"\n                + reduceId);\n          }\n          // if map was preemptable we would do add them to toBePreempted here\n        }\n      }\n    }\n  }\n<fim_suffix>  @Override\n  public void handleFailedContainer(TaskAttemptId attemptID) {\n    toBePreempted.remove(attemptID);\n    checkpoints.remove(attemptID.getTaskId());\n  }<fim_middle>// function below has no smell\n"}