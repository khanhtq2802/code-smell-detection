{"text": "<fim_prefix> * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.solr.search.join;\nimport java.io.IOException;\nimport java.util.Arrays;\nimport org.apache.lucene.index.DocValues;\nimport org.apache.lucene.index.LeafReaderContext;\nimport org.apache.lucene.index.MultiDocValues;\nimport org.apache.lucene.index.OrdinalMap;\nimport org.apache.lucene.index.SortedDocValues;\nimport org.apache.lucene.index.SortedSetDocValues;\nimport org.apache.lucene.util.ArrayUtil;\nimport org.apache.lucene.util.BytesRef;\nimport org.apache.lucene.util.CharsRefBuilder;\nimport org.apache.lucene.util.LongValues;\nimport org.apache.solr.common.util.NamedList;\nimport org.apache.solr.request.DocValuesFacets;\nimport org.apache.solr.schema.FieldType;\nimport org.apache.solr.schema.SchemaField;\nimport org.apache.solr.search.DocIterator;\nimport org.apache.solr.search.SolrIndexSearcher;\n/**\n * This class is responsible for collecting block join facet counts for particular field\n */\nclass BlockJoinFieldFacetAccumulator {\n  private String fieldName;\n  private FieldType fieldType;\n  private int currentSegment = -1;\n  // for term lookups only\n  private SortedSetDocValues topSSDV;\n  private int[] globalCounts;\n  private SortedSetDocValues segmentSSDV;\n  // elems are : facet value counter<<32 | last parent doc num \n  private long[] segmentAccums = new long[0];\n  // for mapping per-segment ords to global ones\n  private OrdinalMap ordinalMap;\n  private SchemaField schemaField;\n  private SortedDocValues segmentSDV;\n  BlockJoinFieldFacetAccumulator(String fieldName, SolrIndexSearcher searcher) throws IOException {\n    this.fieldName = fieldName;\n    schemaField = searcher.getSchema().getField(fieldName);\n    fieldType = schemaField.getType();\n    ordinalMap = null;\n    if (schemaField.multiValued()) {\n      topSSDV = searcher.getSlowAtomicReader().getSortedSetDocValues(fieldName);\n      if (topSSDV instanceof MultiDocValues.MultiSortedSetDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedSetDocValues) topSSDV).mapping;\n      }\n    } else {\n      SortedDocValues single = searcher.getSlowAtomicReader().getSortedDocValues(fieldName);\n      if (single instanceof MultiDocValues.MultiSortedDocValues) {\n        ordinalMap = ((MultiDocValues.MultiSortedDocValues) single).mapping;\n      }\n      if (single != null) {\n        topSSDV = DocValues.singleton(single);\n      }\n    }\n  }\n  private boolean initSegmentData(String fieldName, LeafReaderContext leaf) throws IOException {\n    segmentSSDV = DocValues.getSortedSet(leaf.reader(), fieldName);\n    segmentAccums  = ArrayUtil.grow(segmentAccums, (int)segmentSSDV.getValueCount()+1);//+1\n    // zero counts, -1 parent\n    Arrays.fill(segmentAccums,0,(int)segmentSSDV.getValueCount()+1, 0x00000000ffffffffL);\n    segmentSDV = DocValues.unwrapSingleton(segmentSSDV);\n    return segmentSSDV.getValueCount()!=0;// perhaps we need to count \"missings\"?? \n  }\n  interface AggregatableDocIter extends DocIterator {\n    void reset();\n    /** a key to aggregate the current document */\n    int getAggKey();\n  }\n  static class SortedIntsAggDocIterator implements AggregatableDocIter {\n    private int[] childDocs;\n    private int childCount;\n    private int parentDoc;\n    private int pos=-1;\n    public SortedIntsAggDocIterator(int[] childDocs, int childCount, int parentDoc) {\n      this.childDocs = childDocs;\n      this.childCount = childCount;\n      this.parentDoc = parentDoc;\n    }\n    @Override\n    public boolean hasNext() {\n      return pos<childCount;\n    }\n    @Override\n    public Integer next() {\n      return nextDoc();\n    }\n    @Override\n    public int nextDoc() {\n      return childDocs[pos++];\n    }\n    @Override\n    public float score() {\n      return 0;\n    }\n    @Override\n    public void reset() {\n      pos=0;\n    }\n    @Override\n    public int getAggKey(){\n      return parentDoc;\n    }\n  }\n  void updateCountsWithMatchedBlock(AggregatableDocIter iter) throws IOException {\n    if (segmentSDV != null) {\n      // some codecs may optimize SORTED_SET storage for single-valued fields\n      for (iter.reset(); iter.hasNext(); ) {\n        final int docNum = iter.nextDoc();\n        if (docNum > segmentSDV.docID()) {\n          segmentSDV.advance(docNum);\n        }\n        int term;\n        if (docNum == segmentSDV.docID()) {\n          term = segmentSDV.ordValue();\n        } else {\n          term = -1;\n        }\n        accumulateTermOrd(term, iter.getAggKey());\n        //System.out.println(\"doc# \"+docNum+\" \"+fieldName+\" term# \"+term+\" tick \"+Long.toHexString(segmentAccums[1+term]));\n      }\n    } else {\n      for (iter.reset(); iter.hasNext(); ) {\n        final int docNum = iter.nextDoc();\n        if (docNum > segmentSSDV.docID()) {\n          segmentSSDV.advance(docNum);\n        }\n        if (docNum == segmentSSDV.docID()) {\n          int term = (int) segmentSSDV.nextOrd();\n          do { // absent values are designated by term=-1, first iteration counts [0] as \"missing\", and exit, otherwise it spins \n            accumulateTermOrd(term, iter.getAggKey());\n          } while (term>=0 && (term = (int) segmentSSDV.nextOrd()) >= 0);\n        }\n      }\n    }\n  }\n  String getFieldName() {\n    return fieldName;\n  }\n  /** copy paste from {@link DocValuesFacets} */\n  NamedList<Integer> getFacetValue() throws IOException {\n    NamedList<Integer> facetValue = new NamedList<>();\n    final CharsRefBuilder charsRef = new CharsRefBuilder(); // if there is no globs, take segment's ones\n    for (int i = 1; i< (globalCounts!=null ? globalCounts.length: segmentAccums.length); i++) {\n      int count = globalCounts!=null ? globalCounts[i] : (int)(segmentAccums [i]>>32);\n      if (count > 0) {\n        BytesRef term = topSSDV.lookupOrd(-1 + i);\n        fieldType.indexedToReadable(term, charsRef);\n        facetValue.add(charsRef.toString(), count);\n      }\n    }\n    return facetValue;\n  }\n  // @todo we can track in max term nums to loop only changed range while migrating and labeling \n  private void accumulateTermOrd(int term, int parentDoc) {\n    long accum = segmentAccums[1+term];\n    if(((int)(accum & 0xffffffffL))!=parentDoc)\n    {// incrementing older 32, reset smaller 32, set them to the new parent\n      segmentAccums[1+term] = ((accum +(0x1L<<32))&0xffffffffL<<32)|parentDoc;\n    }\n  }\n  void setNextReader(LeafReaderContext context) throws IOException {\n    initSegmentData(fieldName, context);\n    currentSegment = context.ord;\n  }\n  void migrateGlobal(){\n    if (currentSegment<0 // no hits\n        || segmentAccums.length==0 \n        || ordinalMap==null) { // single segment\n      return;\n    }\n    if(globalCounts==null){\n      // it might be just a single segment \n        globalCounts = new int[(int) ordinalMap.getValueCount()+ /*[0] for missing*/1];\n    }else{\n      assert currentSegment>=0;\n    }\n    migrateGlobal(globalCounts, segmentAccums, currentSegment, ordinalMap);\n  }\n  /** folds counts in segment ordinal space (segCounts) into global ordinal space (counts) \n   * copy paste-from {@link DocValuesFacets#migrateGlobal(int[], int[], int, OrdinalMap)}*/\n<fim_suffix>  void migrateGlobal(int counts[], long segCounts[], int subIndex, OrdinalMap map) {\n    final LongValues ordMap = map.getGlobalOrds(subIndex);\n    // missing count\n    counts[0] += (int) (segCounts[0]>>32);\n    // migrate actual ordinals\n    for (int ord = 1; ord <= segmentSSDV.getValueCount(); ord++) {\n      int count = (int) (segCounts[ord]>>32);\n      if (count != 0) {\n        counts[1+(int) ordMap.get(ord-1)] += count;\n      }\n    }\n  }<fim_middle>// function below has no smell\n"}