{"text": "<fim_prefix>/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing,\n * software distributed under the License is distributed on an\n * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n * KIND, either express or implied.  See the License for the\n * specific language governing permissions and limitations\n * under the License.\n */\npackage org.apache.hive.hcatalog.mapreduce;\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.hive.common.FileUtils;\nimport org.apache.hadoop.hive.common.JavaUtils;\nimport org.apache.hadoop.hive.metastore.HiveMetaHook;\nimport org.apache.hadoop.hive.ql.io.AcidUtils;\nimport org.apache.hadoop.hive.ql.io.IOConstants;\nimport org.apache.hadoop.hive.ql.metadata.HiveException;\nimport org.apache.hadoop.hive.ql.metadata.DefaultStorageHandler;\nimport org.apache.hadoop.hive.ql.plan.TableDesc;\nimport org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider;\nimport org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProvider;\nimport org.apache.hadoop.hive.serde2.AbstractSerDe;\nimport org.apache.hadoop.mapred.InputFormat;\nimport org.apache.hadoop.mapred.JobConf;\nimport org.apache.hadoop.mapred.OutputFormat;\nimport org.apache.hive.hcatalog.common.HCatConstants;\nimport org.apache.hive.hcatalog.common.HCatUtil;\nimport org.apache.hive.hcatalog.data.schema.HCatFieldSchema;\nimport org.apache.hive.hcatalog.data.schema.HCatSchema;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport java.io.IOException;\nimport java.util.ArrayList;\nimport java.util.LinkedList;\nimport java.util.List;\nimport java.util.Map;\n/**\n *  This class is used to encapsulate the InputFormat, OutputFormat and SerDe\n *  artifacts of tables which don't define a SerDe. This StorageHandler assumes\n *  the supplied storage artifacts are for a file-based storage system.\n */\npublic class FosterStorageHandler extends DefaultStorageHandler {\n  private static final Logger LOG = LoggerFactory.getLogger(FosterStorageHandler.class);\n  public Configuration conf;\n  /** The directory under which data is initially written for a non partitioned table */\n  protected static final String TEMP_DIR_NAME = \"_TEMP\";\n  private Class<? extends InputFormat> ifClass;\n  private Class<? extends OutputFormat> ofClass;\n  private Class<? extends AbstractSerDe> serDeClass;\n  public FosterStorageHandler(String ifName, String ofName, String serdeName) throws ClassNotFoundException {\n    this((Class<? extends InputFormat>) JavaUtils.loadClass(ifName),\n      (Class<? extends OutputFormat>) JavaUtils.loadClass(ofName),\n      (Class<? extends AbstractSerDe>) JavaUtils.loadClass(serdeName));\n  }\n  public FosterStorageHandler(Class<? extends InputFormat> ifClass,\n                Class<? extends OutputFormat> ofClass,\n                Class<? extends AbstractSerDe> serDeClass) {\n    this.ifClass = ifClass;\n    this.ofClass = ofClass;\n    this.serDeClass = serDeClass;\n  }\n  @Override\n  public Class<? extends InputFormat> getInputFormatClass() {\n    return ifClass;    //To change body of overridden methods use File | Settings | File Templates.\n  }\n  @Override\n  public Class<? extends OutputFormat> getOutputFormatClass() {\n    return ofClass;    //To change body of overridden methods use File | Settings | File Templates.\n  }\n  @Override\n  public Class<? extends AbstractSerDe> getSerDeClass() {\n    return serDeClass;  //To change body of implemented methods use File | Settings | File Templates.\n  }\n  @Override\n  public HiveMetaHook getMetaHook() {\n    return null;\n  }\n  @Override\n  public void configureJobConf(TableDesc tableDesc, JobConf jobConf) {\n    //do nothing currently\n  }\n<fim_suffix>  @Override\n  public void configureInputJobProperties(TableDesc tableDesc,\n                      Map<String, String> jobProperties) {\n    try {\n      Map<String, String> tableProperties = tableDesc.getJobProperties();\n      String jobInfoProperty = tableProperties.get(HCatConstants.HCAT_KEY_JOB_INFO);\n      if (jobInfoProperty != null) {\n        LinkedList<InputJobInfo> inputJobInfos = (LinkedList<InputJobInfo>) HCatUtil.deserialize(\n                jobInfoProperty);\n        if (inputJobInfos == null || inputJobInfos.isEmpty()) {\n          throw new IOException(\"No InputJobInfo was set in job config\");\n        }\n        InputJobInfo inputJobInfo = inputJobInfos.getLast();\n        HCatTableInfo tableInfo = inputJobInfo.getTableInfo();\n        HCatSchema dataColumns = tableInfo.getDataColumns();\n        List<HCatFieldSchema> dataFields = dataColumns.getFields();\n        StringBuilder columnNamesSb = new StringBuilder();\n        StringBuilder typeNamesSb = new StringBuilder();\n        for (HCatFieldSchema dataField : dataFields) {\n        if (columnNamesSb.length() > 0) {\n            columnNamesSb.append(\",\");\n            typeNamesSb.append(\":\");\n          }\n          columnNamesSb.append(dataField.getName());\n          typeNamesSb.append(dataField.getTypeString());\n        }\n        jobProperties.put(IOConstants.SCHEMA_EVOLUTION_COLUMNS, columnNamesSb.toString());\n        jobProperties.put(IOConstants.SCHEMA_EVOLUTION_COLUMNS_TYPES, typeNamesSb.toString());\n        boolean isTransactionalTable = AcidUtils.isTablePropertyTransactional(tableProperties);\n        AcidUtils.AcidOperationalProperties acidOperationalProperties =\n                AcidUtils.getAcidOperationalProperties(tableProperties);\n        AcidUtils.setAcidOperationalProperties(\n            jobProperties, isTransactionalTable, acidOperationalProperties);\n      }\n    } catch (IOException e) {\n      throw new IllegalStateException(\"Failed to set output path\", e);\n    }\n  }\n  @Override\n  public void configureOutputJobProperties(TableDesc tableDesc,\n                       Map<String, String> jobProperties) {\n    try {\n      OutputJobInfo jobInfo = (OutputJobInfo)\n        HCatUtil.deserialize(tableDesc.getJobProperties().get(\n          HCatConstants.HCAT_KEY_OUTPUT_INFO));\n      String parentPath = jobInfo.getTableInfo().getTableLocation();\n      String dynHash = tableDesc.getJobProperties().get(\n        HCatConstants.HCAT_DYNAMIC_PTN_JOBID);\n      String idHash = tableDesc.getJobProperties().get(\n          HCatConstants.HCAT_OUTPUT_ID_HASH);\n      // For dynamic partitioned writes without all keyvalues specified,\n      // we create a temp dir for the associated write job\n      if (dynHash != null) {\n        // if external table and custom root specified, update the parent path\n        if (Boolean.parseBoolean((String)tableDesc.getProperties().get(\"EXTERNAL\"))\n            && jobInfo.getCustomDynamicRoot() != null\n            && jobInfo.getCustomDynamicRoot().length() > 0) {\n          parentPath = new Path(parentPath, jobInfo.getCustomDynamicRoot()).toString();\n        }\n        parentPath = new Path(parentPath, FileOutputCommitterContainer.DYNTEMP_DIR_NAME + dynHash).toString();\n      } else {\n        parentPath = new Path(parentPath,FileOutputCommitterContainer.SCRATCH_DIR_NAME + idHash).toString();\n      }\n      String outputLocation;\n      if ((dynHash != null)\n          && Boolean.parseBoolean((String)tableDesc.getProperties().get(\"EXTERNAL\"))\n          && jobInfo.getCustomDynamicPath() != null\n          && jobInfo.getCustomDynamicPath().length() > 0) {\n        // dynamic partitioning with custom path; resolve the custom path\n        // using partition column values\n        outputLocation = HCatFileUtil.resolveCustomPath(jobInfo, null, true);\n      } else if ((dynHash == null)\n           && Boolean.parseBoolean((String)tableDesc.getProperties().get(\"EXTERNAL\"))\n           && jobInfo.getLocation() != null && jobInfo.getLocation().length() > 0) {\n        // honor custom location for external table apart from what metadata specifies\n        outputLocation = jobInfo.getLocation();\n      } else if (dynHash == null && jobInfo.getPartitionValues().size() == 0) {<fim_middle>// function below is feature envy and long method\n"}