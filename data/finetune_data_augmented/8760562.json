{"text": "<fim_prefix>package org.apache.hadoop.examples;\n\n/**\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\nimport java.io.BufferedReader;\nimport java.io.IOException;\nimport java.io.InputStreamReader;\nimport java.util.StringTokenizer;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.conf.Configured;\nimport org.apache.hadoop.fs.FileSystem;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.Mapper;\nimport org.apache.hadoop.mapreduce.Reducer;\nimport org.apache.hadoop.mapreduce.TaskCounter;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\nimport org.apache.hadoop.util.Tool;\nimport org.apache.hadoop.util.ToolRunner;\n\nimport com.google.common.base.Charsets;\n\n<fim_suffix>public class WordMedian extends Configured implements Tool {\n\n  private double median = 0;\n  private final static IntWritable ONE = new IntWritable(1);\n\n  /**\n   * Maps words from line of text into a key-value pair; the length of the word\n   * as the key, and 1 as the value.\n   */\n  public static class WordMedianMapper extends\n      Mapper<Object, Text, IntWritable, IntWritable> {\n\n    private IntWritable length = new IntWritable();\n\n    /**\n     * Emits a key-value pair for counting the word. Outputs are (IntWritable,\n     * IntWritable).\n     * \n     * @param value\n     *          This will be a line of text coming in from our input file.\n     */\n    public void map(Object key, Text value, Context context)\n        throws IOException, InterruptedException {\n      StringTokenizer itr = new StringTokenizer(value.toString());\n      while (itr.hasMoreTokens()) {\n        String string = itr.nextToken();\n        length.set(string.length());\n        context.write(length, ONE);\n      }\n    }\n  }\n\n  /**\n   * Performs integer summation of all the values for each key.\n   */\n  public static class WordMedianReducer extends\n      Reducer<IntWritable, IntWritable, IntWritable, IntWritable> {\n\n    private IntWritable val = new IntWritable();\n\n    /**\n     * Sums all the individual values within the iterator and writes them to the\n     * same key.\n     * \n     * @param key\n     *          This will be a length of a word that was read.\n     * @param values\n     *          This will be an iterator of all the values associated with that\n     *          key.\n     */\n    public void reduce(IntWritable key, Iterable<IntWritable> values,\n        Context context) throws IOException, InterruptedException {\n\n      int sum = 0;\n      for (IntWritable value : values) {\n        sum += value.get();\n      }\n      val.set(sum);\n      context.write(key, val);\n    }\n  }\n\n  /**\n   * This is a standard program to read and find a median value based on a file\n   * of word counts such as: 1 456, 2 132, 3 56... Where the first values are\n   * the word lengths and the following values are the number of times that\n   * words of that length appear.\n   * \n   * @param path\n   *          The path to read the HDFS file from (part-r-00000...00001...etc).\n   * @param medianIndex1\n   *          The first length value to look for.\n   * @param medianIndex2\n   *          The second length value to look for (will be the same as the first\n   *          if there are an even number of words total).\n   * @throws IOException\n   *           If file cannot be found, we throw an exception.\n   * */\n  private double readAndFindMedian(String path, int medianIndex1,\n      int medianIndex2, Configuration conf) throws IOException {\n    FileSystem fs = FileSystem.get(conf);\n    Path file = new Path(path, \"part-r-00000\");\n\n    if (!fs.exists(file))\n      throw new IOException(\"Output not found!\");\n\n    BufferedReader br = null;\n\n    try {\n      br = new BufferedReader(new InputStreamReader(fs.open(file), Charsets.UTF_8));\n      int num = 0;\n\n      String line;\n      while ((line = br.readLine()) != null) {\n        StringTokenizer st = new StringTokenizer(line);\n\n        // grab length\n        String currLen = st.nextToken();\n\n        // grab count\n        String lengthFreq = st.nextToken();\n\n        int prevNum = num;\n        num += Integer.parseInt(lengthFreq);\n\n        if (medianIndex2 >= prevNum && medianIndex1 <= num) {\n          System.out.println(\"The median is: \" + currLen);\n          br.close();\n          return Double.parseDouble(currLen);\n        } else if (medianIndex2 >= prevNum && medianIndex1 < num) {\n          String nextCurrLen = st.nextToken();\n          double theMedian = (Integer.parseInt(currLen) + Integer\n              .parseInt(nextCurrLen)) / 2.0;\n          System.out.println(\"The median is: \" + theMedian);\n          br.close();\n          return theMedian;\n        }\n      }\n    } finally {\n      if (br != null) {\n        br.close();\n      }\n    }\n    // error, no median found\n    return -1;\n  }\n\n  public static void main(String[] args) throws Exception {\n    ToolRunner.run(new Configuration(), new WordMedian(), args);\n  }\n\n  @Override\n  public int run(String[] args) throws Exception {\n    if (args.length != 2) {\n      System.err.println(\"Usage: wordmedian <in> <out>\");\n      return 0;\n    }\n\n    setConf(new Configuration());\n    Configuration conf = getConf();\n\n    @SuppressWarnings(\"deprecation\")\n    Job job = new Job(conf, \"word median\");\n    job.setJarByClass(WordMedian.class);\n    job.setMapperClass(WordMedianMapper.class);\n    job.setCombinerClass(WordMedianReducer.class);\n    job.setReducerClass(WordMedianReducer.class);\n    job.setOutputKeyClass(IntWritable.class);\n    job.setOutputValueClass(IntWritable.class);\n    FileInputFormat.addInputPath(job, new Path(args[0]));\n    FileOutputFormat.setOutputPath(job, new Path(args[1]));\n    boolean result = job.waitForCompletion(true);\n\n    // Wait for JOB 1 -- get middle value to check for Median\n\n    long totalWords = job.getCounters()\n        .getGroup(TaskCounter.class.getCanonicalName())\n        .findCounter(\"MAP_OUTPUT_RECORDS\", \"Map output records\").getValue();\n    int medianIndex1 = (int) Math.ceil((totalWords / 2.0));\n    int medianIndex2 = (int) Math.floor((totalWords / 2.0));\n\n    median = readAndFindMedian(args[1], medianIndex1, medianIndex2, conf);\n\n    return (result ? 0 : 1);\n  }\n\n  public double getMedian() {\n    return median;\n  }\n}<fim_middle>// class below is blob\n"}