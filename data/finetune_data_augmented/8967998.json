{"text": "<fim_prefix>/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.hadoop.hive.ql.metadata.formatting;\nimport java.io.Closeable;\nimport java.io.DataOutputStream;\nimport java.io.IOException;\nimport java.io.OutputStream;\nimport java.io.UnsupportedEncodingException;\nimport java.net.URLDecoder;\nimport java.nio.charset.StandardCharsets;\nimport java.util.ArrayList;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Set;\nimport java.util.concurrent.TimeUnit;\nimport org.apache.commons.io.IOUtils;\nimport org.apache.commons.lang.StringUtils;\nimport org.apache.hadoop.hive.ql.session.SessionState;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport org.apache.hadoop.fs.FileStatus;\nimport org.apache.hadoop.fs.FileSystem;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.hive.conf.HiveConf;\nimport org.apache.hadoop.hive.metastore.TableType;\nimport org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj;\nimport org.apache.hadoop.hive.metastore.api.FieldSchema;\nimport org.apache.hadoop.hive.metastore.api.PrincipalType;\nimport org.apache.hadoop.hive.metastore.api.WMFullResourcePlan;\nimport org.apache.hadoop.hive.metastore.api.WMResourcePlan;\nimport org.apache.hadoop.hive.metastore.api.WMValidateResourcePlanResponse;\nimport org.apache.hadoop.hive.ql.metadata.CheckConstraint;\nimport org.apache.hadoop.hive.ql.metadata.DefaultConstraint;\nimport org.apache.hadoop.hive.ql.metadata.ForeignKeyInfo;\nimport org.apache.hadoop.hive.ql.metadata.Hive;\nimport org.apache.hadoop.hive.ql.metadata.HiveException;\nimport org.apache.hadoop.hive.ql.metadata.NotNullConstraint;\nimport org.apache.hadoop.hive.ql.metadata.Partition;\nimport org.apache.hadoop.hive.ql.metadata.PrimaryKeyInfo;\nimport org.apache.hadoop.hive.ql.metadata.StorageHandlerInfo;\nimport org.apache.hadoop.hive.ql.metadata.Table;\nimport org.apache.hadoop.hive.ql.metadata.UniqueConstraint;\nimport org.codehaus.jackson.JsonGenerator;\nimport org.codehaus.jackson.map.ObjectMapper;\nimport static org.apache.hadoop.hive.conf.Constants.MATERIALIZED_VIEW_REWRITING_TIME_WINDOW;\n/**\n * Format table and index information for machine readability using\n * json.\n */\npublic class JsonMetaDataFormatter implements MetaDataFormatter {\n  private static final Logger LOG = LoggerFactory.getLogger(JsonMetaDataFormatter.class);\n  /**\n   * Convert the map to a JSON string.\n   */\n<fim_suffix>  private void asJson(OutputStream out, Map<String, Object> data)\n      throws HiveException\n      {\n    try {\n      new ObjectMapper().writeValue(out, data);\n    } catch (IOException e) {\n      throw new HiveException(\"Unable to convert to json\", e);\n    }\n      }\n  /**\n   * Write an error message.\n   */\n  @Override\n  public void error(OutputStream out, String msg, int errorCode, String sqlState)\n      throws HiveException\n      {\n    error(out, msg, errorCode, sqlState, null);\n      }\n  @Override\n  public void error(OutputStream out, String errorMessage, int errorCode, String sqlState, String errorDetail) throws HiveException {\n    MapBuilder mb = MapBuilder.create().put(\"error\", errorMessage);\n    if(errorDetail != null) {\n      mb.put(\"errorDetail\", errorDetail);\n    }\n    mb.put(\"errorCode\", errorCode);\n    if(sqlState != null) {\n      mb.put(\"sqlState\", sqlState);\n    }\n    asJson(out,mb.build());\n  }\n  /**\n   * Show a list of tables.\n   */\n  @Override\n  public void showTables(DataOutputStream out, Set<String> tables)\n      throws HiveException {\n    asJson(out, MapBuilder.create().put(\"tables\", tables).build());\n  }\n  /**\n   * Show a list of tables including table types.\n   */\n  @Override\n  public void showTablesExtended(DataOutputStream out, List<Table> tables)\n      throws HiveException {\n    if (tables.isEmpty()) {\n      // Nothing to do\n      return;\n    }\n    MapBuilder builder = MapBuilder.create();\n    ArrayList<Map<String, Object>> res = new ArrayList<Map<String, Object>>();\n    for (Table table : tables) {\n      final String tableName = table.getTableName();\n      final String tableType = table.getTableType().toString();\n      res.add(builder\n          .put(\"Table Name\", tableName)\n          .put(\"Table Type\", tableType)\n          .build());\n    }\n    asJson(out, builder.put(\"tables\", res).build());\n  }\n  /**\n   * Show a list of materialized views.\n   */\n  @Override\n  public void showMaterializedViews(DataOutputStream out, List<Table> materializedViews)\n      throws HiveException {\n    if (materializedViews.isEmpty()) {\n      // Nothing to do\n      return;\n    }\n    MapBuilder builder = MapBuilder.create();\n    ArrayList<Map<String, Object>> res = new ArrayList<Map<String, Object>>();\n    for (Table mv : materializedViews) {\n      final String mvName = mv.getTableName();\n      final String rewriteEnabled = mv.isRewriteEnabled() ? \"Yes\" : \"No\";\n      // Currently, we only support manual refresh\n      // TODO: Update whenever we have other modes\n      final String refreshMode = \"Manual refresh\";\n      final String timeWindowString = mv.getProperty(MATERIALIZED_VIEW_REWRITING_TIME_WINDOW);\n      final String mode;\n      if (!org.apache.commons.lang.StringUtils.isEmpty(timeWindowString)) {\n        long time = HiveConf.toTime(timeWindowString,\n            HiveConf.getDefaultTimeUnit(HiveConf.ConfVars.HIVE_MATERIALIZED_VIEW_REWRITING_TIME_WINDOW),\n            TimeUnit.MINUTES);\n        if (time > 0L) {\n          mode = refreshMode + \" (Valid for \" + time + \"min)\";\n        } else if (time == 0L) {\n          mode = refreshMode + \" (Valid until source tables modified)\";\n        } else {\n          mode = refreshMode + \" (Valid always)\";\n        }\n      } else {\n        mode = refreshMode;\n      }\n      res.add(builder\n          .put(\"MV Name\", mvName)\n          .put(\"Rewriting Enabled\", rewriteEnabled)\n          .put(\"Mode\", mode)\n          .build());\n    }\n    asJson(out, builder.put(\"materialized views\", res).build());\n  }\n  /**\n   * Describe table.\n   */\n  @Override\n  public void describeTable(DataOutputStream out, String colPath,\n      String tableName, Table tbl, Partition part, List<FieldSchema> cols,\n      boolean isFormatted, boolean isExt,\n      boolean isOutputPadded, List<ColumnStatisticsObj> colStats,\n      PrimaryKeyInfo pkInfo, ForeignKeyInfo fkInfo,\n      UniqueConstraint ukInfo, NotNullConstraint nnInfo, DefaultConstraint dInfo,\n      CheckConstraint cInfo, StorageHandlerInfo storageHandlerInfo) throws HiveException {\n    MapBuilder builder = MapBuilder.create();\n    builder.put(\"columns\", makeColsUnformatted(cols));\n    if (isExt) {\n      if (part != null) {\n        builder.put(\"partitionInfo\", part.getTPartition());\n      }\n      else {\n        builder.put(\"tableInfo\", tbl.getTTable());\n      }\n      if (pkInfo != null && !pkInfo.getColNames().isEmpty()) {\n        builder.put(\"primaryKeyInfo\", pkInfo);\n      }\n      if (fkInfo != null && !fkInfo.getForeignKeys().isEmpty()) {\n        builder.put(\"foreignKeyInfo\", fkInfo);\n      }\n      if (ukInfo != null && !ukInfo.getUniqueConstraints().isEmpty()) {\n        builder.put(\"uniqueConstraintInfo\", ukInfo);\n      }\n      if (nnInfo != null && !nnInfo.getNotNullConstraints().isEmpty()) {\n        builder.put(\"notNullConstraintInfo\", nnInfo);\n      }<fim_middle>// function below has no smell\n"}