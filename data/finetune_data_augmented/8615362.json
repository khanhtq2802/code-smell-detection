{"text": "<fim_prefix>        if (msgs[rmtMapIdx] == null)\n            msgs[rmtMapIdx] = new HadoopShuffleMessage(job.id(), rmtRdcIdx, msgSize);\n        visit(map, rmtMapIdx, rmtRdcIdx);\n        if (flush && msgs[rmtMapIdx].offset() != 0)\n            send(rmtMapIdx, rmtRdcIdx, 0);\n    }\n    /**\n     * Flush remote direct context.\n     *\n     * @param rmtMapIdx Remote map index.\n     * @param rmtDirectCtx Remote direct context.\n     * @param reset Whether to perform reset.\n     */\n    private void sendShuffleMessage(int rmtMapIdx, @Nullable HadoopDirectDataOutputContext rmtDirectCtx,\n        boolean reset) {\n        if (rmtDirectCtx == null)\n            return;\n        int cnt = rmtDirectCtx.count();\n        if (cnt == 0)\n            return;\n        int rmtRdcIdx = stripeMappers ? rmtMapIdx % totalReducerCnt : rmtMapIdx;\n        HadoopDirectDataOutputState state = rmtDirectCtx.state();\n        if (reset)\n            rmtDirectCtx.reset();\n        HadoopDirectShuffleMessage msg = new HadoopDirectShuffleMessage(job.id(), rmtRdcIdx, cnt,\n            state.buffer(), state.bufferLength(), state.dataLength());\n        T nodeId = reduceAddrs[rmtRdcIdx];\n        io.apply(nodeId, msg);\n        remoteShuffleState(nodeId).onShuffleMessage();\n    }\n    /**\n     * Visit output map.\n     *\n     * @param map Map.\n     * @param rmtMapIdx Remote map index.\n     * @param rmtRdcIdx Remote reducer index.\n     * @throws IgniteCheckedException If failed.\n     */\n    private void visit(HadoopMultimap map, final int rmtMapIdx, final int rmtRdcIdx) throws IgniteCheckedException {\n        map.visit(false, new HadoopMultimap.Visitor() {\n            /** */\n            private long keyPtr;\n            /** */\n            private int keySize;\n            /** */\n            private boolean keyAdded;\n            /** {@inheritDoc} */\n            @Override public void onKey(long keyPtr, int keySize) {\n                this.keyPtr = keyPtr;\n                this.keySize = keySize;\n                keyAdded = false;\n            }\n            private boolean tryAdd(long valPtr, int valSize) {\n                HadoopShuffleMessage msg = msgs[rmtMapIdx];\n                if (!keyAdded) { // Add key and value.\n                    int size = keySize + valSize;\n                    if (!msg.available(size, false))\n                        return false;\n                    msg.addKey(keyPtr, keySize);\n                    msg.addValue(valPtr, valSize);\n                    keyAdded = true;\n                    return true;\n                }\n                if (!msg.available(valSize, true))\n                    return false;\n                msg.addValue(valPtr, valSize);\n                return true;\n            }\n            /** {@inheritDoc} */\n            @Override public void onValue(long valPtr, int valSize) {\n                if (tryAdd(valPtr, valSize))\n                    return;\n                send(rmtMapIdx, rmtRdcIdx, keySize + valSize);\n                keyAdded = false;\n                if (!tryAdd(valPtr, valSize))\n                    throw new IllegalStateException();\n            }\n        });\n    }\n    /**\n     * Send message.\n     *\n     * @param rmtMapIdx Remote map index.\n     * @param rmtRdcIdx Remote reducer index.\n     * @param newBufMinSize Min new buffer size.\n     */\n    private void send(int rmtMapIdx, int rmtRdcIdx, int newBufMinSize) {\n        HadoopShuffleMessage msg = msgs[rmtMapIdx];\n        final long msgId = msg.id();\n        final GridFutureAdapter<?> fut;\n        if (embedded)\n            fut = null;\n        else {\n            fut = new GridFutureAdapter<>();\n            IgniteBiTuple<HadoopShuffleMessage, GridFutureAdapter<?>> old = sentMsgs.putIfAbsent(msgId,\n                new IgniteBiTuple<HadoopShuffleMessage, GridFutureAdapter<?>>(msg, fut));\n            assert old == null;\n        }\n        try {\n            io.apply(reduceAddrs[rmtRdcIdx], msg);\n            if (embedded)\n                remoteShuffleState(reduceAddrs[rmtRdcIdx]).onShuffleMessage();\n        }\n        catch (GridClosureException e) {\n            if (fut != null)\n                fut.onDone(U.unwrap(e));\n        }\n        if (fut != null) {\n            fut.listen(new IgniteInClosure<IgniteInternalFuture<?>>() {\n                @Override public void apply(IgniteInternalFuture<?> f) {\n                    try {\n                        f.get();\n                        // Clean up the future from map only if there was no exception.\n                        // Otherwise flush() should fail.\n                        sentMsgs.remove(msgId);\n                    }\n                    catch (IgniteCheckedException e) {\n                        log.error(\"Failed to send message.\", e);\n                    }\n                }\n            });\n        }\n        msgs[rmtMapIdx] = newBufMinSize == 0 ? null : new HadoopShuffleMessage(job.id(), rmtRdcIdx,\n            Math.max(msgSize, newBufMinSize));\n    }\n    /** {@inheritDoc} */\n    @Override public void close() throws IgniteCheckedException {\n        if (snd != null) {\n            snd.cancel();\n            try {\n                snd.join();\n            }\n            catch (InterruptedException e) {\n                throw new IgniteInterruptedCheckedException(e);\n            }\n        }\n        close(locMaps);\n        close(rmtMaps);\n    }\n    /**\n     * @param maps Maps.\n     */\n    private void close(AtomicReferenceArray<HadoopMultimap> maps) {\n        for (int i = 0; i < maps.length(); i++) {\n            HadoopMultimap map = maps.get(i);\n            if (map != null)\n                map.close();\n        }\n    }\n    /**\n     * @return Future.\n     */\n    @SuppressWarnings(\"unchecked\")\n    public IgniteInternalFuture<?> flush() throws IgniteCheckedException {\n        if (log.isDebugEnabled())\n            log.debug(\"Flushing job \" + job.id() + \" on address \" + locReduceAddr);\n        flushed = true;\n        if (totalReducerCnt == 0)\n            return new GridFinishedFuture<>();\n        if (!stripeMappers) {\n            U.await(ioInitLatch);\n            GridWorker snd0 = snd;\n            if (snd0 != null) {\n                if (log.isDebugEnabled())\n                    log.debug(\"Cancelling sender thread.\");\n                snd0.cancel();\n                try {\n                    snd0.join();\n                    if (log.isDebugEnabled())\n                        log.debug(\"Finished waiting for sending thread to complete on shuffle job flush: \" + job.id());\n                }\n                catch (InterruptedException e) {\n                    throw new IgniteInterruptedCheckedException(e);\n                }\n            }\n            collectUpdatesAndSend(true); // With flush.\n            if (log.isDebugEnabled())\n                log.debug(\"Finished sending collected updates to remote reducers: \" + job.id());\n        }\n        GridCompoundFuture fut = new GridCompoundFuture<>();\n        if (embedded) {\n            boolean sent = false;\n            for (Map.Entry<T, HadoopShuffleRemoteState> rmtStateEntry : remoteShuffleStates().entrySet()) {\n                T dest = rmtStateEntry.getKey();\n                HadoopShuffleRemoteState rmtState = rmtStateEntry.getValue();\n                HadoopShuffleFinishRequest req = new HadoopShuffleFinishRequest(job.id(), rmtState.messageCount());\n                io.apply(dest, req);\n                if (log.isDebugEnabled())\n                    log.debug(\"Sent shuffle finish request [jobId=\" + job.id() + \", dest=\" + dest +\n                        \", req=\" + req + ']');\n                fut.add(rmtState.future());\n                sent = true;\n            }\n            if (sent)\n                fut.markInitialized();\n            else\n                return new GridFinishedFuture<>();\n        }\n        else {\n            for (IgniteBiTuple<HadoopShuffleMessage, GridFutureAdapter<?>> tup : sentMsgs.values())\n                fut.add(tup.get2());\n            fut.markInitialized();\n            if (log.isDebugEnabled())\n                log.debug(\"Collected futures to compound futures for flush: \" + sentMsgs.size());\n        }\n        return fut;\n    }\n    /**\n     * @param taskCtx Task context.\n     * @return Output.\n     * @throws IgniteCheckedException If failed.\n     */\n<fim_suffix>    public HadoopTaskOutput output(HadoopTaskContext taskCtx) throws IgniteCheckedException {\n        switch (taskCtx.taskInfo().type()) {\n            case MAP:\n                assert !job.info().hasCombiner() : \"The output creation is allowed if combiner has not been defined.\";\n            case COMBINE:\n                return new PartitionedOutput(taskCtx);\n            default:\n                throw new IllegalStateException(\"Illegal type: \" + taskCtx.taskInfo().type());\n        }\n    }<fim_middle>// function below has no smell\n"}