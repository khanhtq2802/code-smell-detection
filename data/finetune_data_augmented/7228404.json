{"text": "<fim_prefix>     * an external source to a Kafka topic. Unlimited (very large) blocking time and retries\n     * are used to handle broker failures. Source records are committed when sends succeed.\n     *\n     */\n    public static class KafkaSink extends AbstractScenario {\n        private final String topic;\n        public KafkaSink(String bootstrapServers, String topic) {\n            super(bootstrapServers);\n            this.topic = topic;\n        }\n        public Flux<?> flux() {\n            SenderOptions<Integer, Person> senderOptions = senderOptions()\n                    .producerProperty(ProducerConfig.ACKS_CONFIG, \"all\")\n                    .producerProperty(ProducerConfig.MAX_BLOCK_MS_CONFIG, Long.MAX_VALUE)\n                    .producerProperty(ProducerConfig.RETRIES_CONFIG, Integer.MAX_VALUE);\n            Flux<Person> srcFlux = source().flux();\n            return sender(senderOptions)\n                    .send(srcFlux.map(p -> SenderRecord.create(new ProducerRecord<>(topic, p.id(), p), p.id())))\n                    .doOnError(e -> log.error(\"Send failed, terminating.\", e))\n                    .doOnNext(r -> {\n                        int id = r.correlationMetadata();\n                        log.trace(\"Successfully stored person with id {} in Kafka\", id);\n                        source.commit(id);\n                    })\n                    .doOnCancel(() -> close());\n        }\n    }\n    /**\n     * This sample demonstrates the use of Kafka as a sink when messages are transferred from\n     * an external source to a Kafka topic. Unlimited (very large) blocking time and retries\n     * are used to handle broker failures. Each source record is transformed into multiple Kafka\n     * records and the result records are sent to Kafka using chained outbound sequences.\n     * Source records are committed when sends succeed.\n     *\n     */\n    public static class KafkaSinkChain extends AbstractScenario {\n        private final String topic1;\n        private final String topic2;\n        public KafkaSinkChain(String bootstrapServers, String topic1, String topic2) {\n            super(bootstrapServers);\n            this.topic1 = topic1;\n            this.topic2 = topic2;\n        }\n        public Flux<?> flux() {\n            SenderOptions<Integer, Person> senderOptions = senderOptions()\n                    .producerProperty(ProducerConfig.ACKS_CONFIG, \"all\")\n                    .producerProperty(ProducerConfig.MAX_BLOCK_MS_CONFIG, Long.MAX_VALUE)\n                    .producerProperty(ProducerConfig.RETRIES_CONFIG, Integer.MAX_VALUE);\n            KafkaSender<Integer, Person> sender = sender(senderOptions);\n            Flux<Person> srcFlux = source().flux();\n            return srcFlux.concatMap(p ->\n                    sender.createOutbound()\n                          .send(Mono.just(new ProducerRecord<>(topic1, p.id(), p)))\n                          .send(Mono.just(new ProducerRecord<>(topic2, p.id(), p.upperCase())))\n                          .then()\n                          .doOnSuccess(v -> source.commit(p.id())))\n                          .doOnCancel(() -> close());\n        }\n    }\n    /**\n     * This sample demonstrates the use of Kafka as a source when messages are transferred from\n     * a Kafka topic to an external sink. Kafka offsets are committed when records are successfully\n     * transferred. Unlimited retries on the source Kafka Flux ensure that the Kafka consumer is\n     * restarted if there are any exceptions while processing records.\n     */\n    public static class KafkaSource extends AbstractScenario {\n        private final String topic;\n        private final Scheduler scheduler;\n        public KafkaSource(String bootstrapServers, String topic) {\n            super(bootstrapServers);\n            this.topic = topic;\n            this.scheduler = Schedulers.newSingle(\"sample\", true);\n        }\n        public Flux<?> flux() {\n            return KafkaReceiver.create(receiverOptions(Collections.singletonList(topic)).commitInterval(Duration.ZERO))\n                           .receive()\n                           .publishOn(scheduler)\n                           .concatMap(m -> storeInDB(m.value())\n                                          .thenEmpty(m.receiverOffset().commit()))\n                           .retry()\n                           .doOnCancel(() -> close());\n        }\n        public Mono<Void> storeInDB(Person person) {\n            log.info(\"Successfully processed person with id {} from Kafka\", person.id());\n            return Mono.empty();\n        }\n        public void close() {\n            super.close();\n            scheduler.dispose();\n        }\n    }\n    /**\n     * This sample demonstrates a flow where messages are consumed from a Kafka topic, transformed\n     * and the results stored in another Kafka topic. Manual acknowledgement ensures that offsets from\n     * the source are committed only after they have been transferred to the destination. Acknowledged\n     * offsets are committed periodically.\n     */\n    public static class KafkaTransform extends AbstractScenario {\n        private final String sourceTopic;\n        private final String destTopic;\n        public KafkaTransform(String bootstrapServers, String sourceTopic, String destTopic) {\n            super(bootstrapServers);\n            this.sourceTopic = sourceTopic;\n            this.destTopic = destTopic;\n        }\n        public Flux<?> flux() {\n            KafkaSender<Integer, Person> sender = sender(senderOptions());\n            return KafkaReceiver.create(receiverOptions(Collections.singleton(sourceTopic)))\n                                .receive()\n                                .map(m -> SenderRecord.create(transform(m.value()), m.receiverOffset()))\n                                .as(sender::send)\n                                .doOnNext(m -> m.correlationMetadata().acknowledge())\n                                .doOnCancel(() -> close());\n        }\n        public ProducerRecord<Integer, Person> transform(Person p) {\n            Person transformed = new Person(p.id(), p.firstName(), p.lastName());\n            transformed.email(p.firstName().toLowerCase(Locale.ROOT) + \"@kafka.io\");\n            return new ProducerRecord<>(destTopic, p.id(), transformed);\n        }\n    }\n    /**\n     * This sample demonstrates a flow with at-most once delivery. A topic with replication factor one\n     * combined with a producer with acks=0 and no retries ensures that messages that could not be sent\n     * to Kafka on the first attempt are dropped. On the consumer side, {@link KafkaReceiver#receiveAtmostOnce()}\n     * commits offsets before delivery to the application to ensure that if the consumer restarts,\n     * messages are not redelivered.\n     */\n    public static class AtmostOnce extends AbstractScenario {\n        private final String sourceTopic;\n        private final String destTopic;\n        public AtmostOnce(String bootstrapServers, String sourceTopic, String destTopic) {\n            super(bootstrapServers);\n            this.sourceTopic = sourceTopic;\n            this.destTopic = destTopic;\n        }\n        @Override\n        public SenderOptions<Integer, Person> senderOptions() {\n            return super.senderOptions()\n                        .producerProperty(ProducerConfig.ACKS_CONFIG, \"0\")\n                        .producerProperty(ProducerConfig.RETRIES_CONFIG, \"0\")\n                        .stopOnError(false);\n        }\n        public Flux<?> flux() {\n            KafkaSender<Integer, Person> sender = sender(senderOptions());\n            return KafkaReceiver.create(receiverOptions(Collections.singleton(sourceTopic)))\n                                .receiveAtmostOnce()\n                                .map(cr -> SenderRecord.create(transform(cr.value()), cr.offset()))\n                                .as(sender::send)\n                                .doOnCancel(() -> close());\n        }\n        public ProducerRecord<Integer, Person> transform(Person p) {\n            Person transformed = new Person(p.id(), p.firstName(), p.lastName());\n            transformed.email(p.firstName().toLowerCase(Locale.ROOT) + \"@kafka.io\");\n            return new ProducerRecord<>(destTopic, p.id(), transformed);\n        }\n    }\n    /**\n     * This sample demonstrates the use of transactions to send data to multiple topic partitions,\n     * such that if one of the sends fails, the transaction is aborted and the data from uncommitted\n     * transactions are not visible to consumers configured with {@link ConsumerConfig#ISOLATION_LEVEL_CONFIG}\n     * <code>read_committed</code>.\n     *\n     */\n    public static class TransactionalSend extends AbstractScenario {\n        private final String destTopic1;\n        private final String destTopic2;\n        private Scheduler scheduler = Schedulers.newSingle(\"transaction-scheduler\", true);\n        public TransactionalSend(String bootstrapServers, String destTopic1, String destTopic2) {\n            super(bootstrapServers);\n            this.destTopic1 = destTopic1;\n            this.destTopic2 = destTopic2;\n        }\n        @Override\n        public SenderOptions<Integer, Person> senderOptions() {\n            return super.senderOptions()\n                        .producerProperty(ProducerConfig.ACKS_CONFIG, \"all\")\n                        .producerProperty(ProducerConfig.TRANSACTIONAL_ID_CONFIG, \"TransactionalSend\");\n        }\n<fim_suffix>        @Override\n        public Flux<?> flux() {\n            sender = sender(senderOptions());\n            Flux<Person> srcFlux = source().flux();\n            return sender\n                    .sendTransactionally(srcFlux.map(p -> records(p)))\n                    .concatMap(r -> r)\n                    .doOnNext(r -> log.info(\"Sent record successfully {}\", r))\n                    .doOnError(e -> log.error(\"Send failed, terminating.\", e))\n                    .doOnCancel(() -> close());\n        }<fim_middle>// function below has no smell\n"}