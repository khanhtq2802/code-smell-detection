{"text": "<fim_prefix>    writeSchemaToFile(schema, filePath, null, fs, overwrite, perm);\n  }\n  /**\n   * Write a schema to a file\n   * @param schema the schema\n   * @param filePath the target file\n   * @param tempFilePath if not null then this path is used for a temporary file used to stage the write\n   * @param fs a {@link FileSystem}\n   * @param overwrite should any existing target file be overwritten?\n   * @param perm permissions\n   * @throws IOException\n   */\n  public static void writeSchemaToFile(Schema schema, Path filePath, Path tempFilePath, FileSystem fs, boolean overwrite,\n      FsPermission perm)\n      throws IOException {\n    boolean fileExists = fs.exists(filePath);\n    if (!overwrite) {\n      Preconditions.checkState(!fileExists, filePath + \" already exists\");\n    } else {\n      // delete the target file now if not using a staging file\n      if (fileExists && null == tempFilePath) {\n        HadoopUtils.deletePath(fs, filePath, true);\n        // file has been removed\n        fileExists = false;\n      }\n    }\n    // If the file exists then write to a temp file to make the replacement as close to atomic as possible\n    Path writeFilePath = fileExists ? tempFilePath : filePath;\n    try (DataOutputStream dos = fs.create(writeFilePath)) {\n      dos.writeChars(schema.toString());\n    }\n    fs.setPermission(writeFilePath, perm);\n    // Replace existing file with the staged file\n    if (fileExists) {\n      if (!fs.delete(filePath, true)) {\n        throw new IOException(\n            String.format(\"Failed to delete %s while renaming %s to %s\", filePath, tempFilePath, filePath));\n      }\n      HadoopUtils.movePath(fs, tempFilePath, fs, filePath, true, fs.getConf());\n    }\n  }\n  /**\n   * Get the latest avro schema for a directory\n   * @param directory the input dir that contains avro files\n   * @param fs the {@link FileSystem} for the given directory.\n   * @param latest true to return latest schema, false to return oldest schema\n   * @return the latest/oldest schema in the directory\n   * @throws IOException\n   */\n  public static Schema getDirectorySchema(Path directory, FileSystem fs, boolean latest) throws IOException {\n    Schema schema = null;\n    try (Closer closer = Closer.create()) {\n      List<FileStatus> files = getDirectorySchemaHelper(directory, fs);\n      if (files == null || files.size() == 0) {\n        LOG.warn(\"There is no previous avro file in the directory: \" + directory);\n      } else {\n        FileStatus file = latest ? files.get(0) : files.get(files.size() - 1);\n        LOG.debug(\"Path to get the avro schema: \" + file);\n        FsInput fi = new FsInput(file.getPath(), fs.getConf());\n        GenericDatumReader<GenericRecord> genReader = new GenericDatumReader<>();\n        schema = closer.register(new DataFileReader<>(fi, genReader)).getSchema();\n      }\n    } catch (IOException ioe) {\n      throw new IOException(\"Cannot get the schema for directory \" + directory, ioe);\n    }\n    return schema;\n  }\n  /**\n   * Get the latest avro schema for a directory\n   * @param directory the input dir that contains avro files\n   * @param conf configuration\n   * @param latest true to return latest schema, false to return oldest schema\n   * @return the latest/oldest schema in the directory\n   * @throws IOException\n   */\n  public static Schema getDirectorySchema(Path directory, Configuration conf, boolean latest) throws IOException {\n    return getDirectorySchema(directory, FileSystem.get(conf), latest);\n  }\n  private static List<FileStatus> getDirectorySchemaHelper(Path directory, FileSystem fs) throws IOException {\n    List<FileStatus> files = Lists.newArrayList();\n    if (fs.exists(directory)) {\n      getAllNestedAvroFiles(fs.getFileStatus(directory), files, fs);\n      if (files.size() > 0) {\n        Collections.sort(files, FileListUtils.LATEST_MOD_TIME_ORDER);\n      }\n    }\n    return files;\n  }\n  private static void getAllNestedAvroFiles(FileStatus dir, List<FileStatus> files, FileSystem fs) throws IOException {\n    if (dir.isDirectory()) {\n      FileStatus[] filesInDir = fs.listStatus(dir.getPath());\n      if (filesInDir != null) {\n        for (FileStatus f : filesInDir) {\n          getAllNestedAvroFiles(f, files, fs);\n        }\n      }\n    } else if (dir.getPath().getName().endsWith(AVRO_SUFFIX)) {\n      files.add(dir);\n    }\n  }\n  /**\n   * Merge oldSchema and newSchame. Set a field default value to null, if this field exists in the old schema but not in the new schema.\n   * @param oldSchema\n   * @param newSchema\n   * @return schema that contains all the fields in both old and new schema.\n   */\n  public static Schema nullifyFieldsForSchemaMerge(Schema oldSchema, Schema newSchema) {\n    if (oldSchema == null) {\n      LOG.warn(\"No previous schema available, use the new schema instead.\");\n      return newSchema;\n    }\n    if (!(oldSchema.getType().equals(Type.RECORD) && newSchema.getType().equals(Type.RECORD))) {\n      LOG.warn(\"Both previous schema and new schema need to be record type. Quit merging schema.\");\n      return newSchema;\n    }\n    List<Field> combinedFields = Lists.newArrayList();\n    for (Field newFld : newSchema.getFields()) {\n      combinedFields.add(new Field(newFld.name(), newFld.schema(), newFld.doc(), newFld.defaultValue()));\n    }\n    for (Field oldFld : oldSchema.getFields()) {\n      if (newSchema.getField(oldFld.name()) == null) {\n        List<Schema> union = Lists.newArrayList();\n        Schema oldFldSchema = oldFld.schema();\n        if (oldFldSchema.getType().equals(Type.UNION)) {\n          union.add(Schema.create(Type.NULL));\n          for (Schema itemInUion : oldFldSchema.getTypes()) {\n            if (!itemInUion.getType().equals(Type.NULL)) {\n              union.add(itemInUion);\n            }\n          }\n          Schema newFldSchema = Schema.createUnion(union);\n          combinedFields.add(new Field(oldFld.name(), newFldSchema, oldFld.doc(), oldFld.defaultValue()));\n        } else {\n          union.add(Schema.create(Type.NULL));\n          union.add(oldFldSchema);\n          Schema newFldSchema = Schema.createUnion(union);\n          combinedFields.add(new Field(oldFld.name(), newFldSchema, oldFld.doc(), oldFld.defaultValue()));\n        }\n      }\n    }\n    Schema mergedSchema =\n        Schema.createRecord(newSchema.getName(), newSchema.getDoc(), newSchema.getNamespace(), newSchema.isError());\n    mergedSchema.setFields(combinedFields);\n    return mergedSchema;\n  }\n  /**\n   * Remove map, array, enum fields, as well as union fields that contain map, array or enum,\n   * from an Avro schema. A schema with these fields cannot be used as Mapper key in a\n   * MapReduce job.\n   */\n  public static Optional<Schema> removeUncomparableFields(Schema schema) {\n    return removeUncomparableFields(schema, Sets.<Schema> newHashSet());\n  }\n  private static Optional<Schema> removeUncomparableFields(Schema schema, Set<Schema> processed) {\n    switch (schema.getType()) {\n      case RECORD:\n        return removeUncomparableFieldsFromRecord(schema, processed);\n      case UNION:\n        return removeUncomparableFieldsFromUnion(schema, processed);\n      case MAP:\n        return Optional.absent();\n      case ARRAY:\n        return Optional.absent();\n      case ENUM:\n        return Optional.absent();\n      default:\n        return Optional.of(schema);\n    }\n  }\n<fim_suffix>  private static Optional<Schema> removeUncomparableFieldsFromRecord(Schema record, Set<Schema> processed) {\n    Preconditions.checkArgument(record.getType() == Schema.Type.RECORD);\n    if (processed.contains(record)) {\n      return Optional.absent();\n    }\n    processed.add(record);\n    List<Field> fields = Lists.newArrayList();\n    for (Field field : record.getFields()) {\n      Optional<Schema> newFieldSchema = removeUncomparableFields(field.schema(), processed);\n      if (newFieldSchema.isPresent()) {\n        fields.add(new Field(field.name(), newFieldSchema.get(), field.doc(), field.defaultValue()));\n      }\n    }\n    Schema newSchema = Schema.createRecord(record.getName(), record.getDoc(), record.getNamespace(), false);\n    newSchema.setFields(fields);\n    return Optional.of(newSchema);\n  }<fim_middle>// function below has no smell\n"}