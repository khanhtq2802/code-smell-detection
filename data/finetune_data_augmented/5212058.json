{"text": "<fim_prefix>import com.amazonaws.util.json.JSONException;\nimport com.amazonaws.util.json.JSONObject;\n/**\n * Writes documents to CloudSearch.\n */\npublic class CloudSearchIndexWriter implements IndexWriter {\n  private static final Logger LOG = LoggerFactory\n      .getLogger(MethodHandles.lookup().lookupClass());\n  private static final int MAX_SIZE_BATCH_BYTES = 5242880;\n  private static final int MAX_SIZE_DOC_BYTES = 1048576;\n  private static final SimpleDateFormat DATE_FORMAT = new SimpleDateFormat(\n      \"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\");\n  private AmazonCloudSearchDomainClient client;\n  private int maxDocsInBatch = -1;\n  private StringBuffer buffer;\n  private int numDocsInBatch = 0;\n  private boolean dumpBatchFilesToTemp = false;\n  private Configuration conf;\n  private Map<String, String> csfields = new HashMap<String, String>();\n  private String endpoint;\n  private String regionName;\n  @Override\n  public void open(Configuration conf, String name) throws IOException {\n    //Implementation not required\n  }\n  @Override\n  public void open(IndexWriterParams parameters) throws IOException {\n    //    LOG.debug(\"CloudSearchIndexWriter.open() name={} \", name);\n    endpoint = parameters.get(CloudSearchConstants.ENDPOINT);\n    dumpBatchFilesToTemp = parameters\n        .getBoolean(CloudSearchConstants.BATCH_DUMP, false);\n    this.regionName = parameters.get(CloudSearchConstants.REGION);\n    if (StringUtils.isBlank(endpoint) && !dumpBatchFilesToTemp) {\n      String message = \"Missing CloudSearch endpoint. Should set it set via -D \"\n          + CloudSearchConstants.ENDPOINT + \" or in nutch-site.xml\";\n      message += \"\\n\" + describe();\n      LOG.error(message);\n      throw new RuntimeException(message);\n    }\n    maxDocsInBatch = parameters.getInt(CloudSearchConstants.MAX_DOCS_BATCH, -1);\n    buffer = new StringBuffer(MAX_SIZE_BATCH_BYTES).append('[');\n    if (dumpBatchFilesToTemp) {\n      // only dumping to local file\n      // no more config required\n      return;\n    }\n    if (StringUtils.isBlank(endpoint)) {\n      throw new RuntimeException(\"endpoint not set for CloudSearch\");\n    }\n    AmazonCloudSearchClient cl = new AmazonCloudSearchClient();\n    if (StringUtils.isNotBlank(regionName)) {\n      cl.setRegion(RegionUtils.getRegion(regionName));\n    }\n    String domainName = null;\n    // retrieve the domain name\n    DescribeDomainsResult domains = cl\n        .describeDomains(new DescribeDomainsRequest());\n    Iterator<DomainStatus> dsiter = domains.getDomainStatusList().iterator();\n    while (dsiter.hasNext()) {\n      DomainStatus ds = dsiter.next();\n      if (ds.getDocService().getEndpoint().equals(endpoint)) {\n        domainName = ds.getDomainName();\n        break;\n      }\n    }\n    // check domain name\n    if (StringUtils.isBlank(domainName)) {\n      throw new RuntimeException(\n          \"No domain name found for CloudSearch endpoint\");\n    }\n    DescribeIndexFieldsResult indexDescription = cl.describeIndexFields(\n        new DescribeIndexFieldsRequest().withDomainName(domainName));\n    for (IndexFieldStatus ifs : indexDescription.getIndexFields()) {\n      String indexname = ifs.getOptions().getIndexFieldName();\n      String indextype = ifs.getOptions().getIndexFieldType();\n      LOG.info(\"CloudSearch index name {} of type {}\", indexname, indextype);\n      csfields.put(indexname, indextype);\n    }\n    client = new AmazonCloudSearchDomainClient();\n    client.setEndpoint(endpoint);\n  }\n  @Override\n  public void delete(String url) throws IOException {\n    try {\n      JSONObject doc_builder = new JSONObject();\n      doc_builder.put(\"type\", \"delete\");\n      // generate the id from the url\n      String ID = CloudSearchUtils.getID(url);\n      doc_builder.put(\"id\", ID);\n      // add to the batch\n      addToBatch(doc_builder.toString(2), url);\n    } catch (JSONException e) {\n      LOG.error(\"Exception caught while building JSON object\", e);\n    }\n  }\n  @Override\n  public void update(NutchDocument doc) throws IOException {\n    write(doc);\n  }\n  @Override\n  public void write(NutchDocument doc) throws IOException {\n    try {\n      JSONObject doc_builder = new JSONObject();\n      doc_builder.put(\"type\", \"add\");\n      String url = doc.getField(\"url\").toString();\n      // generate the id from the url\n      String ID = CloudSearchUtils.getID(url);\n      doc_builder.put(\"id\", ID);\n      JSONObject fields = new JSONObject();\n      for (final Entry<String, NutchField> e : doc) {\n        String fieldname = cleanFieldName(e.getKey());\n        String type = csfields.get(fieldname);\n        // undefined in index\n        if (!dumpBatchFilesToTemp && type == null) {\n          LOG.info(\n              \"Field {} not defined in CloudSearch domain for {} - skipping.\",\n              fieldname, url);\n          continue;\n        }\n        List<Object> values = e.getValue().getValues();\n        // write the values\n        for (Object value : values) {\n          // Convert dates to an integer\n          if (value instanceof Date) {\n            Date d = (Date) value;\n            value = DATE_FORMAT.format(d);\n          }\n          // normalise strings\n          else if (value instanceof String) {\n            value = CloudSearchUtils.stripNonCharCodepoints((String) value);\n          }\n          fields.accumulate(fieldname, value);\n        }\n      }\n      doc_builder.put(\"fields\", fields);\n      addToBatch(doc_builder.toString(2), url);\n    } catch (JSONException e) {\n      LOG.error(\"Exception caught while building JSON object\", e);\n    }\n  }\n  private void addToBatch(String currentDoc, String url) throws IOException {\n    int currentDocLength = currentDoc.getBytes(StandardCharsets.UTF_8).length;\n    // check that the doc is not too large -> skip it if it does\n    if (currentDocLength > MAX_SIZE_DOC_BYTES) {\n      LOG.error(\"Doc too large. currentDoc.length {} : {}\", currentDocLength,\n          url);\n      return;\n    }\n    int currentBufferLength = buffer.toString()\n        .getBytes(StandardCharsets.UTF_8).length;\n    LOG.debug(\"currentDoc.length {}, buffer length {}\", currentDocLength,\n        currentBufferLength);\n    // can add it to the buffer without overflowing?\n    if (currentDocLength + 2 + currentBufferLength < MAX_SIZE_BATCH_BYTES) {\n      if (numDocsInBatch != 0)\n        buffer.append(',');\n      buffer.append(currentDoc);\n      numDocsInBatch++;\n    }\n    // flush the previous batch and create a new one with this doc\n    else {\n      commit();\n      buffer.append(currentDoc);\n      numDocsInBatch++;\n    }\n    // have we reached the max number of docs in a batch after adding\n    // this doc?\n    if (maxDocsInBatch > 0 && numDocsInBatch == maxDocsInBatch) {\n      commit();\n    }\n  }\n  @Override\n  public void commit() throws IOException {\n    // nothing to do\n    if (numDocsInBatch == 0) {\n      return;\n    }\n    // close the array\n    buffer.append(']');\n    LOG.info(\"Sending {} docs to CloudSearch\", numDocsInBatch);\n    byte[] bb = buffer.toString().getBytes(StandardCharsets.UTF_8);\n    if (dumpBatchFilesToTemp) {\n      try {\n        File temp = File.createTempFile(\"CloudSearch_\", \".json\");\n        FileUtils.writeByteArrayToFile(temp, bb);\n        LOG.info(\"Wrote batch file {}\", temp.getName());\n      } catch (IOException e1) {\n        LOG.error(\"Exception while generating batch file\", e1);\n      } finally {\n        // reset buffer and doc counter\n        buffer = new StringBuffer(MAX_SIZE_BATCH_BYTES).append('[');\n        numDocsInBatch = 0;\n      }\n      return;\n    }\n    // not in debug mode\n    try (InputStream inputStream = new ByteArrayInputStream(bb)) {\n      UploadDocumentsRequest batch = new UploadDocumentsRequest();\n      batch.setContentLength((long) bb.length);\n      batch.setContentType(ContentType.Applicationjson);\n      batch.setDocuments(inputStream);\n      @SuppressWarnings(\"unused\")\n      UploadDocumentsResult result = client.uploadDocuments(batch);\n    } catch (Exception e) {\n      LOG.error(\"Exception while sending batch\", e);\n      LOG.error(buffer.toString());\n    } finally {\n      // reset buffer and doc counter\n      buffer = new StringBuffer(MAX_SIZE_BATCH_BYTES).append('[');\n      numDocsInBatch = 0;\n    }\n  }\n<fim_suffix>  @Override\n  public void close() throws IOException {\n    // This will flush any unsent documents.\n    commit();\n    // close the client\n    if (client != null) {\n      client.shutdown();\n    }\n  }<fim_middle>// function below has no smell\n"}