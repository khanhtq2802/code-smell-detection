{"text": "<fim_prefix>\n<fim_suffix>public class Exchanger<V> {\n    /*\n     * Overview: The core algorithm is, for an exchange \"slot\",\n     * and a participant (caller) with an item:\n     *\n     * for (;;) {\n     *   if (slot is empty) {                       // offer\n     *     place item in a Node;\n     *     if (can CAS slot from empty to node) {\n     *       wait for release;\n     *       return matching item in node;\n     *     }\n     *   }\n     *   else if (can CAS slot from node to empty) { // release\n     *     get the item in node;\n     *     set matching item in node;\n     *     release waiting thread;\n     *   }\n     *   // else retry on CAS failure\n     * }\n     *\n     * This is among the simplest forms of a \"dual data structure\" --\n     * see Scott and Scherer's DISC 04 paper and\n     * http://www.cs.rochester.edu/research/synchronization/pseudocode/duals.html\n     *\n     * This works great in principle. But in practice, like many\n     * algorithms centered on atomic updates to a single location, it\n     * scales horribly when there are more than a few participants\n     * using the same Exchanger. So the implementation instead uses a\n     * form of elimination arena, that spreads out this contention by\n     * arranging that some threads typically use different slots,\n     * while still ensuring that eventually, any two parties will be\n     * able to exchange items. That is, we cannot completely partition\n     * across threads, but instead give threads arena indices that\n     * will on average grow under contention and shrink under lack of\n     * contention. We approach this by defining the Nodes that we need\n     * anyway as ThreadLocals, and include in them per-thread index\n     * and related bookkeeping state. (We can safely reuse per-thread\n     * nodes rather than creating them fresh each time because slots\n     * alternate between pointing to a node vs null, so cannot\n     * encounter ABA problems. However, we do need some care in\n     * resetting them between uses.)\n     *\n     * Implementing an effective arena requires allocating a bunch of\n     * space, so we only do so upon detecting contention (except on\n     * uniprocessors, where they wouldn't help, so aren't used).\n     * Otherwise, exchanges use the single-slot slotExchange method.\n     * On contention, not only must the slots be in different\n     * locations, but the locations must not encounter memory\n     * contention due to being on the same cache line (or more\n     * generally, the same coherence unit).  Because, as of this\n     * writing, there is no way to determine cacheline size, we define\n     * a value that is enough for common platforms.  Additionally,\n     * extra care elsewhere is taken to avoid other false/unintended\n     * sharing and to enhance locality, including adding padding (via\n     * @Contended) to Nodes, embedding \"bound\" as an Exchanger field.\n     *\n     * The arena starts out with only one used slot. We expand the\n     * effective arena size by tracking collisions; i.e., failed CASes\n     * while trying to exchange. By nature of the above algorithm, the\n     * only kinds of collision that reliably indicate contention are\n     * when two attempted releases collide -- one of two attempted\n     * offers can legitimately fail to CAS without indicating\n     * contention by more than one other thread. (Note: it is possible\n     * but not worthwhile to more precisely detect contention by\n     * reading slot values after CAS failures.)  When a thread has\n     * collided at each slot within the current arena bound, it tries\n     * to expand the arena size by one. We track collisions within\n     * bounds by using a version (sequence) number on the \"bound\"\n     * field, and conservatively reset collision counts when a\n     * participant notices that bound has been updated (in either\n     * direction).\n     *\n     * The effective arena size is reduced (when there is more than\n     * one slot) by giving up on waiting after a while and trying to\n     * decrement the arena size on expiration. The value of \"a while\"\n     * is an empirical matter.  We implement by piggybacking on the\n     * use of spin->yield->block that is essential for reasonable\n     * waiting performance anyway -- in a busy exchanger, offers are\n     * usually almost immediately released, in which case context\n     * switching on multiprocessors is extremely slow/wasteful.  Arena\n     * waits just omit the blocking part, and instead cancel. The spin\n     * count is empirically chosen to be a value that avoids blocking\n     * 99% of the time under maximum sustained exchange rates on a\n     * range of test machines. Spins and yields entail some limited\n     * randomness (using a cheap xorshift) to avoid regular patterns\n     * that can induce unproductive grow/shrink cycles. (Using a\n     * pseudorandom also helps regularize spin cycle duration by\n     * making branches unpredictable.)  Also, during an offer, a\n     * waiter can \"know\" that it will be released when its slot has\n     * changed, but cannot yet proceed until match is set.  In the\n     * mean time it cannot cancel the offer, so instead spins/yields.\n     * Note: It is possible to avoid this secondary check by changing\n     * the linearization point to be a CAS of the match field (as done\n     * in one case in the Scott & Scherer DISC paper), which also\n     * increases asynchrony a bit, at the expense of poorer collision\n     * detection and inability to always reuse per-thread nodes. So\n     * the current scheme is typically a better tradeoff.\n     *\n     * On collisions, indices traverse the arena cyclically in reverse\n     * order, restarting at the maximum index (which will tend to be\n     * sparsest) when bounds change. (On expirations, indices instead\n     * are halved until reaching 0.) It is possible (and has been\n     * tried) to use randomized, prime-value-stepped, or double-hash\n     * style traversal instead of simple cyclic traversal to reduce\n     * bunching.  But empirically, whatever benefits these may have\n     * don't overcome their added overhead: We are managing operations\n     * that occur very quickly unless there is sustained contention,\n     * so simpler/faster control policies work better than more\n     * accurate but slower ones.\n     *\n     * Because we use expiration for arena size control, we cannot\n     * throw TimeoutExceptions in the timed version of the public\n     * exchange method until the arena size has shrunken to zero (or\n     * the arena isn't enabled). This may delay response to timeout\n     * but is still within spec.\n     *\n     * Essentially all of the implementation is in methods\n     * slotExchange and arenaExchange. These have similar overall\n     * structure, but differ in too many details to combine. The\n     * slotExchange method uses the single Exchanger field \"slot\"\n     * rather than arena array elements. However, it still needs\n     * minimal collision detection to trigger arena construction.\n     * (The messiest part is making sure interrupt status and\n     * InterruptedExceptions come out right during transitions when\n     * both methods may be called. This is done by using null return\n     * as a sentinel to recheck interrupt status.)\n     *\n     * As is too common in this sort of code, methods are monolithic\n     * because most of the logic relies on reads of fields that are\n     * maintained as local variables so can't be nicely factored --\n     * mainly, here, bulky spin->yield->block/cancel code.  Note that\n     * field Node.item is not declared as volatile even though it is\n     * read by releasing threads, because they only do so after CAS\n     * operations that must precede access, and all uses by the owning\n     * thread are otherwise acceptably ordered by other operations.\n     * (Because the actual points of atomicity are slot CASes, it\n     * would also be legal for the write to Node.match in a release to\n     * be weaker than a full volatile write. However, this is not done\n     * because it could allow further postponement of the write,\n     * delaying progress.)\n     */\n    /**\n     * The index distance (as a shift value) between any two used slots\n     * in the arena, spacing them out to avoid false sharing.\n     */\n    private static final int ASHIFT = 5;\n    /**\n     * The maximum supported arena index. The maximum allocatable\n     * arena size is MMASK + 1. Must be a power of two minus one, less\n     * than (1<<(31-ASHIFT)). The cap of 255 (0xff) more than suffices<fim_middle>// class below has no smell\n"}