{"text": "<fim_prefix>/**\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.crunch.lib.sort;\n\nimport java.io.IOException;\nimport java.lang.reflect.Array;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.Collections;\nimport java.util.Iterator;\n\nimport org.apache.avro.Schema;\nimport org.apache.avro.mapred.AvroKey;\nimport org.apache.crunch.io.CompositePathIterable;\nimport org.apache.crunch.io.avro.AvroFileReaderFactory;\nimport org.apache.crunch.io.seq.SeqFileReaderFactory;\nimport org.apache.crunch.types.writable.WritableDeepCopier;\nimport org.apache.hadoop.conf.Configurable;\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.FileSystem;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.RawComparator;\nimport org.apache.hadoop.io.Writable;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.Partitioner;\n\n/**\n * A partition-aware {@code Partitioner} instance that can work with either Avro or Writable-formatted\n * keys.\n */\npublic class TotalOrderPartitioner<K, V> extends Partitioner<K, V> implements Configurable {\n\n  public static final String DEFAULT_PATH = \"_partition.lst\";\n  public static final String PARTITIONER_PATH = \n    \"crunch.totalorderpartitioner.path\";\n  \n  private Configuration conf;\n  private Node<K> partitions;\n  \n  @Override\n  public Configuration getConf() {\n    return conf;\n  }\n\n  @Override\n  public void setConf(Configuration conf) {\n    try {\n      this.conf = conf;\n      String parts = getPartitionFile(conf);\n      final Path partFile = new Path(parts);\n      final FileSystem fs = (DEFAULT_PATH.equals(parts))\n        ? FileSystem.getLocal(conf)     // assume in DistributedCache\n        : partFile.getFileSystem(conf);\n\n      Job job = new Job(conf);\n      Class<K> keyClass = (Class<K>)job.getMapOutputKeyClass();\n      RawComparator<K> comparator =\n          (RawComparator<K>) job.getSortComparator();\n      K[] splitPoints = readPartitions(fs, partFile, keyClass, conf, comparator);\n      int numReduceTasks = job.getNumReduceTasks();\n      if (splitPoints.length != numReduceTasks - 1) {\n        throw new IOException(\"Wrong number of partitions in keyset\");\n      }\n      partitions = new BinarySearchNode(splitPoints, comparator);\n    } catch (IOException e) {\n      throw new IllegalArgumentException(\"Can't read partitions file\", e);\n    }\n  }\n\n  @Override\n  public int getPartition(K key, V value, int modulo) {\n    return partitions.findPartition(key);\n  }\n\n  public static void setPartitionFile(Configuration conf, Path p) {\n    conf.set(PARTITIONER_PATH, p.toString());\n  }\n\n  public static String getPartitionFile(Configuration conf) {\n    return conf.get(PARTITIONER_PATH, DEFAULT_PATH);\n  }\n  \n  @SuppressWarnings(\"unchecked\") // map output key class\n  private K[] readPartitions(FileSystem fs, Path p, Class<K> keyClass,\n      Configuration conf, final RawComparator<K> comparator) throws IOException {\n    ArrayList<K> parts = new ArrayList<K>();\n    String schema = conf.get(\"crunch.schema\");\n    if (schema != null) {\n      Schema s = (new Schema.Parser()).parse(schema);\n      AvroFileReaderFactory<K> a = new AvroFileReaderFactory<K>(s);\n      Iterator<K> iter = CompositePathIterable.create(fs, p, a).iterator();\n      while (iter.hasNext()) {\n        parts.add((K) new AvroKey<K>(iter.next()));\n      }\n    } else {\n      WritableDeepCopier wdc = new WritableDeepCopier(keyClass);\n      SeqFileReaderFactory<K> s = new SeqFileReaderFactory<K>(keyClass);\n      Iterator<K> iter = CompositePathIterable.create(fs, p, s).iterator();\n      while (iter.hasNext()) {\n        parts.add((K) wdc.deepCopy((Writable) iter.next()));\n      }\n    }\n    Collections.sort(parts, comparator);\n    return parts.toArray((K[])Array.newInstance(keyClass, parts.size()));\n  }\n  \n  /**\n   * Interface to the partitioner to locate a key in the partition keyset.\n   */\n  public interface Node<T> {\n    /**\n     * Locate partition in keyset K, st [Ki..Ki+1) defines a partition,\n     * with implicit K0 = -inf, Kn = +inf, and |K| = #partitions - 1.\n     */\n    int findPartition(T key);\n  }\n\n  public static class BinarySearchNode<K> implements Node<K> {\n    private final K[] splitPoints;\n    private final RawComparator<K> comparator;\n<fim_suffix>    public BinarySearchNode(K[] splitPoints, RawComparator<K> comparator) {\n      this.splitPoints = splitPoints;\n      this.comparator = comparator;\n    }\n    public int findPartition(K key) {\n      final int pos = Arrays.binarySearch(splitPoints, key, comparator) + 1;\n      return (pos < 0) ? -pos : pos;\n    }\n  }\n}<fim_middle>// function below has no smell\n"}