{"text": "<fim_prefix>                           metricSampleCount.incrementAndGet();\n                         } else {\n                           LOG.error(\"Failed to produce partition metric sample for {} of timestamp {} due to exception\",\n                                     sample.entity().tp(), sample.sampleTime(), e);\n                         }\n                       }\n                     });\n    }\n    final AtomicInteger brokerMetricSampleCount = new AtomicInteger(0);\n    for (BrokerMetricSample sample : samples.brokerMetricSamples()) {\n      _producer.send(new ProducerRecord<>(_brokerMetricSampleStoreTopic, sample.toBytes()),\n                     new Callback() {\n                       @Override\n                       public void onCompletion(RecordMetadata recordMetadata, Exception e) {\n                         if (e == null) {\n                           brokerMetricSampleCount.incrementAndGet();\n                         } else {\n                           LOG.error(\"Failed to produce model training sample due to exception\", e);\n                         }\n                       }\n                     });\n    }\n    _producer.flush();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Stored {} partition metric samples and {} broker metric samples to Kafka\",\n                metricSampleCount.get(), brokerMetricSampleCount.get());\n    }\n  }\n  @Override\n  public void loadSamples(SampleLoader sampleLoader) {\n    LOG.info(\"Starting loading samples.\");\n    long startMs = System.currentTimeMillis();\n    AtomicLong numPartitionMetricSamples = new AtomicLong(0L);\n    AtomicLong numBrokerMetricSamples = new AtomicLong(0L);\n    AtomicLong totalSamples = new AtomicLong(0L);\n    AtomicLong numLoadedSamples = new AtomicLong(0L);\n    try {\n      prepareConsumers();\n      for (KafkaConsumer<byte[], byte[]> consumer : _consumers) {\n        _metricProcessorExecutor.submit(\n            new MetricLoader(consumer, sampleLoader, numLoadedSamples, numPartitionMetricSamples, numBrokerMetricSamples,\n                             totalSamples));\n      }\n      // Blocking waiting for the metric loading to finish.\n      _metricProcessorExecutor.shutdown();\n      _metricProcessorExecutor.awaitTermination(Long.MAX_VALUE, TimeUnit.MILLISECONDS);\n    } catch (Exception e) {\n      LOG.error(\"Received exception when loading samples\", e);\n    } finally {\n      _consumers.forEach(Consumer::close);\n      try {\n        _metricProcessorExecutor.awaitTermination(30000, TimeUnit.MILLISECONDS);\n      } catch (InterruptedException e) {\n        LOG.warn(\"Interrupted during waiting for metrics processor to shutdown.\");\n      }\n    }\n    long endMs = System.currentTimeMillis();\n    long addedPartitionSampleCount = sampleLoader.partitionSampleCount();\n    long addedBrokerSampleCount = sampleLoader.brokerSampleCount();\n    long discardedPartitionMetricSamples = numPartitionMetricSamples.get() - addedPartitionSampleCount;\n    long discardedBrokerMetricSamples = numBrokerMetricSamples.get() - addedBrokerSampleCount;\n    LOG.info(\"Sample loading finished. Loaded {}{} partition metrics samples and {}{} broker metric samples in {} ms.\",\n             addedPartitionSampleCount,\n             discardedPartitionMetricSamples > 0 ? String.format(\"(%d discarded)\", discardedPartitionMetricSamples) : \"\",\n             sampleLoader.brokerSampleCount(),\n             discardedBrokerMetricSamples > 0 ? String.format(\"(%d discarded)\", discardedBrokerMetricSamples) : \"\",\n             endMs - startMs);\n  }\n  @Override\n  public double sampleLoadingProgress() {\n    return _loadingProgress;\n  }\n  @Override\n  public void evictSamplesBefore(long timestamp) {\n    //TODO: use the deleteMessageBefore method to delete old samples.\n  }\n  @Override\n  public void close() {\n    _shutdown = true;\n    _producer.close(300L, TimeUnit.SECONDS);\n  }\n  private void prepareConsumers() {\n    int numConsumers = _consumers.size();\n    List<List<TopicPartition>> assignments = new ArrayList<>();\n    for (int i = 0; i < numConsumers; i++) {\n      assignments.add(new ArrayList<>());\n    }\n    int j = 0;\n    for (String topic : Arrays.asList(_partitionMetricSampleStoreTopic, _brokerMetricSampleStoreTopic)) {\n      for (PartitionInfo partInfo : _consumers.get(0).partitionsFor(topic)) {\n        assignments.get(j++ % numConsumers).add(new TopicPartition(partInfo.topic(), partInfo.partition()));\n      }\n    }\n    for (int i = 0; i < numConsumers; i++) {\n      _consumers.get(i).assign(assignments.get(i));\n    }\n  }\n  private class MetricLoader implements Runnable {\n    private final SampleLoader _sampleLoader;\n    private final AtomicLong _numLoadedSamples;\n    private final AtomicLong _numPartitionMetricSamples;\n    private final AtomicLong _numBrokerMetricSamples;\n    private final AtomicLong _totalSamples;\n    private final KafkaConsumer<byte[], byte[]> _consumer;\n    MetricLoader(KafkaConsumer<byte[], byte[]> consumer,\n                 SampleLoader sampleLoader,\n                 AtomicLong numLoadedSamples,\n                 AtomicLong numPartitionMetricSamples,\n                 AtomicLong numBrokerMetricSamples,\n                 AtomicLong totalSamples) {\n      _consumer = consumer;\n      _sampleLoader = sampleLoader;\n      _numLoadedSamples = numLoadedSamples;\n      _numPartitionMetricSamples = numPartitionMetricSamples;\n      _numBrokerMetricSamples = numBrokerMetricSamples;\n      _totalSamples = totalSamples;\n    }\n    @Override\n    public void run() {\n      try {\n        prepareConsumerOffset();\n        Map<TopicPartition, Long> beginningOffsets = _consumer.beginningOffsets(_consumer.assignment());\n        Map<TopicPartition, Long> endOffsets = _consumer.endOffsets(_consumer.assignment());\n        LOG.debug(\"Loading beginning offsets: {}, loading end offsets: {}\", beginningOffsets, endOffsets);\n        for (Map.Entry<TopicPartition, Long> entry : beginningOffsets.entrySet()) {\n          _totalSamples.addAndGet(endOffsets.get(entry.getKey()) - entry.getValue());\n          _loadingProgress = (double) _numLoadedSamples.get() / _totalSamples.get();\n        }\n        while (!sampleLoadingFinished(endOffsets)) {\n          try {\n            ConsumerRecords<byte[], byte[]> consumerRecords = _consumer.poll(SAMPLE_POLL_TIMEOUT);\n            if (consumerRecords == SHUTDOWN_RECORDS) {\n              LOG.trace(\"Metric loader received empty records\");\n              return;\n            }\n            Set<PartitionMetricSample> partitionMetricSamples = new HashSet<>();\n            Set<BrokerMetricSample> brokerMetricSamples = new HashSet<>();\n            for (ConsumerRecord<byte[], byte[]> record : consumerRecords) {\n              try {\n                if (record.topic().equals(_partitionMetricSampleStoreTopic)) {\n                  PartitionMetricSample sample = PartitionMetricSample.fromBytes(record.value());\n                  partitionMetricSamples.add(sample);\n                  LOG.trace(\"Loaded partition metric sample {}\", sample);\n                } else if (record.topic().equals(_brokerMetricSampleStoreTopic)) {\n                  BrokerMetricSample sample = BrokerMetricSample.fromBytes(record.value());\n                  // For some legacy BrokerMetricSample, there is no timestamp in the broker samples. In this case\n                  // we use the record timestamp as the broker metric timestamp.\n                  sample.close(record.timestamp());\n                  brokerMetricSamples.add(sample);\n                  LOG.trace(\"Loaded broker metric sample {}\", sample);\n                }\n              } catch (UnknownVersionException e) {\n                LOG.warn(\"Ignoring sample due to\", e);\n              }\n            }\n            if (!partitionMetricSamples.isEmpty() || !brokerMetricSamples.isEmpty()) {\n              _sampleLoader.loadSamples(new MetricSampler.Samples(partitionMetricSamples, brokerMetricSamples));\n              _numPartitionMetricSamples.getAndAdd(partitionMetricSamples.size());\n              _numBrokerMetricSamples.getAndAdd(brokerMetricSamples.size());\n              _loadingProgress = (double) _numLoadedSamples.addAndGet(consumerRecords.count()) / _totalSamples.get();\n            }\n          } catch (KafkaException ke) {\n            if (ke.getMessage().toLowerCase().contains(\"record is corrupt\")) {\n              for (TopicPartition tp : _consumer.assignment()) {\n                long position = _consumer.position(tp);\n                if (position < endOffsets.get(tp)) {\n                  _consumer.seek(tp, position + 1);\n                }\n              }\n            } else {\n              LOG.error(\"Metric loader received exception:\", ke);\n            }\n          } catch (Exception e) {\n            if (_shutdown) {\n              return;\n            } else {\n              LOG.error(\"Metric loader received exception:\", e);\n            }\n          }\n        }\n        LOG.info(\"Metric loader finished loading samples.\");\n      } catch (Throwable t) {\n        LOG.warn(\"Encountered error when loading sample from Kafka.\", t);\n      }\n    }\n<fim_suffix>    private boolean sampleLoadingFinished(Map<TopicPartition, Long> endOffsets) {\n      for (Map.Entry<TopicPartition, Long> entry : endOffsets.entrySet()) {\n        long position = _consumer.position(entry.getKey());\n        if (position < entry.getValue()) {\n          LOG.debug(\"Partition {} is still lagging. Current position: {}, LEO: {}\", entry.getKey(),\n                    position, entry.getValue());\n          return false;\n        }\n      }\n      return true;\n    }<fim_middle>// function below has no smell\n"}