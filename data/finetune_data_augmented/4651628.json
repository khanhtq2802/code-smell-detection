{"text": "<fim_prefix>\t\t@Nonnull DataOutputSerializer outputStream,\n\t\t@Nonnull DataInputDeserializer inputStream,\n\t\t@Nonnull RocksDBWriteBatchWrapper batchWrapper,\n\t\t@Nonnull OrderedByteArraySetCache orderedByteArraySetCache) {\n\t\tthis.db = db;\n\t\tthis.columnFamilyHandle = columnFamilyHandle;\n\t\tthis.byteOrderProducingSerializer = byteOrderProducingSerializer;\n\t\tthis.batchWrapper = batchWrapper;\n\t\tthis.outputView = outputStream;\n\t\tthis.inputView = inputStream;\n\t\tthis.orderedCache = orderedByteArraySetCache;\n\t\tthis.allElementsInCache = false;\n\t\tthis.groupPrefixBytes = createKeyGroupBytes(keyGroupId, keyGroupPrefixBytes);\n\t\tthis.seekHint = groupPrefixBytes;\n\t\tthis.internalIndex = HeapPriorityQueueElement.NOT_CONTAINED;\n\t}\n\t@Nullable\n\t@Override\n\tpublic E peek() {\n\t\tcheckRefillCacheFromStore();\n\t\tif (peekCache != null) {\n\t\t\treturn peekCache;\n\t\t}\n\t\tbyte[] firstBytes = orderedCache.peekFirst();\n\t\tif (firstBytes != null) {\n\t\t\tpeekCache = deserializeElement(firstBytes);\n\t\t\treturn peekCache;\n\t\t} else {\n\t\t\treturn null;\n\t\t}\n\t}\n\t@Nullable\n\t@Override\n\tpublic E poll() {\n\t\tcheckRefillCacheFromStore();\n\t\tfinal byte[] firstBytes = orderedCache.pollFirst();\n\t\tif (firstBytes == null) {\n\t\t\treturn null;\n\t\t}\n\t\t// write-through sync\n\t\tremoveFromRocksDB(firstBytes);\n\t\tif (orderedCache.isEmpty()) {\n\t\t\tseekHint = firstBytes;\n\t\t}\n\t\tif (peekCache != null) {\n\t\t\tE fromCache = peekCache;\n\t\t\tpeekCache = null;\n\t\t\treturn fromCache;\n\t\t} else {\n\t\t\treturn deserializeElement(firstBytes);\n\t\t}\n\t}\n\t@Override\n\tpublic boolean add(@Nonnull E toAdd) {\n\t\tcheckRefillCacheFromStore();\n\t\tfinal byte[] toAddBytes = serializeElement(toAdd);\n\t\tfinal boolean cacheFull = orderedCache.isFull();\n\t\tif ((!cacheFull && allElementsInCache) ||\n\t\t\tLEXICOGRAPHIC_BYTE_COMPARATOR.compare(toAddBytes, orderedCache.peekLast()) < 0) {\n\t\t\tif (cacheFull) {\n\t\t\t\t// we drop the element with lowest priority from the cache\n\t\t\t\torderedCache.pollLast();\n\t\t\t\t// the dropped element is now only in the store\n\t\t\t\tallElementsInCache = false;\n\t\t\t}\n\t\t\tif (orderedCache.add(toAddBytes)) {\n\t\t\t\t// write-through sync\n\t\t\t\taddToRocksDB(toAddBytes);\n\t\t\t\tif (toAddBytes == orderedCache.peekFirst()) {\n\t\t\t\t\tpeekCache = null;\n\t\t\t\t\treturn true;\n\t\t\t\t}\n\t\t\t}\n\t\t} else {\n\t\t\t// we only added to the store\n\t\t\taddToRocksDB(toAddBytes);\n\t\t\tallElementsInCache = false;\n\t\t}\n\t\treturn false;\n\t}\n\t@Override\n\tpublic boolean remove(@Nonnull E toRemove) {\n\t\tcheckRefillCacheFromStore();\n\t\tfinal byte[] oldHead = orderedCache.peekFirst();\n\t\tif (oldHead == null) {\n\t\t\treturn false;\n\t\t}\n\t\tfinal byte[] toRemoveBytes = serializeElement(toRemove);\n\t\t// write-through sync\n\t\tremoveFromRocksDB(toRemoveBytes);\n\t\torderedCache.remove(toRemoveBytes);\n\t\tif (orderedCache.isEmpty()) {\n\t\t\tseekHint = toRemoveBytes;\n\t\t\tpeekCache = null;\n\t\t\treturn true;\n\t\t}\n\t\tif (oldHead != orderedCache.peekFirst()) {\n\t\t\tpeekCache = null;\n\t\t\treturn true;\n\t\t}\n\t\treturn false;\n\t}\n\t@Override\n\tpublic void addAll(@Nullable Collection<? extends E> toAdd) {\n\t\tif (toAdd == null) {\n\t\t\treturn;\n\t\t}\n\t\tfor (E element : toAdd) {\n\t\t\tadd(element);\n\t\t}\n\t}\n\t@Override\n\tpublic boolean isEmpty() {\n\t\tcheckRefillCacheFromStore();\n\t\treturn orderedCache.isEmpty();\n\t}\n\t@Nonnull\n\t@Override\n\tpublic CloseableIterator<E> iterator() {\n\t\treturn new DeserializingIteratorWrapper(orderedBytesIterator());\n\t}\n\t/**\n\t * This implementation comes at a relatively high cost per invocation. It should not be called repeatedly when it is\n\t * clear that the value did not change. Currently this is only truly used to realize certain higher-level tests.\n\t */\n\t@Override\n\tpublic int size() {\n\t\tif (allElementsInCache) {\n\t\t\treturn orderedCache.size();\n\t\t} else {\n\t\t\tint count = 0;\n\t\t\ttry (final RocksBytesIterator iterator = orderedBytesIterator()) {\n\t\t\t\twhile (iterator.hasNext()) {\n\t\t\t\t\titerator.next();\n\t\t\t\t\t++count;\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn count;\n\t\t}\n\t}\n\t@Override\n\tpublic int getInternalIndex() {\n\t\treturn internalIndex;\n\t}\n\t@Override\n\tpublic void setInternalIndex(int newIndex) {\n\t\tthis.internalIndex = newIndex;\n\t}\n\t@Nonnull\n\tprivate RocksBytesIterator orderedBytesIterator() {\n\t\tflushWriteBatch();\n\t\treturn new RocksBytesIterator(\n\t\t\tnew RocksIteratorWrapper(\n\t\t\t\tdb.newIterator(columnFamilyHandle)));\n\t}\n\t/**\n\t * Ensures that recent writes are flushed and reflect in the RocksDB instance.\n\t */\n\tprivate void flushWriteBatch() {\n\t\ttry {\n\t\t\tbatchWrapper.flush();\n\t\t} catch (RocksDBException e) {\n\t\t\tthrow new FlinkRuntimeException(e);\n\t\t}\n\t}\n\tprivate void addToRocksDB(@Nonnull byte[] toAddBytes) {\n\t\ttry {\n\t\t\tbatchWrapper.put(columnFamilyHandle, toAddBytes, DUMMY_BYTES);\n\t\t} catch (RocksDBException e) {\n\t\t\tthrow new FlinkRuntimeException(e);\n\t\t}\n\t}\n\tprivate void removeFromRocksDB(@Nonnull byte[] toRemoveBytes) {\n\t\ttry {\n\t\t\tbatchWrapper.remove(columnFamilyHandle, toRemoveBytes);\n\t\t} catch (RocksDBException e) {\n\t\t\tthrow new FlinkRuntimeException(e);\n\t\t}\n\t}\n\tprivate void checkRefillCacheFromStore() {\n\t\tif (!allElementsInCache && orderedCache.isEmpty()) {\n\t\t\ttry (final RocksBytesIterator iterator = orderedBytesIterator()) {\n\t\t\t\torderedCache.bulkLoadFromOrderedIterator(iterator);\n\t\t\t\tallElementsInCache = !iterator.hasNext();\n\t\t\t} catch (Exception e) {\n\t\t\t\tthrow new FlinkRuntimeException(\"Exception while refilling store from iterator.\", e);\n\t\t\t}\n\t\t}\n\t}\n\tprivate static boolean isPrefixWith(byte[] bytes, byte[] prefixBytes) {\n\t\tfor (int i = 0; i < prefixBytes.length; ++i) {\n\t\t\tif (bytes[i] != prefixBytes[i]) {\n\t\t\t\treturn false;\n\t\t\t}\n\t\t}\n\t\treturn true;\n\t}\n\t@Nonnull\n\tprivate byte[] createKeyGroupBytes(int keyGroupId, int numPrefixBytes) {\n\t\toutputView.clear();\n\t\ttry {\n\t\t\tRocksDBKeySerializationUtils.writeKeyGroup(keyGroupId, numPrefixBytes, outputView);\n\t\t} catch (IOException e) {\n\t\t\tthrow new FlinkRuntimeException(\"Could not write key-group bytes.\", e);\n\t\t}\n\t\treturn outputView.getCopyOfBuffer();\n\t}\n\t@Nonnull\n\tprivate byte[] serializeElement(@Nonnull E element) {\n\t\ttry {\n\t\t\toutputView.clear();\n\t\t\toutputView.write(groupPrefixBytes);\n\t\t\tbyteOrderProducingSerializer.serialize(element, outputView);\n\t\t\treturn outputView.getCopyOfBuffer();\n\t\t} catch (IOException e) {\n\t\t\tthrow new FlinkRuntimeException(\"Error while serializing the element.\", e);\n\t\t}\n\t}\n<fim_suffix>\t@Nonnull\n\tprivate E deserializeElement(@Nonnull byte[] bytes) {\n\t\ttry {\n\t\t\tfinal int numPrefixBytes = groupPrefixBytes.length;\n\t\t\tinputView.setBuffer(bytes, numPrefixBytes, bytes.length - numPrefixBytes);\n\t\t\treturn byteOrderProducingSerializer.deserialize(inputView);\n\t\t} catch (IOException e) {\n\t\t\tthrow new FlinkRuntimeException(\"Error while deserializing the element.\", e);\n\t\t}\n\t}<fim_middle>// function below has no smell\n"}