{"text": "<fim_prefix>/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.lucene.codecs.lucene70;\nimport static org.apache.lucene.codecs.lucene70.Lucene70DocValuesFormat.DIRECT_MONOTONIC_BLOCK_SHIFT;\nimport static org.apache.lucene.codecs.lucene70.Lucene70DocValuesFormat.NUMERIC_BLOCK_SHIFT;\nimport static org.apache.lucene.codecs.lucene70.Lucene70DocValuesFormat.NUMERIC_BLOCK_SIZE;\nimport java.io.Closeable; // javadocs\nimport java.io.IOException;\nimport java.util.Arrays;\nimport java.util.HashMap;\nimport java.util.HashSet;\nimport java.util.Map;\nimport java.util.Set;\nimport org.apache.lucene.codecs.CodecUtil;\nimport org.apache.lucene.codecs.DocValuesConsumer;\nimport org.apache.lucene.codecs.DocValuesProducer;\nimport org.apache.lucene.index.BinaryDocValues;\nimport org.apache.lucene.index.DocValues;\nimport org.apache.lucene.index.EmptyDocValuesProducer;\nimport org.apache.lucene.index.FieldInfo;\nimport org.apache.lucene.index.IndexFileNames;\nimport org.apache.lucene.index.SegmentWriteState;\nimport org.apache.lucene.index.SortedDocValues;\nimport org.apache.lucene.index.SortedNumericDocValues;\nimport org.apache.lucene.index.SortedSetDocValues;\nimport org.apache.lucene.index.TermsEnum;\nimport org.apache.lucene.search.DocIdSetIterator;\nimport org.apache.lucene.search.SortedSetSelector;\nimport org.apache.lucene.store.ByteBuffersDataOutput;\nimport org.apache.lucene.store.ByteBuffersIndexOutput;\nimport org.apache.lucene.store.IndexOutput;\nimport org.apache.lucene.util.BytesRef;\nimport org.apache.lucene.util.BytesRefBuilder;\nimport org.apache.lucene.util.IOUtils;\nimport org.apache.lucene.util.MathUtil;\nimport org.apache.lucene.util.StringHelper;\nimport org.apache.lucene.util.packed.DirectMonotonicWriter;\nimport org.apache.lucene.util.packed.DirectWriter;\n/** writer for {@link Lucene70DocValuesFormat} */\nfinal class Lucene70DocValuesConsumer extends DocValuesConsumer implements Closeable {\n  IndexOutput data, meta;\n  final int maxDoc;\n  /** expert: Creates a new writer */\n  public Lucene70DocValuesConsumer(SegmentWriteState state, String dataCodec, String dataExtension, String metaCodec, String metaExtension) throws IOException {\n    boolean success = false;\n    try {\n      String dataName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, dataExtension);\n      data = state.directory.createOutput(dataName, state.context);\n      CodecUtil.writeIndexHeader(data, dataCodec, Lucene70DocValuesFormat.VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);\n      String metaName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, metaExtension);\n      meta = state.directory.createOutput(metaName, state.context);\n      CodecUtil.writeIndexHeader(meta, metaCodec, Lucene70DocValuesFormat.VERSION_CURRENT, state.segmentInfo.getId(), state.segmentSuffix);\n      maxDoc = state.segmentInfo.maxDoc();\n      success = true;\n    } finally {\n      if (!success) {\n        IOUtils.closeWhileHandlingException(this);\n      }\n    }\n  }\n<fim_suffix>  @Override\n  public void close() throws IOException {\n    boolean success = false;\n    try {\n      if (meta != null) {\n        meta.writeInt(-1); // write EOF marker\n        CodecUtil.writeFooter(meta); // write checksum\n      }\n      if (data != null) {\n        CodecUtil.writeFooter(data); // write checksum\n      }\n      success = true;\n    } finally {\n      if (success) {\n        IOUtils.close(data, meta);\n      } else {\n        IOUtils.closeWhileHandlingException(data, meta);\n      }\n      meta = data = null;\n    }\n  }\n  @Override\n  public void addNumericField(FieldInfo field, DocValuesProducer valuesProducer) throws IOException {\n    meta.writeInt(field.number);\n    meta.writeByte(Lucene70DocValuesFormat.NUMERIC);\n    writeValues(field, new EmptyDocValuesProducer() {\n      @Override\n      public SortedNumericDocValues getSortedNumeric(FieldInfo field) throws IOException {\n        return DocValues.singleton(valuesProducer.getNumeric(field));\n      }\n    });\n  }\n  private static class MinMaxTracker {\n    long min, max, numValues, spaceInBits;\n    MinMaxTracker() {\n      reset();\n      spaceInBits = 0;\n    }\n    private void reset() {\n      min = Long.MAX_VALUE;\n      max = Long.MIN_VALUE;\n      numValues = 0;\n    }\n    /** Accumulate a new value. */\n    void update(long v) {\n      min = Math.min(min, v);\n      max = Math.max(max, v);\n      ++numValues;\n    }\n    /** Update the required space. */\n    void finish() {\n      if (max > min) {\n        spaceInBits += DirectWriter.unsignedBitsRequired(max - min) * numValues;\n      }\n    }\n    /** Update space usage and get ready for accumulating values for the next block. */\n    void nextBlock() {\n      finish();\n      reset();\n    }\n  }\n  private long[] writeValues(FieldInfo field, DocValuesProducer valuesProducer) throws IOException {\n    SortedNumericDocValues values = valuesProducer.getSortedNumeric(field);\n    int numDocsWithValue = 0;\n    MinMaxTracker minMax = new MinMaxTracker();\n    MinMaxTracker blockMinMax = new MinMaxTracker();\n    long gcd = 0;\n    Set<Long> uniqueValues = new HashSet<>();\n    for (int doc = values.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = values.nextDoc()) {\n      for (int i = 0, count = values.docValueCount(); i < count; ++i) {\n        long v = values.nextValue();\n        if (gcd != 1) {\n          if (v < Long.MIN_VALUE / 2 || v > Long.MAX_VALUE / 2) {\n            // in that case v - minValue might overflow and make the GCD computation return\n            // wrong results. Since these extreme values are unlikely, we just discard\n            // GCD computation for them\n            gcd = 1;\n          } else if (minMax.numValues != 0) { // minValue needs to be set first\n            gcd = MathUtil.gcd(gcd, v - minMax.min);\n          }\n        }\n        minMax.update(v);\n        blockMinMax.update(v);\n        if (blockMinMax.numValues == NUMERIC_BLOCK_SIZE) {\n          blockMinMax.nextBlock();\n        }\n        if (uniqueValues != null\n            && uniqueValues.add(v)\n            && uniqueValues.size() > 256) {\n          uniqueValues = null;\n        }\n      }\n      numDocsWithValue++;\n    }\n    minMax.finish();\n    blockMinMax.finish();\n    final long numValues = minMax.numValues;\n    long min = minMax.min;\n    final long max = minMax.max;\n    assert blockMinMax.spaceInBits <= minMax.spaceInBits;\n    if (numDocsWithValue == 0) {\n      meta.writeLong(-2);\n      meta.writeLong(0L);\n    } else if (numDocsWithValue == maxDoc) {\n      meta.writeLong(-1);\n      meta.writeLong(0L);\n    } else {\n      long offset = data.getFilePointer();\n      meta.writeLong(offset);\n      values = valuesProducer.getSortedNumeric(field);\n      IndexedDISI.writeBitSet(values, data);\n      meta.writeLong(data.getFilePointer() - offset);\n    }\n    meta.writeLong(numValues);\n    final int numBitsPerValue;\n    boolean doBlocks = false;\n    Map<Long, Integer> encode = null;\n    if (min >= max) {<fim_middle>// function below is long method\n"}