{"text": "<fim_prefix>    }\n    @Override\n    public long computeNorm(FieldInvertState state) {\n      assert state.length > 0;\n      while (true) {\n        long norm = norms[index++];\n        if (norm != 0) {\n          return norm;\n        }\n      }\n    }\n    @Override\n    public SimScorer scorer(float boost, CollectionStatistics collectionStats, TermStatistics... termStats) {\n      throw new UnsupportedOperationException();\n    }\n  }\n  @Override\n  protected void addRandomFields(Document doc) {\n    // TODO: improve\n    doc.add(new TextField(\"foobar\", TestUtil.randomSimpleString(random()), Field.Store.NO));\n  }\n  @Override\n  public void testMergeStability() throws Exception {\n    // TODO: can we improve this base test to just have subclasses declare the extensions to check,\n    // rather than a blacklist to exclude? we need to index stuff to get norms, but we dont care about testing\n    // the PFs actually doing that...\n    assumeTrue(\"The MockRandom PF randomizes content on the fly, so we can't check it\", false);\n  }\n  // TODO: test thread safety (e.g. across different fields) explicitly here\n  /*\n   * LUCENE-6006: Tests undead norms.\n   *                                 .....            \n   *                             C C  /            \n   *                            /<   /             \n   *             ___ __________/_#__=o             \n   *            /(- /(\\_\\________   \\              \n   *            \\ ) \\ )_      \\o     \\             \n   *            /|\\ /|\\       |'     |             \n   *                          |     _|             \n   *                          /o   __\\             \n   *                         / '     |             \n   *                        / /      |             \n   *                       /_/\\______|             \n   *                      (   _(    <              \n   *                       \\    \\    \\             \n   *                        \\    \\    |            \n   *                         \\____\\____\\           \n   *                         ____\\_\\__\\_\\          \n   *                       /`   /`     o\\          \n   *                       |___ |_______|\n   *\n   */\n  public void testUndeadNorms() throws Exception {\n    Directory dir = applyCreatedVersionMajor(newDirectory());\n    RandomIndexWriter w = new RandomIndexWriter(random(), dir);\n    int numDocs = atLeast(500);\n    List<Integer> toDelete = new ArrayList<>();\n    for(int i=0;i<numDocs;i++) {\n      Document doc = new Document();\n      doc.add(new StringField(\"id\", \"\"+i, Field.Store.NO));\n      if (random().nextInt(5) == 1) {\n        toDelete.add(i);\n        doc.add(new TextField(\"content\", \"some content\", Field.Store.NO));\n      }\n      w.addDocument(doc);\n    }\n    for(Integer id : toDelete) {\n      w.deleteDocuments(new Term(\"id\", \"\"+id));\n    }\n    w.forceMerge(1);\n    IndexReader r = maybeWrapWithMergingReader(w.getReader());\n    assertFalse(r.hasDeletions());\n    // Confusingly, norms should exist, and should all be 0, even though we deleted all docs that had the field \"content\".  They should not\n    // be undead:\n    NumericDocValues norms = MultiDocValues.getNormValues(r, \"content\");\n    assertNotNull(norms);\n    if (codecSupportsSparsity()) {\n      assertEquals(DocIdSetIterator.NO_MORE_DOCS, norms.nextDoc());\n    } else {\n      for(int i=0;i<r.maxDoc();i++) {\n        assertEquals(i, norms.nextDoc());\n        assertEquals(0, norms.longValue());\n      }\n    }\n    r.close();\n    w.close();\n    dir.close();\n  }\n  public void testThreads() throws Exception {\n    float density = codecSupportsSparsity() == false || random().nextBoolean() ? 1f : random().nextFloat();\n    int numDocs = atLeast(500);\n    final FixedBitSet docsWithField = new FixedBitSet(numDocs);\n    final int numDocsWithField = Math.max(1, (int) (density * numDocs));\n    if (numDocsWithField == numDocs) {\n      docsWithField.set(0, numDocs);\n    } else {\n      int i = 0;\n      while (i < numDocsWithField) {\n        int doc = random().nextInt(numDocs);\n        if (docsWithField.get(doc) == false) {\n          docsWithField.set(doc);\n          ++i;\n        }\n      }\n    }\n    long norms[] = new long[numDocsWithField];\n    for (int i = 0; i < numDocsWithField; i++) {\n      norms[i] = random().nextLong();\n    }\n    Directory dir = applyCreatedVersionMajor(newDirectory());\n    Analyzer analyzer = new MockAnalyzer(random(), MockTokenizer.WHITESPACE, false);\n    IndexWriterConfig conf = newIndexWriterConfig(analyzer);conf.setMergePolicy(NoMergePolicy.INSTANCE);\n    conf.setSimilarity(new CannedNormSimilarity(norms));\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, conf);\n    Document doc = new Document();\n    Field idField = new StringField(\"id\", \"\", Field.Store.NO);\n    Field indexedField = new TextField(\"indexed\", \"\", Field.Store.NO);\n    Field dvField = new NumericDocValuesField(\"dv\", 0);\n    doc.add(idField);\n    doc.add(indexedField);\n    doc.add(dvField);\n    for (int i = 0, j = 0; i < numDocs; i++) {\n      idField.setStringValue(Integer.toString(i));\n      if (docsWithField.get(i) == false) {\n        Document doc2 = new Document();\n        doc2.add(idField);\n        writer.addDocument(doc2);\n      } else {\n        long value = norms[j++];\n        dvField.setLongValue(value);\n        indexedField.setStringValue(value == 0 ? \"\" : \"a\");\n        writer.addDocument(doc);\n      }\n      if (random().nextInt(31) == 0) {\n        writer.commit();\n      }\n    }\n    DirectoryReader reader = maybeWrapWithMergingReader(writer.getReader());\n    writer.close();\n    final int numThreads = TestUtil.nextInt(random(), 3, 30);\n    Thread[] threads = new Thread[numThreads];\n    final CountDownLatch latch = new CountDownLatch(1);\n    for (int i = 0; i < numThreads; ++i) {\n      threads[i] = new Thread(new Runnable() {\n        @Override\n        public void run() {\n          try {\n            latch.await();\n            checkNormsVsDocValues(reader);\n            TestUtil.checkReader(reader);\n          } catch (Exception e) {\n            throw new RuntimeException(e);\n          }\n        }\n      });\n    }\n    for (Thread thread : threads) {\n      thread.start();\n    }\n    latch.countDown();\n    for (Thread thread : threads) {\n      thread.join();\n    }\n    reader.close();\n    dir.close();\n  }\n<fim_suffix>  public void testIndependantIterators() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriterConfig conf = newIndexWriterConfig().setMergePolicy(newLogMergePolicy());\n    CannedNormSimilarity sim = new CannedNormSimilarity(new long[] {42, 10, 20});\n    conf.setSimilarity(sim);\n    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, conf);\n    Document doc = new Document();\n    Field indexedField = new TextField(\"indexed\", \"a\", Field.Store.NO);\n    doc.add(indexedField);\n    for (int i = 0; i < 3; ++i) {\n      writer.addDocument(doc);\n    }\n    writer.forceMerge(1);\n    LeafReader r = getOnlyLeafReader(maybeWrapWithMergingReader(writer.getReader()));\n    NumericDocValues n1 = r.getNormValues(\"indexed\");\n    NumericDocValues n2 = r.getNormValues(\"indexed\");\n    assertEquals(0, n1.nextDoc());\n    assertEquals(42, n1.longValue());\n    assertEquals(1, n1.nextDoc());\n    assertEquals(10, n1.longValue());\n    assertEquals(0, n2.nextDoc());\n    assertEquals(42, n2.longValue());\n    assertEquals(1, n2.nextDoc());\n    assertEquals(10, n2.longValue());\n    assertEquals(2, n2.nextDoc());\n    assertEquals(20, n2.longValue());\n    assertEquals(2, n1.nextDoc());\n    assertEquals(20, n1.longValue());\n    assertEquals(DocIdSetIterator.NO_MORE_DOCS, n1.nextDoc());\n    assertEquals(DocIdSetIterator.NO_MORE_DOCS, n2.nextDoc());\n    IOUtils.close(r, writer, dir);\n  }<fim_middle>// function below has no smell\n"}