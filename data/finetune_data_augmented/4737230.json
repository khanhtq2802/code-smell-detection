{"text": "<fim_prefix>  // Schema handles\n  private Schema readerSchema;\n  private Schema writerSchema;\n  public AbstractRealtimeRecordReader(HoodieRealtimeFileSplit split, JobConf job) {\n    this.split = split;\n    this.jobConf = job;\n    LOG.info(\"cfg ==> \" + job.get(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR));\n    try {\n      baseFileSchema = readSchema(jobConf, split.getPath());\n      init();\n    } catch (IOException e) {\n      throw new HoodieIOException(\n          \"Could not create HoodieRealtimeRecordReader on path \" + this.split.getPath(), e);\n    }\n  }\n  /**\n   * Reads the schema from the parquet file. This is different from ParquetUtils as it uses the\n   * twitter parquet to support hive 1.1.0\n   */\n  private static MessageType readSchema(Configuration conf, Path parquetFilePath) {\n    try {\n      return ParquetFileReader.readFooter(conf, parquetFilePath).getFileMetaData().getSchema();\n    } catch (IOException e) {\n      throw new HoodieIOException(\"Failed to read footer for parquet \" + parquetFilePath, e);\n    }\n  }\n  protected static String arrayWritableToString(ArrayWritable writable) {\n    if (writable == null) {\n      return \"null\";\n    }\n    StringBuilder builder = new StringBuilder();\n    Writable[] values = writable.get();\n    builder.append(String.format(\"(Size: %s)[\", values.length));\n    for (Writable w : values) {\n      if (w instanceof ArrayWritable) {\n        builder.append(arrayWritableToString((ArrayWritable) w)).append(\" \");\n      } else {\n        builder.append(w).append(\" \");\n      }\n    }\n    builder.append(\"]\");\n    return builder.toString();\n  }\n  /**\n   * Given a comma separated list of field names and positions at which they appear on Hive, return\n   * a ordered list of field names, that can be passed onto storage.\n   */\n  private static List<String> orderFields(String fieldNameCsv, String fieldOrderCsv, List<String> partitioningFields) {\n    String[] fieldOrders = fieldOrderCsv.split(\",\");\n    List<String> fieldNames = Arrays.stream(fieldNameCsv.split(\",\"))\n        .filter(fn -> !partitioningFields.contains(fn)).collect(Collectors.toList());\n    // Hive does not provide ids for partitioning fields, so check for lengths excluding that.\n    if (fieldNames.size() != fieldOrders.length) {\n      throw new HoodieException(String\n          .format(\"Error ordering fields for storage read. #fieldNames: %d, #fieldPositions: %d\",\n              fieldNames.size(), fieldOrders.length));\n    }\n    TreeMap<Integer, String> orderedFieldMap = new TreeMap<>();\n    for (int ox = 0; ox < fieldOrders.length; ox++) {\n      orderedFieldMap.put(Integer.parseInt(fieldOrders[ox]), fieldNames.get(ox));\n    }\n    return new ArrayList<>(orderedFieldMap.values());\n  }\n  /**\n   * Generate a reader schema off the provided writeSchema, to just project out the provided\n   * columns\n   */\n  public static Schema generateProjectionSchema(Schema writeSchema, List<String> fieldNames) {\n    /**\n     * Avro & Presto field names seems to be case sensitive (support fields differing only in case)\n     * whereas Hive/Impala/SparkSQL(default) are case-insensitive. Spark allows this to be configurable\n     * using spark.sql.caseSensitive=true\n     *\n     * For a RT table setup with no delta-files (for a latest file-slice) -> we translate parquet schema to Avro\n     * Here the field-name case is dependent on parquet schema. Hive (1.x/2.x/CDH) translate column projections\n     * to lower-cases\n     *\n     */\n    List<Schema.Field> projectedFields = new ArrayList<>();\n    Map<String, Schema.Field> schemaFieldsMap = writeSchema.getFields().stream()\n        .map(r -> Pair.of(r.name().toLowerCase(), r)).collect(Collectors.toMap(Pair::getLeft, Pair::getRight));\n    for (String fn : fieldNames) {\n      Schema.Field field = schemaFieldsMap.get(fn.toLowerCase());\n      if (field == null) {\n        throw new HoodieException(\"Field \" + fn + \" not found in log schema. Query cannot proceed! \"\n            + \"Derived Schema Fields: \"\n            + new ArrayList<>(schemaFieldsMap.keySet()));\n      }\n      projectedFields\n          .add(new Schema.Field(field.name(), field.schema(), field.doc(), field.defaultValue()));\n    }\n    Schema projectedSchema = Schema\n        .createRecord(writeSchema.getName(), writeSchema.getDoc(), writeSchema.getNamespace(), writeSchema.isError());\n    projectedSchema.setFields(projectedFields);\n    return projectedSchema;\n  }\n  /**\n   * Convert the projected read from delta record into an array writable\n   */\n  public static Writable avroToArrayWritable(Object value, Schema schema) {\n    // if value is null, make a NullWritable\n    // Hive 2.x does not like NullWritable\n    if (value == null) {\n      return null;\n      //return NullWritable.get();\n    }\n    Writable[] wrapperWritable;\n    switch (schema.getType()) {\n      case STRING:\n        return new Text(value.toString());\n      case BYTES:\n        return new BytesWritable((byte[]) value);\n      case INT:\n        return new IntWritable((Integer) value);\n      case LONG:\n        return new LongWritable((Long) value);\n      case FLOAT:\n        return new FloatWritable((Float) value);\n      case DOUBLE:\n        return new DoubleWritable((Double) value);\n      case BOOLEAN:\n        return new BooleanWritable((Boolean) value);\n      case NULL:\n        return null;\n        // return NullWritable.get();\n      case RECORD:\n        GenericRecord record = (GenericRecord) value;\n        Writable[] values1 = new Writable[schema.getFields().size()];\n        int index1 = 0;\n        for (Schema.Field field : schema.getFields()) {\n          values1[index1++] = avroToArrayWritable(record.get(field.name()), field.schema());\n        }\n        return new ArrayWritable(Writable.class, values1);\n      case ENUM:\n        return new Text(value.toString());\n      case ARRAY:\n        GenericArray arrayValue = (GenericArray) value;\n        Writable[] values2 = new Writable[arrayValue.size()];\n        int index2 = 0;\n        for (Object obj : arrayValue) {\n          values2[index2++] = avroToArrayWritable(obj, schema.getElementType());\n        }\n        wrapperWritable = new Writable[]{new ArrayWritable(Writable.class, values2)};\n        return new ArrayWritable(Writable.class, wrapperWritable);\n      case MAP:\n        Map mapValue = (Map) value;\n        Writable[] values3 = new Writable[mapValue.size()];\n        int index3 = 0;\n        for (Object entry : mapValue.entrySet()) {\n          Map.Entry mapEntry = (Map.Entry) entry;\n          Writable[] mapValues = new Writable[2];\n          mapValues[0] = new Text(mapEntry.getKey().toString());\n          mapValues[1] = avroToArrayWritable(mapEntry.getValue(), schema.getValueType());\n          values3[index3++] = new ArrayWritable(Writable.class, mapValues);\n        }\n        wrapperWritable = new Writable[]{new ArrayWritable(Writable.class, values3)};\n        return new ArrayWritable(Writable.class, wrapperWritable);\n      case UNION:\n        List<Schema> types = schema.getTypes();\n        if (types.size() != 2) {\n          throw new IllegalArgumentException(\"Only support union with 2 fields\");\n        }\n        Schema s1 = types.get(0);\n        Schema s2 = types.get(1);\n        if (s1.getType() == Schema.Type.NULL) {\n          return avroToArrayWritable(value, s2);\n        } else if (s2.getType() == Schema.Type.NULL) {\n          return avroToArrayWritable(value, s1);\n        } else {\n          throw new IllegalArgumentException(\"Only support union with null\");\n        }\n      case FIXED:\n        return new BytesWritable(((GenericFixed) value).bytes());\n      default:\n        return null;\n    }\n  }\n<fim_suffix>  public static Schema readSchemaFromLogFile(FileSystem fs, Path path) throws IOException {\n    Reader reader = HoodieLogFormat.newReader(fs, new HoodieLogFile(path), null);\n    HoodieAvroDataBlock lastBlock = null;\n    while (reader.hasNext()) {\n      HoodieLogBlock block = reader.next();\n      if (block instanceof HoodieAvroDataBlock) {\n        lastBlock = (HoodieAvroDataBlock) block;\n      }\n    }\n    reader.close();\n    if (lastBlock != null) {\n      return lastBlock.getSchema();\n    }\n    return null;\n  }<fim_middle>// function below has no smell\n"}