{"text": "<fim_prefix>            filterRel.getCluster().getTypeFactory(), true));\n    FilterDesc filDesc = new FilterDesc(filCondExpr, false);\n    ArrayList<ColumnInfo> cinfoLst = createColInfos(inputOpAf.inputs.get(0));\n    FilterOperator filOp = (FilterOperator) OperatorFactory.getAndMakeChild(filDesc,\n        new RowSchema(cinfoLst), inputOpAf.inputs.get(0));\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Generated \" + filOp + \" with row schema: [\" + filOp.getSchema() + \"]\");\n    }\n    return inputOpAf.clone(filOp);\n  }\n  // use this function to make the union \"flat\" for both execution and explain\n  // purpose\n  private List<RelNode> extractRelNodeFromUnion(HiveUnion unionRel) {\n    List<RelNode> ret = new ArrayList<RelNode>();\n    for (RelNode input : unionRel.getInputs()) {\n      if (input instanceof HiveUnion) {\n        ret.addAll(extractRelNodeFromUnion((HiveUnion) input));\n      } else {\n        ret.add(input);\n      }\n    }\n    return ret;\n  }\n  OpAttr visit(HiveUnion unionRel) throws SemanticException {\n    // 1. Convert inputs\n    List<RelNode> inputsList = extractRelNodeFromUnion(unionRel);\n    OpAttr[] inputs = new OpAttr[inputsList.size()];\n    for (int i = 0; i < inputs.length; i++) {\n      inputs[i] = dispatch(inputsList.get(i));\n    }\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Translating operator rel#\" + unionRel.getId() + \":\" + unionRel.getRelTypeName()\n          + \" with row type: [\" + unionRel.getRowType() + \"]\");\n    }\n    // 2. Create a new union operator\n    UnionDesc unionDesc = new UnionDesc();\n    unionDesc.setNumInputs(inputs.length);\n    String tableAlias = getHiveDerivedTableAlias();\n    ArrayList<ColumnInfo> cinfoLst = createColInfos(inputs[0].inputs.get(0), tableAlias);\n    Operator<?>[] children = new Operator<?>[inputs.length];\n    for (int i = 0; i < children.length; i++) {\n      if (i == 0) {\n        children[i] = inputs[i].inputs.get(0);\n      } else {\n        Operator<?> op = inputs[i].inputs.get(0);\n        // We need to check if the other input branches for union is following the first branch\n        // We may need to cast the data types for specific columns.\n        children[i] = genInputSelectForUnion(op, cinfoLst);\n      }\n    }\n    Operator<? extends OperatorDesc> unionOp = OperatorFactory.getAndMakeChild(\n        semanticAnalyzer.getOpContext(), unionDesc, new RowSchema(cinfoLst), children);\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Generated \" + unionOp + \" with row schema: [\" + unionOp.getSchema() + \"]\");\n    }\n    //TODO: Can columns retain virtualness out of union\n    // 3. Return result\n    return new OpAttr(tableAlias, inputs[0].vcolsInCalcite, unionOp);\n  }\n  OpAttr visit(HiveSortExchange exchangeRel) throws SemanticException {\n    OpAttr inputOpAf = dispatch(exchangeRel.getInput());\n    String tabAlias = inputOpAf.tabAlias;\n    if (tabAlias == null || tabAlias.length() == 0) {\n      tabAlias = getHiveDerivedTableAlias();\n    }\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Translating operator rel#\" + exchangeRel.getId() + \":\"\n          + exchangeRel.getRelTypeName() + \" with row type: [\" + exchangeRel.getRowType() + \"]\");\n    }\n    RelDistribution distribution = exchangeRel.getDistribution();\n    if (distribution.getType() != Type.HASH_DISTRIBUTED) {\n      throw new SemanticException(\"Only hash distribution supported for LogicalExchange\");\n    }\n    ExprNodeDesc[] expressions = new ExprNodeDesc[exchangeRel.getJoinKeys().size()];\n    for (int index = 0; index < exchangeRel.getJoinKeys().size(); index++) {\n      expressions[index] = convertToExprNode(exchangeRel.getJoinKeys().get(index),\n          exchangeRel.getInput(), inputOpAf.tabAlias, inputOpAf);\n    }\n    exchangeRel.setJoinExpressions(expressions);\n    ReduceSinkOperator rsOp = genReduceSink(inputOpAf.inputs.get(0), tabAlias, expressions,\n        -1, -1, Operation.NOT_ACID, hiveConf);\n    return new OpAttr(tabAlias, inputOpAf.vcolsInCalcite, rsOp);\n  }\n  private OpAttr genPTF(OpAttr inputOpAf, WindowingSpec wSpec) throws SemanticException {\n    Operator<?> input = inputOpAf.inputs.get(0);\n    wSpec.validateAndMakeEffective();\n    WindowingComponentizer groups = new WindowingComponentizer(wSpec);\n    RowResolver rr = new RowResolver();\n    for (ColumnInfo ci : input.getSchema().getSignature()) {\n      rr.put(inputOpAf.tabAlias, ci.getInternalName(), ci);\n    }\n    while (groups.hasNext()) {\n      wSpec = groups.next(hiveConf, semanticAnalyzer, unparseTranslator, rr);\n      // 1. Create RS and backtrack Select operator on top\n      ArrayList<ExprNodeDesc> keyCols = new ArrayList<ExprNodeDesc>();\n      ArrayList<ExprNodeDesc> partCols = new ArrayList<ExprNodeDesc>();\n      StringBuilder order = new StringBuilder();\n      StringBuilder nullOrder = new StringBuilder();\n      for (PartitionExpression partCol : wSpec.getQueryPartitionSpec().getExpressions()) {\n        ExprNodeDesc partExpr = semanticAnalyzer.genExprNodeDesc(partCol.getExpression(), rr);\n        if (ExprNodeDescUtils.indexOf(partExpr, partCols) < 0) {\n          keyCols.add(partExpr);\n          partCols.add(partExpr);\n          order.append('+');\n          nullOrder.append('a');\n        }\n      }\n      if (wSpec.getQueryOrderSpec() != null) {\n        for (OrderExpression orderCol : wSpec.getQueryOrderSpec().getExpressions()) {\n          ExprNodeDesc orderExpr = semanticAnalyzer.genExprNodeDesc(orderCol.getExpression(), rr);\n          char orderChar = orderCol.getOrder() == PTFInvocationSpec.Order.ASC ? '+' : '-';\n          char nullOrderChar = orderCol.getNullOrder() == PTFInvocationSpec.NullOrder.NULLS_FIRST ? 'a' : 'z';\n          int index = ExprNodeDescUtils.indexOf(orderExpr, keyCols);\n          if (index >= 0) {\n            order.setCharAt(index, orderChar);\n            nullOrder.setCharAt(index, nullOrderChar);\n            continue;\n          }\n          keyCols.add(orderExpr);\n          order.append(orderChar);\n          nullOrder.append(nullOrderChar);\n        }\n      }\n      SelectOperator selectOp = genReduceSinkAndBacktrackSelect(input,\n          keyCols.toArray(new ExprNodeDesc[keyCols.size()]), 0, partCols,\n          order.toString(), nullOrder.toString(), -1, Operation.NOT_ACID, hiveConf);\n      // 2. Finally create PTF\n      PTFTranslator translator = new PTFTranslator();\n      PTFDesc ptfDesc = translator.translate(wSpec, semanticAnalyzer, hiveConf, rr,\n          unparseTranslator);\n      RowResolver ptfOpRR = ptfDesc.getFuncDef().getOutputShape().getRr();\n      Operator<?> ptfOp = OperatorFactory.getAndMakeChild(\n          ptfDesc, new RowSchema(ptfOpRR.getColumnInfos()), selectOp);\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Generated \" + ptfOp + \" with row schema: [\" + ptfOp.getSchema() + \"]\");\n      }\n      // 3. Prepare for next iteration (if any)\n      rr = ptfOpRR;\n      input = ptfOp;\n    }\n    return inputOpAf.clone(input);\n  }\n<fim_suffix>  private static SelectOperator genReduceSinkAndBacktrackSelect(Operator<?> input,\n          ExprNodeDesc[] keys, int tag, ArrayList<ExprNodeDesc> partitionCols, String order,\n          String nullOrder, int numReducers, Operation acidOperation, HiveConf hiveConf)\n              throws SemanticException {\n    return genReduceSinkAndBacktrackSelect(input, keys, tag, partitionCols, order, nullOrder,\n        numReducers, acidOperation, hiveConf, input.getSchema().getColumnNames());\n  }<fim_middle>// function below has no smell\n"}