{"text": "<fim_prefix>/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.hadoop.hive.ql.exec;\nimport java.io.IOException;\nimport java.io.Serializable;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.Collections;\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.hive.common.ObjectPair;\nimport org.apache.hadoop.hive.conf.HiveConf;\nimport org.apache.hadoop.hive.ql.CompilationOpContext;\nimport org.apache.hadoop.hive.ql.exec.persistence.RowContainer;\nimport org.apache.hadoop.hive.ql.io.AcidUtils;\nimport org.apache.hadoop.hive.ql.io.HiveInputFormat;\nimport org.apache.hadoop.hive.ql.metadata.HiveException;\nimport org.apache.hadoop.hive.ql.plan.BucketMapJoinContext;\nimport org.apache.hadoop.hive.ql.plan.FetchWork;\nimport org.apache.hadoop.hive.ql.plan.MapJoinDesc;\nimport org.apache.hadoop.hive.ql.plan.MapredLocalWork;\nimport org.apache.hadoop.hive.ql.plan.OperatorDesc;\nimport org.apache.hadoop.hive.ql.plan.SMBJoinDesc;\nimport org.apache.hadoop.hive.ql.plan.api.OperatorType;\nimport org.apache.hadoop.hive.serde2.ColumnProjectionUtils;\nimport org.apache.hadoop.hive.serde2.objectinspector.InspectableObject;\nimport org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\nimport org.apache.hadoop.io.WritableComparable;\nimport org.apache.hadoop.io.WritableComparator;\nimport org.apache.hadoop.mapred.JobConf;\nimport org.apache.hadoop.security.UserGroupInformation;\nimport org.apache.hadoop.util.PriorityQueue;\nimport org.apache.hive.common.util.ReflectionUtil;\n/**\n * Sorted Merge Map Join Operator.\n */\npublic class SMBMapJoinOperator extends AbstractMapJoinOperator<SMBJoinDesc> implements\n    Serializable {\n  private static final long serialVersionUID = 1L;\n  private static final Logger LOG = LoggerFactory.getLogger(SMBMapJoinOperator.class\n      .getName());\n  private MapredLocalWork localWork = null;\n  private Map<String, MergeQueue> aliasToMergeQueue = Collections.emptyMap();\n  transient List<Object>[] keyWritables;\n  transient List<Object>[] nextKeyWritables;\n  RowContainer<List<Object>>[] nextGroupStorage;\n  RowContainer<List<Object>>[] candidateStorage;\n  transient String[] tagToAlias;\n  private transient boolean[] fetchDone;\n  private transient boolean[] foundNextKeyGroup;\n  transient boolean firstFetchHappened = false;\n  private transient boolean inputFileChanged = false;\n  transient boolean localWorkInited = false;\n  transient boolean initDone = false;\n  // This join has been converted to a SMB join by the hive optimizer. The user did not\n  // give a mapjoin hint in the query. The hive optimizer figured out that the join can be\n  // performed as a smb join, based on all the tables/partitions being joined.\n  private transient boolean convertedAutomaticallySMBJoin = false;\n  /** Kryo ctor. */\n  protected SMBMapJoinOperator() {\n    super();\n  }\n  public SMBMapJoinOperator(CompilationOpContext ctx) {\n    super(ctx);\n  }\n  public SMBMapJoinOperator(AbstractMapJoinOperator<? extends MapJoinDesc> mapJoinOp) {\n    super(mapJoinOp);\n  }\n<fim_suffix>  @Override\n  protected void initializeOp(Configuration hconf) throws HiveException {\n    // If there is a sort-merge join followed by a regular join, the SMBJoinOperator may not\n    // get initialized at all. Consider the following query:\n    // A SMB B JOIN C\n    // For the mapper processing C, The SMJ is not initialized, no need to close it either.\n    initDone = true;\n    super.initializeOp(hconf);\n    closeCalled = false;\n    this.firstFetchHappened = false;\n    this.inputFileChanged = false;\n    // get the largest table alias from order\n    int maxAlias = 0;\n    for (byte pos = 0; pos < order.length; pos++) {\n      if (pos > maxAlias) {\n        maxAlias = pos;\n      }\n    }\n    maxAlias += 1;\n    nextGroupStorage = new RowContainer[maxAlias];\n    candidateStorage = new RowContainer[maxAlias];\n    keyWritables = new ArrayList[maxAlias];\n    nextKeyWritables = new ArrayList[maxAlias];\n    fetchDone = new boolean[maxAlias];\n    foundNextKeyGroup = new boolean[maxAlias];\n    int bucketSize;\n    // For backwards compatibility reasons we honor the older\n    // HIVEMAPJOINBUCKETCACHESIZE if set different from default.\n    // By hive 0.13 we should remove this code.\n    int oldVar = HiveConf.getIntVar(hconf, HiveConf.ConfVars.HIVEMAPJOINBUCKETCACHESIZE);\n    if (oldVar != 100) {\n      bucketSize = oldVar;\n    } else {\n      bucketSize = HiveConf.getIntVar(hconf, HiveConf.ConfVars.HIVESMBJOINCACHEROWS);\n    }\n    for (byte pos = 0; pos < order.length; pos++) {\n      RowContainer<List<Object>> rc = JoinUtil.getRowContainer(hconf,\n          rowContainerStandardObjectInspectors[pos],\n          pos, bucketSize,spillTableDesc, conf, !hasFilter(pos),\n          reporter);\n      nextGroupStorage[pos] = rc;\n      RowContainer<List<Object>> candidateRC = JoinUtil.getRowContainer(hconf,\n          rowContainerStandardObjectInspectors[pos],\n          pos, bucketSize,spillTableDesc, conf, !hasFilter(pos),\n          reporter);\n      candidateStorage[pos] = candidateRC;\n    }\n    tagToAlias = conf.convertToArray(conf.getTagToAlias(), String.class);\n    for (byte pos = 0; pos < order.length; pos++) {\n      if (pos != posBigTable) {\n        fetchDone[pos] = false;\n      }\n      foundNextKeyGroup[pos] = false;\n    }\n  }\n  @Override\n  public void initializeLocalWork(Configuration hconf) throws HiveException {\n    initializeMapredLocalWork(this.getConf(), hconf, this.getConf().getLocalWork(), LOG);\n    super.initializeLocalWork(hconf);\n  }\n  public void initializeMapredLocalWork(MapJoinDesc mjConf, Configuration hconf,\n      MapredLocalWork localWork, Logger l4j) throws HiveException {\n    if (localWork == null || localWorkInited) {\n      return;\n    }\n    localWorkInited = true;\n    this.localWork = localWork;\n    aliasToMergeQueue = new HashMap<String, MergeQueue>();\n    // create map local operators\n    Map<String,FetchWork> aliasToFetchWork = localWork.getAliasToFetchWork();\n    Map<String, Operator<? extends OperatorDesc>> aliasToWork = localWork.getAliasToWork();\n    Map<String, DummyStoreOperator> aliasToSinkWork = conf.getAliasToSink();\n    // The operator tree till the sink operator needs to be processed while\n    // fetching the next row to fetch from the priority queue (possibly containing\n    // multiple files in the small table given a file in the big table). The remaining\n    // tree will be processed while processing the join.\n    // Look at comments in DummyStoreOperator for additional explanation.\n    for (Map.Entry<String, FetchWork> entry : aliasToFetchWork.entrySet()) {\n      String alias = entry.getKey();\n      FetchWork fetchWork = entry.getValue();\n      JobConf jobClone = new JobConf(hconf);\n      if (UserGroupInformation.isSecurityEnabled()) {\n        String hadoopAuthToken = System.getenv(UserGroupInformation.HADOOP_TOKEN_FILE_LOCATION);\n        if(hadoopAuthToken != null){\n          jobClone.set(\"mapreduce.job.credentials.binary\", hadoopAuthToken);\n        }\n      }<fim_middle>// function below is long method\n"}