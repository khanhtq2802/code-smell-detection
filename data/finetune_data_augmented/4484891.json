{"text": "<fim_prefix>/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.drill.exec.physical.impl;\n\nimport java.io.IOException;\n\nimport org.apache.drill.common.expression.SchemaPath;\nimport org.apache.drill.common.types.TypeProtos.MinorType;\nimport org.apache.drill.common.types.Types;\nimport org.apache.drill.exec.exception.OutOfMemoryException;\nimport org.apache.drill.exec.ops.FragmentContext;\nimport org.apache.drill.exec.physical.base.Writer;\nimport org.apache.drill.exec.proto.ExecProtos.FragmentHandle;\nimport org.apache.drill.exec.record.AbstractRecordBatch;\nimport org.apache.drill.exec.record.BatchSchema;\nimport org.apache.drill.exec.record.BatchSchema.SelectionVectorMode;\nimport org.apache.drill.exec.record.MaterializedField;\nimport org.apache.drill.exec.record.RecordBatch;\nimport org.apache.drill.exec.record.VectorWrapper;\nimport org.apache.drill.exec.store.EventBasedRecordWriter;\nimport org.apache.drill.exec.store.RecordWriter;\nimport org.apache.drill.exec.vector.AllocationHelper;\nimport org.apache.drill.exec.vector.BigIntVector;\nimport org.apache.drill.exec.vector.VarCharVector;\n\n/* Write the RecordBatch to the given RecordWriter. */\npublic class WriterRecordBatch extends AbstractRecordBatch<Writer> {\n  private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(WriterRecordBatch.class);\n\n  private EventBasedRecordWriter eventBasedRecordWriter;\n  private RecordWriter recordWriter;\n  private long counter = 0;\n  private final RecordBatch incoming;\n  private boolean processed = false;\n  private final String fragmentUniqueId;\n  private BatchSchema schema;\n\n  public WriterRecordBatch(Writer writer, RecordBatch incoming, FragmentContext context, RecordWriter recordWriter) throws OutOfMemoryException {\n    super(writer, context, false);\n    this.incoming = incoming;\n\n    final FragmentHandle handle = context.getHandle();\n    fragmentUniqueId = String.format(\"%d_%d\", handle.getMajorFragmentId(), handle.getMinorFragmentId());\n    this.recordWriter = recordWriter;\n  }\n\n  @Override\n  public int getRecordCount() {\n    return container.getRecordCount();\n  }\n\n  @Override\n  protected void killIncoming(boolean sendUpstream) {\n    incoming.kill(sendUpstream);\n  }\n\n  @Override\n  public BatchSchema getSchema() {\n    return schema;\n  }\n\n  @Override\n  public IterOutcome innerNext() {\n    if (processed) {\n//      cleanup();\n      // if the upstream record batch is already processed and next() is called by\n      // downstream then return NONE to indicate completion\n      return IterOutcome.NONE;\n    }\n\n    // process the complete upstream in one next() call\n    IterOutcome upstream;\n    try {\n      do {\n        upstream = next(incoming);\n\n        switch(upstream) {\n          case OUT_OF_MEMORY:\n          case STOP:\n            return upstream;\n\n          case NOT_YET:\n            break;\n          case NONE:\n            if (schema != null) {\n              // Schema is for the output batch schema which is setup in setupNewSchema(). Since the output\n              // schema is fixed ((Fragment(VARCHAR), Number of records written (BIGINT)) we should set it\n              // up even with 0 records for it to be reported back to the client.\n              break;\n            }\n\n          case OK_NEW_SCHEMA:\n            setupNewSchema();\n            // $FALL-THROUGH$\n          case OK:\n            counter += eventBasedRecordWriter.write(incoming.getRecordCount());\n            logger.debug(\"Total records written so far: {}\", counter);\n\n            for(final VectorWrapper<?> v : incoming) {\n              v.getValueVector().clear();\n            }\n            break;\n\n          default:\n            throw new UnsupportedOperationException();\n        }\n      } while(upstream != IterOutcome.NONE);\n    } catch(IOException ex) {\n      logger.error(\"Failure during query\", ex);\n      kill(false);\n      context.getExecutorState().fail(ex);\n      return IterOutcome.STOP;\n    }\n\n    addOutputContainerData();\n    processed = true;\n\n    closeWriter();\n\n    return IterOutcome.OK_NEW_SCHEMA;\n  }\n\n<fim_suffix>  private void addOutputContainerData() {\n    @SuppressWarnings(\"resource\")\n    final VarCharVector fragmentIdVector = (VarCharVector) container.getValueAccessorById(\n        VarCharVector.class,\n        container.getValueVectorId(SchemaPath.getSimplePath(\"Fragment\")).getFieldIds())\n      .getValueVector();\n    AllocationHelper.allocate(fragmentIdVector, 1, 50);\n    @SuppressWarnings(\"resource\")\n    final BigIntVector summaryVector = (BigIntVector) container.getValueAccessorById(BigIntVector.class,\n            container.getValueVectorId(SchemaPath.getSimplePath(\"Number of records written\")).getFieldIds())\n          .getValueVector();\n    AllocationHelper.allocate(summaryVector, 1, 8);\n    fragmentIdVector.getMutator().setSafe(0, fragmentUniqueId.getBytes());\n    fragmentIdVector.getMutator().setValueCount(1);\n    summaryVector.getMutator().setSafe(0, counter);\n    summaryVector.getMutator().setValueCount(1);\n\n    container.setRecordCount(1);\n  }\n\n  protected void setupNewSchema() throws IOException {\n    try {\n      // update the schema in RecordWriter\n      stats.startSetup();\n      recordWriter.updateSchema(incoming);\n      // Create two vectors for:\n      //   1. Fragment unique id.\n      //   2. Summary: currently contains number of records written.\n      final MaterializedField fragmentIdField =\n          MaterializedField.create(\"Fragment\", Types.required(MinorType.VARCHAR));\n      final MaterializedField summaryField =\n          MaterializedField.create(\"Number of records written\",\n              Types.required(MinorType.BIGINT));\n\n      container.addOrGet(fragmentIdField);\n      container.addOrGet(summaryField);\n      container.buildSchema(BatchSchema.SelectionVectorMode.NONE);\n    } finally {\n      stats.stopSetup();\n    }\n\n    eventBasedRecordWriter = new EventBasedRecordWriter(incoming, recordWriter);\n    container.buildSchema(SelectionVectorMode.NONE);\n    schema = container.getSchema();\n  }\n\n  /** Clean up needs to be performed before closing writer. Partially written data will be removed. */\n  private void closeWriter() {\n    if (recordWriter == null) {\n      return;\n    }\n\n    try {\n      //Perform any post processing tasks prior to cleaning up the writer\n      recordWriter.postProcessing();\n      //Perform any cleanup prior to closing the writer\n      recordWriter.cleanup();\n    } catch(IOException ex) {\n      context.getExecutorState().fail(ex);\n    } finally {\n      try {\n        if (!processed) {\n          recordWriter.abort();\n        }\n      } catch (IOException e) {\n        logger.error(\"Abort failed. There could be leftover output files.\", e);\n      } finally {\n        recordWriter = null;\n      }\n    }\n  }\n\n  @Override\n  public void close() {\n    closeWriter();\n    super.close();\n  }\n\n  @Override\n  public void dump() {\n    logger.error(\"WriterRecordBatch[container={}, popConfig={}, counter={}, fragmentUniqueId={}, schema={}]\",\n        container, popConfig, counter, fragmentUniqueId, schema);\n  }\n}<fim_middle>// function below is feature envy\n"}