{"text": "<fim_prefix>/**\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.crunch.io.hbase;\nimport static org.junit.Assert.assertEquals;\nimport static org.junit.Assert.assertNotNull;\nimport static org.junit.Assert.assertTrue;\nimport java.io.IOException;\nimport java.nio.charset.Charset;\nimport java.util.Map;\nimport java.util.Random;\nimport java.util.concurrent.atomic.AtomicBoolean;\nimport org.apache.crunch.DoFn;\nimport org.apache.crunch.Emitter;\nimport org.apache.crunch.MapFn;\nimport org.apache.crunch.PCollection;\nimport org.apache.crunch.PTable;\nimport org.apache.crunch.Pair;\nimport org.apache.crunch.Pipeline;\nimport org.apache.crunch.PipelineResult;\nimport org.apache.crunch.impl.mr.MRPipeline;\nimport org.apache.crunch.test.TemporaryPath;\nimport org.apache.crunch.test.TemporaryPaths;\nimport org.apache.crunch.types.writable.Writables;\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.hbase.HBaseConfiguration;\nimport org.apache.hadoop.hbase.HBaseTestingUtility;\nimport org.apache.hadoop.hbase.HConstants;\nimport org.apache.hadoop.hbase.TableName;\nimport org.apache.hadoop.hbase.client.Delete;\nimport org.apache.hadoop.hbase.client.Get;\nimport org.apache.hadoop.hbase.client.Put;\nimport org.apache.hadoop.hbase.client.Result;\nimport org.apache.hadoop.hbase.client.Scan;\nimport org.apache.hadoop.hbase.client.Table;\nimport org.apache.hadoop.hbase.io.ImmutableBytesWritable;\nimport org.apache.hadoop.hbase.mapreduce.MultiTableInputFormat;\nimport org.apache.hadoop.hbase.mapreduce.MultiTableInputFormatBase;\nimport org.apache.hadoop.hbase.util.Bytes;\nimport org.junit.After;\nimport org.junit.Before;\nimport org.junit.Rule;\nimport org.junit.Test;\nimport com.google.common.base.Joiner;\nimport com.google.common.collect.ImmutableSet;\npublic class WordCountHBaseIT {\n  static class StringifyFn extends MapFn<Pair<ImmutableBytesWritable, Pair<Result, Result>>, String> {\n    @Override\n    public String map(Pair<ImmutableBytesWritable, Pair<Result, Result>> input) {\n      byte[] firstStrBytes = input.second().first().getValue(WORD_COLFAM, null);\n      byte[] secondStrBytes = input.second().second().getValue(WORD_COLFAM, null);\n      if (firstStrBytes != null && secondStrBytes != null) {\n        return Joiner.on(',').join(new String(firstStrBytes, Charset.forName(\"UTF-8\")),\n                                   new String(secondStrBytes, Charset.forName(\"UTF-8\")));\n      }\n      return \"\";\n    }\n  }\n  @Rule\n  public TemporaryPath tmpDir = TemporaryPaths.create();\n  private static final byte[] COUNTS_COLFAM = Bytes.toBytes(\"cf\");\n  private static final byte[] WORD_COLFAM = Bytes.toBytes(\"cf\");\n  private HBaseTestingUtility hbaseTestUtil;\n  @SuppressWarnings(\"serial\")\n  public static PCollection<Put> wordCount(PTable<ImmutableBytesWritable, Result> words) {\n    PTable<String, Long> counts = words.parallelDo(\n        new DoFn<Pair<ImmutableBytesWritable, Result>, String>() {\n          @Override\n          public void process(Pair<ImmutableBytesWritable, Result> row, Emitter<String> emitter) {\n            byte[] word = row.second().getValue(WORD_COLFAM, null);\n            if (word != null) {\n              emitter.emit(Bytes.toString(word));\n            }\n          }\n        }, words.getTypeFamily().strings()).count();\n    return counts.parallelDo(\"convert to put\", new DoFn<Pair<String, Long>, Put>() {\n      @Override\n      public void process(Pair<String, Long> input, Emitter<Put> emitter) {\n        Put put = new Put(Bytes.toBytes(input.first()));\n        put.addColumn(COUNTS_COLFAM, null, Bytes.toBytes(input.second()));\n        emitter.emit(put);\n      }\n    }, HBaseTypes.puts());\n  }\n  @SuppressWarnings(\"serial\")\n  public static PCollection<Delete> clearCounts(PTable<ImmutableBytesWritable, Result> counts) {\n    return counts.parallelDo(\"convert to delete\", new DoFn<Pair<ImmutableBytesWritable, Result>, Delete>() {\n      @Override\n      public void process(Pair<ImmutableBytesWritable, Result> input, Emitter<Delete> emitter) {\n        Delete delete = new Delete(input.first().get());\n        emitter.emit(delete);\n      }\n    }, HBaseTypes.deletes());\n  }\n<fim_suffix>  @Before\n  public void setUp() throws Exception {\n    Configuration conf = HBaseConfiguration.create(tmpDir.getDefaultConfiguration());\n    conf.set(HConstants.TEMPORARY_FS_DIRECTORY_KEY, tmpDir.getFile(\"hbase-staging\").getAbsolutePath());\n    hbaseTestUtil = new HBaseTestingUtility(conf);\n    hbaseTestUtil.startMiniCluster();\n  }\n  @Test\n  public void testWordCount() throws Exception {\n    run(new MRPipeline(WordCountHBaseIT.class, hbaseTestUtil.getConfiguration()));\n  }\n  @Test\n  public void testWordCountCustomFormat() throws Exception {\n    run(new MRPipeline(WordCountHBaseIT.class, hbaseTestUtil.getConfiguration()), MyTableInputFormat.class);\n    assertTrue(MyTableInputFormat.CONSTRUCTED.get());\n  }\n  @After\n  public void tearDown() throws Exception {\n    hbaseTestUtil.shutdownMiniCluster();\n  }\n  public void run(Pipeline pipeline) throws Exception {\n    run(pipeline, null);\n  }\n  public void run(Pipeline pipeline, Class<? extends MultiTableInputFormatBase> clazz) throws Exception {\n    Random rand = new Random();\n    int postFix = rand.nextInt() & 0x7FFFFFFF;\n    TableName inputTableName = TableName.valueOf(\"crunch_words_\" + postFix);\n    TableName outputTableName = TableName.valueOf(\"crunch_counts_\" + postFix);\n    TableName otherTableName = TableName.valueOf(\"crunch_other_\" + postFix);\n    TableName joinTableName = TableName.valueOf(\"crunch_join_words_\" + postFix);\n    Table inputTable = hbaseTestUtil.createTable(inputTableName, WORD_COLFAM);\n    Table outputTable = hbaseTestUtil.createTable(outputTableName, COUNTS_COLFAM);\n    Table otherTable = hbaseTestUtil.createTable(otherTableName, COUNTS_COLFAM);\n    int key = 0;\n    key = put(inputTable, key, \"cat\");\n    key = put(inputTable, key, \"cat\");\n    key = put(inputTable, key, \"dog\");\n    inputTable.close();\n    //Setup scan using multiple scans that simply cut the rows in half.\n    Scan scan = new Scan();\n    scan.addFamily(WORD_COLFAM);\n    byte[] cutoffPoint = Bytes.toBytes(2);\n    scan.setStopRow(cutoffPoint);\n    Scan scan2 = new Scan();\n    scan.addFamily(WORD_COLFAM);\n    scan2.setStartRow(cutoffPoint);\n    HBaseSourceTarget source = null;\n    if(clazz == null){\n      source = new HBaseSourceTarget(inputTableName, scan, scan2);\n    }else{\n      source = new HBaseSourceTarget(inputTableName, clazz, new Scan[]{scan, scan2});\n    }\n    PTable<ImmutableBytesWritable, Result> words = pipeline.read(source);\n    Map<ImmutableBytesWritable, Result> materialized = words.materializeToMap();\n    assertEquals(3, materialized.size());\n    PCollection<Put> puts = wordCount(words);\n    pipeline.write(puts, new HBaseTarget(outputTableName));\n    pipeline.write(puts, new HBaseTarget(otherTableName));\n    PipelineResult res = pipeline.done();\n    assertTrue(res.succeeded());\n    assertIsLong(otherTable, \"cat\", 2);<fim_middle>// function below has no smell\n"}