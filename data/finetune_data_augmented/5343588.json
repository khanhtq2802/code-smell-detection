{"text": "<fim_prefix>import com.amazonaws.services.dynamodbv2.document.DynamoDB;\nimport com.amazonaws.services.dynamodbv2.document.Item;\nimport com.amazonaws.services.dynamodbv2.document.TableWriteItems;\nimport com.amazonaws.services.dynamodbv2.model.AttributeValue;\nimport com.amazonaws.services.dynamodbv2.model.WriteRequest;\n@SupportsBatching\n@SeeAlso({DeleteDynamoDB.class, GetDynamoDB.class})\n@InputRequirement(Requirement.INPUT_REQUIRED)\n@Tags({\"Amazon\", \"DynamoDB\", \"AWS\", \"Put\", \"Insert\"})\n@CapabilityDescription(\"Puts a document from DynamoDB based on hash and range key.  The table can have either hash and range or hash key alone.\"\n    + \" Currently the keys supported are string and number and value can be json document. \"\n    + \"In case of hash and range keys both key are required for the operation.\"\n    + \" The FlowFile content must be JSON. FlowFile content is mapped to the specified Json Document attribute in the DynamoDB item.\")\n@WritesAttributes({\n    @WritesAttribute(attribute = AbstractDynamoDBProcessor.DYNAMODB_KEY_ERROR_UNPROCESSED, description = \"Dynamo db unprocessed keys\"),\n    @WritesAttribute(attribute = AbstractDynamoDBProcessor.DYNAMODB_RANGE_KEY_VALUE_ERROR, description = \"Dynamod db range key error\"),\n    @WritesAttribute(attribute = AbstractDynamoDBProcessor.DYNAMODB_KEY_ERROR_NOT_FOUND, description = \"Dynamo db key not found\"),\n    @WritesAttribute(attribute = AbstractDynamoDBProcessor.DYNAMODB_ERROR_EXCEPTION_MESSAGE, description = \"Dynamo db exception message\"),\n    @WritesAttribute(attribute = AbstractDynamoDBProcessor.DYNAMODB_ERROR_CODE, description = \"Dynamo db error code\"),\n    @WritesAttribute(attribute = AbstractDynamoDBProcessor.DYNAMODB_ERROR_MESSAGE, description = \"Dynamo db error message\"),\n    @WritesAttribute(attribute = AbstractDynamoDBProcessor.DYNAMODB_ERROR_TYPE, description = \"Dynamo db error type\"),\n    @WritesAttribute(attribute = AbstractDynamoDBProcessor.DYNAMODB_ERROR_SERVICE, description = \"Dynamo db error service\"),\n    @WritesAttribute(attribute = AbstractDynamoDBProcessor.DYNAMODB_ERROR_RETRYABLE, description = \"Dynamo db error is retryable\"),\n    @WritesAttribute(attribute = AbstractDynamoDBProcessor.DYNAMODB_ERROR_REQUEST_ID, description = \"Dynamo db error request id\"),\n    @WritesAttribute(attribute = AbstractDynamoDBProcessor.DYNAMODB_ERROR_STATUS_CODE, description = \"Dynamo db error status code\"),\n    @WritesAttribute(attribute = AbstractDynamoDBProcessor.DYNAMODB_ITEM_IO_ERROR, description = \"IO exception message on creating item\")\n})\n@ReadsAttributes({\n    @ReadsAttribute(attribute = AbstractDynamoDBProcessor.DYNAMODB_ITEM_HASH_KEY_VALUE, description = \"Items hash key value\"),\n    @ReadsAttribute(attribute = AbstractDynamoDBProcessor.DYNAMODB_ITEM_RANGE_KEY_VALUE, description = \"Items range key value\")\n})\n@SystemResourceConsideration(resource = SystemResource.MEMORY)\npublic class PutDynamoDB extends AbstractWriteDynamoDBProcessor {\n    public static final List<PropertyDescriptor> properties = Collections.unmodifiableList(\n        Arrays.asList(TABLE, HASH_KEY_NAME, RANGE_KEY_NAME, HASH_KEY_VALUE, RANGE_KEY_VALUE,\n            HASH_KEY_VALUE_TYPE, RANGE_KEY_VALUE_TYPE, JSON_DOCUMENT, DOCUMENT_CHARSET, BATCH_SIZE,\n            REGION, ACCESS_KEY, SECRET_KEY, CREDENTIALS_FILE, AWS_CREDENTIALS_PROVIDER_SERVICE, TIMEOUT, SSL_CONTEXT_SERVICE,\n            PROXY_CONFIGURATION_SERVICE, PROXY_HOST, PROXY_HOST_PORT, PROXY_USERNAME, PROXY_PASSWORD));\n    /**\n     * Dyamodb max item size limit 400 kb\n     */\n    public static final int DYNAMODB_MAX_ITEM_SIZE = 400 * 1024;\n    @Override\n    protected List<PropertyDescriptor> getSupportedPropertyDescriptors() {\n        return properties;\n    }\n<fim_suffix>    @Override\n    public void onTrigger(final ProcessContext context, final ProcessSession session) {\n        List<FlowFile> flowFiles = session.get(context.getProperty(BATCH_SIZE).evaluateAttributeExpressions().asInteger());\n        if (flowFiles == null || flowFiles.size() == 0) {\n            return;\n        }\n        Map<ItemKeys, FlowFile> keysToFlowFileMap = new HashMap<>();\n        final String table = context.getProperty(TABLE).evaluateAttributeExpressions().getValue();\n        final String hashKeyName = context.getProperty(HASH_KEY_NAME).evaluateAttributeExpressions().getValue();\n        final String hashKeyValueType = context.getProperty(HASH_KEY_VALUE_TYPE).getValue();\n        final String rangeKeyName = context.getProperty(RANGE_KEY_NAME).evaluateAttributeExpressions().getValue();\n        final String rangeKeyValueType = context.getProperty(RANGE_KEY_VALUE_TYPE).getValue();\n        final String jsonDocument = context.getProperty(JSON_DOCUMENT).evaluateAttributeExpressions().getValue();\n        final String charset = context.getProperty(DOCUMENT_CHARSET).evaluateAttributeExpressions().getValue();\n        TableWriteItems tableWriteItems = new TableWriteItems(table);\n        for (FlowFile flowFile : flowFiles) {\n            final Object hashKeyValue = getValue(context, HASH_KEY_VALUE_TYPE, HASH_KEY_VALUE, flowFile);\n            final Object rangeKeyValue = getValue(context, RANGE_KEY_VALUE_TYPE, RANGE_KEY_VALUE, flowFile);\n            if (!isHashKeyValueConsistent(hashKeyName, hashKeyValue, session, flowFile)) {\n                continue;\n            }\n            if (!isRangeKeyValueConsistent(rangeKeyName, rangeKeyValue, session, flowFile)) {\n                continue;\n            }\n            if (!isDataValid(flowFile, jsonDocument)) {\n                flowFile = session.putAttribute(flowFile, AWS_DYNAMO_DB_ITEM_SIZE_ERROR, \"Max size of item + attribute should be 400kb but was \" + flowFile.getSize() + jsonDocument.length());\n                session.transfer(flowFile, REL_FAILURE);\n                continue;\n            }\n            ByteArrayOutputStream baos = new ByteArrayOutputStream();\n            session.exportTo(flowFile, baos);\n            try {\n                if (rangeKeyValue == null || StringUtils.isBlank(rangeKeyValue.toString())) {\n                    tableWriteItems.addItemToPut(new Item().withKeyComponent(hashKeyName, hashKeyValue)\n                        .withJSON(jsonDocument, IOUtils.toString(baos.toByteArray(), charset)));\n                } else {\n                    tableWriteItems.addItemToPut(new Item().withKeyComponent(hashKeyName, hashKeyValue)\n                        .withKeyComponent(rangeKeyName, rangeKeyValue)\n                        .withJSON(jsonDocument, IOUtils.toString(baos.toByteArray(), charset)));\n                }\n            } catch (IOException ioe) {\n                getLogger().error(\"IOException while creating put item : \" + ioe.getMessage());\n                flowFile = session.putAttribute(flowFile, DYNAMODB_ITEM_IO_ERROR, ioe.getMessage());\n                session.transfer(flowFile, REL_FAILURE);\n            }\n            keysToFlowFileMap.put(new ItemKeys(hashKeyValue, rangeKeyValue), flowFile);\n        }\n        if (keysToFlowFileMap.isEmpty()) {\n            return;\n        }\n        final DynamoDB dynamoDB = getDynamoDB();\n        try {\n            BatchWriteItemOutcome outcome = dynamoDB.batchWriteItem(tableWriteItems);\n            handleUnprocessedItems(session, keysToFlowFileMap, table, hashKeyName, hashKeyValueType, rangeKeyName,\n                rangeKeyValueType, outcome);\n            // Handle any remaining flowfiles\n            for (FlowFile flowFile : keysToFlowFileMap.values()) {\n                getLogger().debug(\"Successful posted items to dynamodb : \" + table);\n                session.transfer(flowFile, REL_SUCCESS);\n            }\n        } catch (AmazonServiceException exception) {\n            getLogger().error(\"Could not process flowFiles due to service exception : \" + exception.getMessage());\n            List<FlowFile> failedFlowFiles = processServiceException(session, flowFiles, exception);\n            session.transfer(failedFlowFiles, REL_FAILURE);\n        } catch (AmazonClientException exception) {\n            getLogger().error(\"Could not process flowFiles due to client exception : \" + exception.getMessage());\n            List<FlowFile> failedFlowFiles = processClientException(session, flowFiles, exception);\n            session.transfer(failedFlowFiles, REL_FAILURE);\n        } catch (Exception exception) {\n            getLogger().error(\"Could not process flowFiles due to exception : \" + exception.getMessage());\n            List<FlowFile> failedFlowFiles = processException(session, flowFiles, exception);\n            session.transfer(failedFlowFiles, REL_FAILURE);\n        }\n    }<fim_middle>// function below is long method\n"}