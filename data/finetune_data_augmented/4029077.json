{"text": "<fim_prefix>/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.beam.sdk.io.hcatalog;\nimport static org.apache.beam.vendor.guava.v20_0.com.google.common.base.Preconditions.checkArgument;\nimport com.google.auto.value.AutoValue;\nimport java.util.ArrayList;\nimport java.util.HashMap;\nimport java.util.Iterator;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Map.Entry;\nimport java.util.NoSuchElementException;\nimport javax.annotation.Nullable;\nimport org.apache.beam.sdk.annotations.Experimental;\nimport org.apache.beam.sdk.coders.Coder;\nimport org.apache.beam.sdk.io.BoundedSource;\nimport org.apache.beam.sdk.io.hadoop.WritableCoder;\nimport org.apache.beam.sdk.options.PipelineOptions;\nimport org.apache.beam.sdk.transforms.DoFn;\nimport org.apache.beam.sdk.transforms.PTransform;\nimport org.apache.beam.sdk.transforms.ParDo;\nimport org.apache.beam.sdk.transforms.display.DisplayData;\nimport org.apache.beam.sdk.values.PBegin;\nimport org.apache.beam.sdk.values.PCollection;\nimport org.apache.beam.sdk.values.PDone;\nimport org.apache.beam.vendor.guava.v20_0.com.google.common.annotations.VisibleForTesting;\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.hive.conf.HiveConf;\nimport org.apache.hadoop.hive.metastore.IMetaStoreClient;\nimport org.apache.hadoop.hive.ql.metadata.Table;\nimport org.apache.hadoop.hive.ql.stats.StatsUtils;\nimport org.apache.hive.hcatalog.common.HCatConstants;\nimport org.apache.hive.hcatalog.common.HCatException;\nimport org.apache.hive.hcatalog.common.HCatUtil;\nimport org.apache.hive.hcatalog.data.DefaultHCatRecord;\nimport org.apache.hive.hcatalog.data.HCatRecord;\nimport org.apache.hive.hcatalog.data.transfer.DataTransferFactory;\nimport org.apache.hive.hcatalog.data.transfer.HCatReader;\nimport org.apache.hive.hcatalog.data.transfer.HCatWriter;\nimport org.apache.hive.hcatalog.data.transfer.ReadEntity;\nimport org.apache.hive.hcatalog.data.transfer.ReaderContext;\nimport org.apache.hive.hcatalog.data.transfer.WriteEntity;\nimport org.apache.hive.hcatalog.data.transfer.WriterContext;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n/**\n * IO to read and write data using HCatalog.\n *\n * <h3>Reading using HCatalog</h3>\n *\n * <p>HCatalog source supports reading of HCatRecord from a HCatalog managed source, for eg. Hive.\n *\n * <p>To configure a HCatalog source, you must specify a metastore URI and a table name. Other\n * optional parameters are database &amp; filter For instance:\n *\n * <pre>{@code\n * Map<String, String> configProperties = new HashMap<>();\n * configProperties.put(\"hive.metastore.uris\",\"thrift://metastore-host:port\");\n *\n * pipeline\n *   .apply(HCatalogIO.read()\n *       .withConfigProperties(configProperties)\n *       .withDatabase(\"default\") //optional, assumes default if none specified\n *       .withTable(\"employee\")\n *       .withFilter(filterString) //optional, may be specified if the table is partitioned\n * }</pre>\n *\n * <h3>Writing using HCatalog</h3>\n *\n * <p>HCatalog sink supports writing of HCatRecord to a HCatalog managed source, for eg. Hive.\n *\n * <p>To configure a HCatalog sink, you must specify a metastore URI and a table name. Other\n * optional parameters are database, partition &amp; batchsize The destination table should exist\n * beforehand, the transform does not create a new table if it does not exist For instance:\n *\n * <pre>{@code\n * Map<String, String> configProperties = new HashMap<>();\n * configProperties.put(\"hive.metastore.uris\",\"thrift://metastore-host:port\");\n *\n * pipeline\n *   .apply(...)\n *   .apply(HCatalogIO.write()\n *       .withConfigProperties(configProperties)\n *       .withDatabase(\"default\") //optional, assumes default if none specified\n *       .withTable(\"employee\")\n *       .withPartition(partitionValues) //optional, may be specified if the table is partitioned\n *       .withBatchSize(1024L)) //optional, assumes a default batch size of 1024 if none specified\n * }</pre>\n */\n@Experimental(Experimental.Kind.SOURCE_SINK)\npublic class HCatalogIO {\n  private static final Logger LOG = LoggerFactory.getLogger(HCatalogIO.class);\n  private static final long BATCH_SIZE = 1024L;\n  private static final String DEFAULT_DATABASE = \"default\";\n  /** Write data to Hive. */\n  public static Write write() {\n    return new AutoValue_HCatalogIO_Write.Builder().setBatchSize(BATCH_SIZE).build();\n  }\n  /** Read data from Hive. */\n  public static Read read() {\n    return new AutoValue_HCatalogIO_Read.Builder().setDatabase(DEFAULT_DATABASE).build();\n  }\n  private HCatalogIO() {}\n  /** A {@link PTransform} to read data using HCatalog. */\n  @VisibleForTesting\n  @AutoValue\n  public abstract static class Read extends PTransform<PBegin, PCollection<HCatRecord>> {\n    @Nullable\n    abstract Map<String, String> getConfigProperties();\n    @Nullable\n    abstract String getDatabase();\n    @Nullable\n    abstract String getTable();\n    @Nullable\n    abstract String getFilter();\n    @Nullable\n    abstract ReaderContext getContext();\n    @Nullable\n    abstract Integer getSplitId();\n    abstract Builder toBuilder();\n<fim_suffix>    @AutoValue.Builder\n    abstract static class Builder {\n      abstract Builder setConfigProperties(Map<String, String> configProperties);\n      abstract Builder setDatabase(String database);\n      abstract Builder setTable(String table);\n      abstract Builder setFilter(String filter);\n      abstract Builder setSplitId(Integer splitId);\n      abstract Builder setContext(ReaderContext context);\n      abstract Read build();\n    }\n    /** Sets the configuration properties like metastore URI. */\n    public Read withConfigProperties(Map<String, String> configProperties) {\n      return toBuilder().setConfigProperties(new HashMap<>(configProperties)).build();\n    }\n    /** Sets the database name. This is optional, assumes 'default' database if none specified */\n    public Read withDatabase(String database) {\n      return toBuilder().setDatabase(database).build();\n    }\n    /** Sets the table name to read from. */\n    public Read withTable(String table) {\n      return toBuilder().setTable(table).build();\n    }\n    /** Sets the filter details. This is optional, assumes none if not specified */\n    public Read withFilter(String filter) {\n      return toBuilder().setFilter(filter).build();\n    }\n    Read withSplitId(int splitId) {\n      checkArgument(splitId >= 0, \"Invalid split id-\" + splitId);\n      return toBuilder().setSplitId(splitId).build();\n    }\n    Read withContext(ReaderContext context) {\n      return toBuilder().setContext(context).build();\n    }\n    @Override\n    public PCollection<HCatRecord> expand(PBegin input) {\n      checkArgument(getTable() != null, \"withTable() is required\");\n      checkArgument(getConfigProperties() != null, \"withConfigProperties() is required\");\n      return input.apply(org.apache.beam.sdk.io.Read.from(new BoundedHCatalogSource(this)));\n    }\n    @Override\n    public void populateDisplayData(DisplayData.Builder builder) {\n      super.populateDisplayData(builder);<fim_middle>// class below has no smell\n"}