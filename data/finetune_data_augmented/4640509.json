{"text": "<fim_prefix>\n<fim_suffix>  public static final class OpCopyBlockProto extends\n      com.google.protobuf.GeneratedMessage\n      implements OpCopyBlockProtoOrBuilder {\n    // Use OpCopyBlockProto.newBuilder() to construct.\n    private OpCopyBlockProto(Builder builder) {\n      super(builder);\n    }\n    private OpCopyBlockProto(boolean noInit) {}\n    private static final OpCopyBlockProto defaultInstance;\n    public static OpCopyBlockProto getDefaultInstance() {\n      return defaultInstance;\n    }\n    public OpCopyBlockProto getDefaultInstanceForType() {\n      return defaultInstance;\n    }\n    public static final com.google.protobuf.Descriptors.Descriptor\n        getDescriptor() {\n      return org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos.internal_static_OpCopyBlockProto_descriptor;\n    }\n    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable\n        internalGetFieldAccessorTable() {\n      return org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos.internal_static_OpCopyBlockProto_fieldAccessorTable;\n    }\n    private int bitField0_;\n    // required .BaseHeaderProto header = 1;\n    public static final int HEADER_FIELD_NUMBER = 1;\n    private org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos.BaseHeaderProto header_;\n    public boolean hasHeader() {\n      return ((bitField0_ & 0x00000001) == 0x00000001);\n    }\n    public org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos.BaseHeaderProto getHeader() {\n      return header_;\n    }\n    public org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos.BaseHeaderProtoOrBuilder getHeaderOrBuilder() {\n      return header_;\n    }\n    private void initFields() {\n      header_ = org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos.BaseHeaderProto.getDefaultInstance();\n    }\n    private byte memoizedIsInitialized = -1;\n    public final boolean isInitialized() {\n      byte isInitialized = memoizedIsInitialized;\n      if (isInitialized != -1) return isInitialized == 1;\n      if (!hasHeader()) {\n        memoizedIsInitialized = 0;\n        return false;\n      }\n      if (!getHeader().isInitialized()) {\n        memoizedIsInitialized = 0;\n        return false;\n      }\n      memoizedIsInitialized = 1;\n      return true;\n    }\n    public void writeTo(com.google.protobuf.CodedOutputStream output)\n                        throws java.io.IOException {\n      getSerializedSize();\n      if (((bitField0_ & 0x00000001) == 0x00000001)) {\n        output.writeMessage(1, header_);\n      }\n      getUnknownFields().writeTo(output);\n    }\n    private int memoizedSerializedSize = -1;\n    public int getSerializedSize() {\n      int size = memoizedSerializedSize;\n      if (size != -1) return size;\n      size = 0;\n      if (((bitField0_ & 0x00000001) == 0x00000001)) {\n        size += com.google.protobuf.CodedOutputStream\n          .computeMessageSize(1, header_);\n      }\n      size += getUnknownFields().getSerializedSize();\n      memoizedSerializedSize = size;\n      return size;\n    }\n    private static final long serialVersionUID = 0L;\n    @java.lang.Override\n    protected java.lang.Object writeReplace()\n        throws java.io.ObjectStreamException {\n      return super.writeReplace();\n    }\n    @java.lang.Override\n    public boolean equals(final java.lang.Object obj) {\n      if (obj == this) {\n       return true;\n      }\n      if (!(obj instanceof org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos.OpCopyBlockProto)) {\n        return super.equals(obj);\n      }\n      org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos.OpCopyBlockProto other = (org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos.OpCopyBlockProto) obj;\n      boolean result = true;\n      result = result && (hasHeader() == other.hasHeader());\n      if (hasHeader()) {\n        result = result && getHeader()\n            .equals(other.getHeader());\n      }\n      result = result &&\n          getUnknownFields().equals(other.getUnknownFields());\n      return result;\n    }\n    @java.lang.Override\n    public int hashCode() {\n      int hash = 41;\n      hash = (19 * hash) + getDescriptorForType().hashCode();\n      if (hasHeader()) {\n        hash = (37 * hash) + HEADER_FIELD_NUMBER;\n        hash = (53 * hash) + getHeader().hashCode();\n      }\n      hash = (29 * hash) + getUnknownFields().hashCode();\n      return hash;\n    }\n    public static org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos.OpCopyBlockProto parseFrom(\n        com.google.protobuf.ByteString data)\n        throws com.google.protobuf.InvalidProtocolBufferException {\n      return newBuilder().mergeFrom(data).buildParsed();\n    }\n    public static org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos.OpCopyBlockProto parseFrom(\n        com.google.protobuf.ByteString data,\n        com.google.protobuf.ExtensionRegistryLite extensionRegistry)\n        throws com.google.protobuf.InvalidProtocolBufferException {\n      return newBuilder().mergeFrom(data, extensionRegistry)\n               .buildParsed();\n    }\n    public static org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos.OpCopyBlockProto parseFrom(byte[] data)\n        throws com.google.protobuf.InvalidProtocolBufferException {\n      return newBuilder().mergeFrom(data).buildParsed();\n    }\n    public static org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos.OpCopyBlockProto parseFrom(\n        byte[] data,\n        com.google.protobuf.ExtensionRegistryLite extensionRegistry)\n        throws com.google.protobuf.InvalidProtocolBufferException {\n      return newBuilder().mergeFrom(data, extensionRegistry)\n               .buildParsed();\n    }\n    public static org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos.OpCopyBlockProto parseFrom(java.io.InputStream input)\n        throws java.io.IOException {\n      return newBuilder().mergeFrom(input).buildParsed();\n    }\n    public static org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos.OpCopyBlockProto parseFrom(\n        java.io.InputStream input,\n        com.google.protobuf.ExtensionRegistryLite extensionRegistry)\n        throws java.io.IOException {\n      return newBuilder().mergeFrom(input, extensionRegistry)\n               .buildParsed();\n    }\n    public static org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos.OpCopyBlockProto parseDelimitedFrom(java.io.InputStream input)\n        throws java.io.IOException {\n      Builder builder = newBuilder();\n      if (builder.mergeDelimitedFrom(input)) {\n        return builder.buildParsed();\n      } else {\n        return null;\n      }\n    }\n    public static org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos.OpCopyBlockProto parseDelimitedFrom(\n        java.io.InputStream input,\n        com.google.protobuf.ExtensionRegistryLite extensionRegistry)\n        throws java.io.IOException {\n      Builder builder = newBuilder();\n      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {\n        return builder.buildParsed();\n      } else {\n        return null;\n      }\n    }\n    public static org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos.OpCopyBlockProto parseFrom(\n        com.google.protobuf.CodedInputStream input)\n        throws java.io.IOException {\n      return newBuilder().mergeFrom(input).buildParsed();\n    }\n    public static org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos.OpCopyBlockProto parseFrom(\n        com.google.protobuf.CodedInputStream input,\n        com.google.protobuf.ExtensionRegistryLite extensionRegistry)\n        throws java.io.IOException {\n      return newBuilder().mergeFrom(input, extensionRegistry)\n               .buildParsed();\n    }\n    public static Builder newBuilder() { return Builder.create(); }\n    public Builder newBuilderForType() { return newBuilder(); }\n    public static Builder newBuilder(org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos.OpCopyBlockProto prototype) {\n      return newBuilder().mergeFrom(prototype);\n    }\n    public Builder toBuilder() { return newBuilder(this); }\n    @java.lang.Override\n    protected Builder newBuilderForType(\n        com.google.protobuf.GeneratedMessage.BuilderParent parent) {\n      Builder builder = new Builder(parent);\n      return builder;\n    }\n    public static final class Builder extends\n        com.google.protobuf.GeneratedMessage.Builder<Builder>\n       implements org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos.OpCopyBlockProtoOrBuilder {\n      public static final com.google.protobuf.Descriptors.Descriptor\n          getDescriptor() {\n        return org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos.internal_static_OpCopyBlockProto_descriptor;\n      }\n      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable\n          internalGetFieldAccessorTable() {\n        return org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos.internal_static_OpCopyBlockProto_fieldAccessorTable;\n      }\n      // Construct using org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos.OpCopyBlockProto.newBuilder()\n      private Builder() {\n        maybeForceBuilderInitialization();\n      }\n      private Builder(BuilderParent parent) {\n        super(parent);\n        maybeForceBuilderInitialization();\n      }\n      private void maybeForceBuilderInitialization() {<fim_middle>// class below is data class\n"}