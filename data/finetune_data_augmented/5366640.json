{"text": "<fim_prefix>import org.apache.pig.newplan.logical.relational.LogicalRelationalOperator;\nimport org.apache.pig.newplan.logical.relational.LogicalSchema;\nimport org.apache.pig.parser.QueryParserDriver;\nimport org.apache.pig.parser.QueryParserUtils;\nimport org.apache.pig.pen.ExampleGenerator;\nimport org.apache.pig.scripting.ScriptEngine;\nimport org.apache.pig.tools.grunt.GruntParser;\nimport org.apache.pig.tools.pigstats.EmptyPigStats;\nimport org.apache.pig.tools.pigstats.JobStats;\nimport org.apache.pig.tools.pigstats.OutputStats;\nimport org.apache.pig.tools.pigstats.PigStats;\nimport org.apache.pig.tools.pigstats.PigStats.JobGraph;\nimport org.apache.pig.tools.pigstats.ScriptState;\nimport org.apache.pig.validator.BlackAndWhitelistFilter;\nimport org.apache.pig.validator.PigCommandFilter;\nimport com.google.common.annotations.VisibleForTesting;\n/**\n *\n * A class for Java programs to connect to Pig. Typically a program will create a PigServer\n * instance. The programmer then registers queries using registerQuery() and\n * retrieves results using openIterator() or store(). After doing so, the\n * shutdown() method should be called to free any resources used by the current\n * PigServer instance. Not doing so could result in a memory leak.\n *\n */\n@InterfaceAudience.Public\n@InterfaceStability.Stable\npublic class PigServer {\n    protected final Log log = LogFactory.getLog(getClass());\n    public static final String PRETTY_PRINT_SCHEMA_PROPERTY = \"pig.pretty.print.schema\";\n    private static final String PIG_LOCATION_CHECK_STRICT = \"pig.location.check.strict\";\n    /*\n     * The data structure to support grunt shell operations.\n     * The grunt shell can only work on one graph at a time.\n     * If a script is contained inside another script, the grunt\n     * shell first saves the current graph on the stack and works\n     * on a new graph. After the nested script is done, the grunt\n     * shell pops up the saved graph and continues working on it.\n     */\n    protected final Deque<Graph> graphs = new LinkedList<Graph>();\n    /*\n     * The current Graph the grunt shell is working on.\n     */\n    private Graph currDAG;\n    protected final PigContext pigContext;\n    private String jobName;\n    private String jobPriority;\n    private final static AtomicInteger scopeCounter = new AtomicInteger(0);\n    protected final String scope = constructScope();\n    private boolean validateEachStatement = false;\n    private boolean skipParseInRegisterForBatch = false;\n    private final BlackAndWhitelistFilter filter;\n    private String constructScope() {\n        // scope servers for now as a session id\n        // String user = System.getProperty(\"user.name\", \"DEFAULT_USER_ID\");\n        // String date = (new Date()).toString();\n        // scope is not really used in the system right now. It will\n        // however make your explain statements look lengthy if set to\n        // username-date. For now let's simplify the scope, if a real\n        // scope is needed again, we might need to update all the\n        // operators to not include scope in their name().\n        return \"\" + scopeCounter.incrementAndGet();\n    }\n    @VisibleForTesting\n    public static void resetScope() {\n        scopeCounter.set(0);\n    }\n    /**\n     * @param execTypeString can be 'mapreduce' or 'local'.  Local mode will\n     * use Hadoop's local job runner to execute the job on the local machine.\n     * Mapreduce mode will connect to a cluster to execute the job. If\n     * execTypeString is not one of these two, Pig will deduce the ExecutionEngine\n     * if it is on the classpath and use it for the backend execution.\n     * @throws ExecException\n     * @throws IOException\n     */\n    public PigServer(String execTypeString) throws ExecException, IOException {\n        this(addExecTypeProperty(PropertiesUtil.loadDefaultProperties(), execTypeString));\n    }\n    public PigServer(String execTypeString, Properties properties) throws ExecException, IOException {\n        this(addExecTypeProperty(properties, execTypeString));\n    }\n    public PigServer(Properties properties) throws ExecException, IOException {\n        this(new PigContext(properties));\n    }\n    private static Properties addExecTypeProperty(Properties properties, String execType) {\n        properties.setProperty(\"exectype\", execType);\n        return properties;\n    }\n    /**\n     * @param execType execution type to start the engine.  Local mode will\n     * use Hadoop's local job runner to execute the job on the local machine.\n     * Mapreduce mode will connect to a cluster to execute the job.\n     * @throws ExecException\n     */\n    public PigServer(ExecType execType) throws ExecException {\n        this(execType, PropertiesUtil.loadDefaultProperties());\n    }\n    public PigServer(ExecType execType, Properties properties) throws ExecException {\n        this(new PigContext(execType, properties));\n    }\n    public PigServer(ExecType execType, Configuration conf) throws ExecException {\n        this(new PigContext(execType, conf));\n    }\n    public PigServer(PigContext context) throws ExecException {\n        this(context, true);\n    }\n<fim_suffix>    public PigServer(PigContext context, boolean connect) throws ExecException {\n        this.pigContext = context;\n        currDAG = new Graph(false);\n        jobName = pigContext.getProperties().getProperty(\n                PigContext.JOB_NAME,\n                PigContext.JOB_NAME_PREFIX + \":DefaultJobName\");\n        if (connect) {\n            pigContext.connect();\n        }\n        this.filter = new BlackAndWhitelistFilter(this);\n        addHadoopProperties();\n        addJarsFromProperties();\n        markPredeployedJarsFromProperties();\n        if (ScriptState.get() == null) {\n            // If Pig was started via command line, ScriptState should have been\n            // already initialized in Main. If so, we should not overwrite it.\n            ScriptState.start(pigContext.getExecutionEngine().instantiateScriptState());\n        }\n        PigStats.start(pigContext.getExecutionEngine().instantiatePigStats());\n        // log ATS event includes the caller context\n        String auditId = PigATSClient.getPigAuditId(pigContext);\n        String callerId = (String)pigContext.getProperties().get(PigConfiguration.PIG_LOG_TRACE_ID);\n        log.info(\"Pig Script ID for the session: \" + auditId);\n        if (callerId != null) {\n            log.info(\"Caller ID for session: \" + callerId);\n        }\n        if (Boolean.parseBoolean(pigContext.getProperties()\n                .getProperty(PigConfiguration.PIG_ATS_ENABLED))) {\n            if (Boolean.parseBoolean(pigContext.getProperties()\n                    .getProperty(\"yarn.timeline-service.enabled\", \"false\"))) {\n                PigATSClient.ATSEvent event = new PigATSClient.ATSEvent(auditId, callerId);\n                try {\n                    PigATSClient.getInstance().logEvent(event);\n                } catch (Exception e) {\n                    log.warn(\"Error posting to ATS: \", e);\n                }\n            } else {\n                log.warn(\"ATS is disabled since\"\n                        + \" yarn.timeline-service.enabled set to false\");\n            }\n        }\n        // set hdfs caller context\n        Class callerContextClass = null;\n        try {\n            callerContextClass = Class.forName(\"org.apache.hadoop.ipc.CallerContext\");\n        } catch (ClassNotFoundException e) {\n            // If pre-Hadoop 2.8.0, skip setting CallerContext\n        }\n        if (callerContextClass != null) {\n            try {\n                // Reflection for the following code since it is only available since hadoop 2.8.0:\n                // CallerContext hdfsContext = new CallerContext.Builder(auditId).build();\n                // CallerContext.setCurrent(hdfsContext);\n                Class callerContextBuilderClass = Class.forName(\"org.apache.hadoop.ipc.CallerContext$Builder\");\n                Constructor callerContextBuilderConstruct = callerContextBuilderClass.getConstructor(String.class);\n                Object builder = callerContextBuilderConstruct.newInstance(auditId);\n                Method builderBuildMethod = builder.getClass().getMethod(\"build\");\n                Object hdfsContext = builderBuildMethod.invoke(builder);\n                Method callerContextSetCurrentMethod = callerContextClass.getMethod(\"setCurrent\", hdfsContext.getClass());\n                callerContextSetCurrentMethod.invoke(callerContextClass, hdfsContext);\n            } catch (Exception e) {\n                // Shall not happen unless API change in future Hadoop commons\n                throw new ExecException(e);\n            }\n        }\n    }<fim_middle>// function below is long method\n"}