{"text": "<fim_prefix>    RawKeyValueIterator rIter = null;\n    ShuffleConsumerPlugin shuffleConsumerPlugin = null;\n    Class combinerClass = conf.getCombinerClass();\n    CombineOutputCollector combineCollector = \n      (null != combinerClass) ? \n     new CombineOutputCollector(reduceCombineOutputCounter, reporter, conf) : null;\n    Class<? extends ShuffleConsumerPlugin> clazz =\n          job.getClass(MRConfig.SHUFFLE_CONSUMER_PLUGIN, Shuffle.class, ShuffleConsumerPlugin.class);\n    shuffleConsumerPlugin = ReflectionUtils.newInstance(clazz, job);\n    LOG.info(\"Using ShuffleConsumerPlugin: \" + shuffleConsumerPlugin);\n    ShuffleConsumerPlugin.Context shuffleContext = \n      new ShuffleConsumerPlugin.Context(getTaskID(), job, FileSystem.getLocal(job), umbilical, \n                  super.lDirAlloc, reporter, codec, \n                  combinerClass, combineCollector, \n                  spilledRecordsCounter, reduceCombineInputCounter,\n                  shuffledMapsCounter,\n                  reduceShuffleBytes, failedShuffleCounter,\n                  mergedMapOutputsCounter,\n                  taskStatus, copyPhase, sortPhase, this,\n                  mapOutputFile, localMapFiles);\n    shuffleConsumerPlugin.init(shuffleContext);\n    rIter = shuffleConsumerPlugin.run();\n    // free up the data structures\n    mapOutputFilesOnDisk.clear();\n    sortPhase.complete();                         // sort is complete\n    setPhase(TaskStatus.Phase.REDUCE); \n    statusUpdate(umbilical);\n    Class keyClass = job.getMapOutputKeyClass();\n    Class valueClass = job.getMapOutputValueClass();\n    RawComparator comparator = job.getOutputValueGroupingComparator();\n    if (useNewApi) {\n      runNewReducer(job, umbilical, reporter, rIter, comparator, \n                    keyClass, valueClass);\n    } else {\n      runOldReducer(job, umbilical, reporter, rIter, comparator, \n                    keyClass, valueClass);\n    }\n    shuffleConsumerPlugin.close();\n    done(umbilical, reporter);\n  }\n  @SuppressWarnings(\"unchecked\")\n  private <INKEY,INVALUE,OUTKEY,OUTVALUE>\n  void runOldReducer(JobConf job,\n                     TaskUmbilicalProtocol umbilical,\n                     final TaskReporter reporter,\n                     RawKeyValueIterator rIter,\n                     RawComparator<INKEY> comparator,\n                     Class<INKEY> keyClass,\n                     Class<INVALUE> valueClass) throws IOException {\n    Reducer<INKEY,INVALUE,OUTKEY,OUTVALUE> reducer = \n      ReflectionUtils.newInstance(job.getReducerClass(), job);\n    // make output collector\n    String finalName = getOutputName(getPartition());\n    RecordWriter<OUTKEY, OUTVALUE> out = new OldTrackingRecordWriter<OUTKEY, OUTVALUE>(\n        this, job, reporter, finalName);\n    final RecordWriter<OUTKEY, OUTVALUE> finalOut = out;\n    OutputCollector<OUTKEY,OUTVALUE> collector = \n      new OutputCollector<OUTKEY,OUTVALUE>() {\n        public void collect(OUTKEY key, OUTVALUE value)\n          throws IOException {\n          finalOut.write(key, value);\n          // indicate that progress update needs to be sent\n          reporter.progress();\n        }\n      };\n    // apply reduce function\n    try {\n      //increment processed counter only if skipping feature is enabled\n      boolean incrProcCount = SkipBadRecords.getReducerMaxSkipGroups(job)>0 &&\n        SkipBadRecords.getAutoIncrReducerProcCount(job);\n      ReduceValuesIterator<INKEY,INVALUE> values = isSkipping() ? \n          new SkippingReduceValuesIterator<INKEY,INVALUE>(rIter, \n              comparator, keyClass, valueClass, \n              job, reporter, umbilical) :\n          new ReduceValuesIterator<INKEY,INVALUE>(rIter, \n          job.getOutputValueGroupingComparator(), keyClass, valueClass, \n          job, reporter);\n      values.informReduceProgress();\n      while (values.more()) {\n        reduceInputKeyCounter.increment(1);\n        reducer.reduce(values.getKey(), values, collector, reporter);\n        if(incrProcCount) {\n          reporter.incrCounter(SkipBadRecords.COUNTER_GROUP, \n              SkipBadRecords.COUNTER_REDUCE_PROCESSED_GROUPS, 1);\n        }\n        values.nextKey();\n        values.informReduceProgress();\n      }\n      reducer.close();\n      reducer = null;\n      out.close(reporter);\n      out = null;\n    } finally {\n      IOUtils.cleanup(LOG, reducer);\n      closeQuietly(out, reporter);\n    }\n  }\n  static class OldTrackingRecordWriter<K, V> implements RecordWriter<K, V> {\n    private final RecordWriter<K, V> real;\n    private final org.apache.hadoop.mapred.Counters.Counter reduceOutputCounter;\n    private final org.apache.hadoop.mapred.Counters.Counter fileOutputByteCounter;\n    private final List<Statistics> fsStats;\n    @SuppressWarnings({ \"deprecation\", \"unchecked\" })\n    public OldTrackingRecordWriter(ReduceTask reduce, JobConf job,\n        TaskReporter reporter, String finalName) throws IOException {\n      this.reduceOutputCounter = reduce.reduceOutputCounter;\n      this.fileOutputByteCounter = reduce.fileOutputByteCounter;\n      List<Statistics> matchedStats = null;\n      if (job.getOutputFormat() instanceof FileOutputFormat) {\n        matchedStats = getFsStatistics(FileOutputFormat.getOutputPath(job), job);\n      }\n      fsStats = matchedStats;\n      FileSystem fs = FileSystem.get(job);\n      long bytesOutPrev = getOutputBytes(fsStats);\n      this.real = job.getOutputFormat().getRecordWriter(fs, job, finalName,\n          reporter);\n      long bytesOutCurr = getOutputBytes(fsStats);\n      fileOutputByteCounter.increment(bytesOutCurr - bytesOutPrev);\n    }\n    @Override\n    public void write(K key, V value) throws IOException {\n      long bytesOutPrev = getOutputBytes(fsStats);\n      real.write(key, value);\n      long bytesOutCurr = getOutputBytes(fsStats);\n      fileOutputByteCounter.increment(bytesOutCurr - bytesOutPrev);\n      reduceOutputCounter.increment(1);\n    }\n    @Override\n    public void close(Reporter reporter) throws IOException {\n      long bytesOutPrev = getOutputBytes(fsStats);\n      real.close(reporter);\n      long bytesOutCurr = getOutputBytes(fsStats);\n      fileOutputByteCounter.increment(bytesOutCurr - bytesOutPrev);\n    }\n    private long getOutputBytes(List<Statistics> stats) {\n      if (stats == null) return 0;\n      long bytesWritten = 0;\n      for (Statistics stat: stats) {\n        bytesWritten = bytesWritten + stat.getBytesWritten();\n      }\n      return bytesWritten;\n    }\n  }\n<fim_suffix>  static class NewTrackingRecordWriter<K,V> \n      extends org.apache.hadoop.mapreduce.RecordWriter<K,V> {\n    private final org.apache.hadoop.mapreduce.RecordWriter<K,V> real;\n    private final org.apache.hadoop.mapreduce.Counter outputRecordCounter;\n    private final org.apache.hadoop.mapreduce.Counter fileOutputByteCounter;\n    private final List<Statistics> fsStats;\n    @SuppressWarnings(\"unchecked\")\n    NewTrackingRecordWriter(ReduceTask reduce,\n        org.apache.hadoop.mapreduce.TaskAttemptContext taskContext)\n        throws InterruptedException, IOException {\n      this.outputRecordCounter = reduce.reduceOutputCounter;\n      this.fileOutputByteCounter = reduce.fileOutputByteCounter;\n      List<Statistics> matchedStats = null;\n      if (reduce.outputFormat instanceof org.apache.hadoop.mapreduce.lib.output.FileOutputFormat) {\n        matchedStats = getFsStatistics(org.apache.hadoop.mapreduce.lib.output.FileOutputFormat\n            .getOutputPath(taskContext), taskContext.getConfiguration());\n      }\n      fsStats = matchedStats;\n      long bytesOutPrev = getOutputBytes(fsStats);\n      this.real = (org.apache.hadoop.mapreduce.RecordWriter<K, V>) reduce.outputFormat\n          .getRecordWriter(taskContext);\n      long bytesOutCurr = getOutputBytes(fsStats);\n      fileOutputByteCounter.increment(bytesOutCurr - bytesOutPrev);\n    }\n    @Override\n    public void close(TaskAttemptContext context) throws IOException,\n    InterruptedException {\n      long bytesOutPrev = getOutputBytes(fsStats);\n      real.close(context);\n      long bytesOutCurr = getOutputBytes(fsStats);\n      fileOutputByteCounter.increment(bytesOutCurr - bytesOutPrev);\n    }\n    @Override\n    public void write(K key, V value) throws IOException, InterruptedException {\n      long bytesOutPrev = getOutputBytes(fsStats);\n      real.write(key,value);\n      long bytesOutCurr = getOutputBytes(fsStats);\n      fileOutputByteCounter.increment(bytesOutCurr - bytesOutPrev);\n      outputRecordCounter.increment(1);\n    }\n    private long getOutputBytes(List<Statistics> stats) {\n      if (stats == null) return 0;\n      long bytesWritten = 0;\n      for (Statistics stat: stats) {\n        bytesWritten = bytesWritten + stat.getBytesWritten();\n      }\n      return bytesWritten;\n    }\n  }<fim_middle>// class below has no smell\n"}