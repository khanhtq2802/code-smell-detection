{"text": "<fim_prefix>\n<fim_suffix>public class SparkCubingMerge extends AbstractApplication implements Serializable {\n    protected static final Logger logger = LoggerFactory.getLogger(SparkCubingMerge.class);\n    public static final Option OPTION_CUBE_NAME = OptionBuilder.withArgName(BatchConstants.ARG_CUBE_NAME).hasArg()\n            .isRequired(true).withDescription(\"Cube Name\").create(BatchConstants.ARG_CUBE_NAME);\n    public static final Option OPTION_SEGMENT_ID = OptionBuilder.withArgName(\"segment\").hasArg().isRequired(true)\n            .withDescription(\"Cube Segment Id\").create(\"segmentId\");\n    public static final Option OPTION_META_URL = OptionBuilder.withArgName(\"metaUrl\").hasArg().isRequired(true)\n            .withDescription(\"HDFS metadata url\").create(\"metaUrl\");\n    public static final Option OPTION_OUTPUT_PATH = OptionBuilder.withArgName(BatchConstants.ARG_OUTPUT).hasArg()\n            .isRequired(true).withDescription(\"HFile output path\").create(BatchConstants.ARG_OUTPUT);\n    public static final Option OPTION_INPUT_PATH = OptionBuilder.withArgName(BatchConstants.ARG_INPUT).hasArg()\n            .isRequired(true).withDescription(\"Cuboid files PATH\").create(BatchConstants.ARG_INPUT);\n    private Options options;\n    private String cubeName;\n    private String metaUrl;\n    public SparkCubingMerge() {\n        options = new Options();\n        options.addOption(OPTION_META_URL);\n        options.addOption(OPTION_CUBE_NAME);\n        options.addOption(OPTION_SEGMENT_ID);\n        options.addOption(OPTION_INPUT_PATH);\n        options.addOption(OPTION_OUTPUT_PATH);\n    }\n    @Override\n    protected Options getOptions() {\n        return options;\n    }\n    @Override\n    protected void execute(OptionsHelper optionsHelper) throws Exception {\n        this.metaUrl = optionsHelper.getOptionValue(OPTION_META_URL);\n        this.cubeName = optionsHelper.getOptionValue(OPTION_CUBE_NAME);\n        final String inputPath = optionsHelper.getOptionValue(OPTION_INPUT_PATH);\n        final String segmentId = optionsHelper.getOptionValue(OPTION_SEGMENT_ID);\n        final String outputPath = optionsHelper.getOptionValue(OPTION_OUTPUT_PATH);\n        Class[] kryoClassArray = new Class[] { Class.forName(\"scala.reflect.ClassTag$$anon$1\") };\n        SparkConf conf = new SparkConf().setAppName(\"Merge segments for cube:\" + cubeName + \", segment \" + segmentId);\n        //serialization conf\n        conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\");\n        conf.set(\"spark.kryo.registrator\", \"org.apache.kylin.engine.spark.KylinKryoRegistrator\");\n        conf.set(\"spark.kryo.registrationRequired\", \"true\").registerKryoClasses(kryoClassArray);\n        try (JavaSparkContext sc = new JavaSparkContext(conf)) {\n            SparkUtil.modifySparkHadoopConfiguration(sc.sc()); // set dfs.replication=2 and enable compress\n            KylinSparkJobListener jobListener = new KylinSparkJobListener();\n            sc.sc().addSparkListener(jobListener);\n            HadoopUtil.deletePath(sc.hadoopConfiguration(), new Path(outputPath));\n            final SerializableConfiguration sConf = new SerializableConfiguration(sc.hadoopConfiguration());\n            final KylinConfig envConfig = AbstractHadoopJob.loadKylinConfigFromHdfs(sConf, metaUrl);\n            final CubeInstance cubeInstance = CubeManager.getInstance(envConfig).getCube(cubeName);\n            final CubeDesc cubeDesc = CubeDescManager.getInstance(envConfig).getCubeDesc(cubeInstance.getDescName());\n            final CubeSegment cubeSegment = cubeInstance.getSegmentById(segmentId);\n            final CubeStatsReader cubeStatsReader = new CubeStatsReader(cubeSegment, envConfig);\n            logger.info(\"Input path: {}\", inputPath);\n            logger.info(\"Output path: {}\", outputPath);\n            final Job job = Job.getInstance(sConf.get());\n            SparkUtil.setHadoopConfForCuboid(job, cubeSegment, metaUrl);\n            final MeasureAggregators aggregators = new MeasureAggregators(cubeDesc.getMeasures());\n            final Function2 reduceFunction = new Function2<Object[], Object[], Object[]>() {\n                @Override\n                public Object[] call(Object[] input1, Object[] input2) throws Exception {\n                    Object[] measureObjs = new Object[input1.length];\n                    aggregators.aggregate(input1, input2, measureObjs);\n                    return measureObjs;\n                }\n            };\n            final PairFunction convertTextFunction = new PairFunction<Tuple2<Text, Object[]>, org.apache.hadoop.io.Text, org.apache.hadoop.io.Text>() {\n                private transient volatile boolean initialized = false;\n                BufferedMeasureCodec codec;\n                @Override\n                public Tuple2<org.apache.hadoop.io.Text, org.apache.hadoop.io.Text> call(Tuple2<Text, Object[]> tuple2)\n                        throws Exception {\n                    if (initialized == false) {\n                        synchronized (SparkCubingMerge.class) {\n                            if (initialized == false) {\n                                synchronized (SparkCubingMerge.class) {\n                                    if (initialized == false) {\n                                        KylinConfig kylinConfig = AbstractHadoopJob.loadKylinConfigFromHdfs(sConf, metaUrl);\n                                        try (KylinConfig.SetAndUnsetThreadLocalConfig autoUnset = KylinConfig\n                                                .setAndUnsetThreadLocalConfig(kylinConfig)) {\n                                            CubeDesc desc = CubeDescManager.getInstance(kylinConfig).getCubeDesc(cubeName);\n                                            codec = new BufferedMeasureCodec(desc.getMeasures());\n                                            initialized = true;\n                                        }\n                                    }\n                                }\n                            }\n                        }\n                    }\n                    ByteBuffer valueBuf = codec.encode(tuple2._2());\n                    byte[] encodedBytes = new byte[valueBuf.position()];\n                    System.arraycopy(valueBuf.array(), 0, encodedBytes, 0, valueBuf.position());\n                    return new Tuple2<>(tuple2._1(), new org.apache.hadoop.io.Text(encodedBytes));\n                }\n            };\n            final int totalLevels = cubeSegment.getCuboidScheduler().getBuildLevel();\n            final String[] inputFolders = StringSplitter.split(inputPath, \",\");\n            FileSystem fs = HadoopUtil.getWorkingFileSystem();\n            boolean isLegacyMode = false;\n            for (String inputFolder : inputFolders) {\n                Path baseCuboidPath = new Path(BatchCubingJobBuilder2.getCuboidOutputPathsByLevel(inputFolder, 0));\n                if (fs.exists(baseCuboidPath) == false) {\n                    // doesn't exist sub folder, that means the merged cuboid in one folder (not by layer)\n                    isLegacyMode = true;\n                    break;\n                }\n            }\n            if (isLegacyMode == true) {\n                // merge all layer's cuboid at once, this might be hard for Spark\n                List<JavaPairRDD<Text, Object[]>> mergingSegs = Lists.newArrayListWithExpectedSize(inputFolders.length);\n                for (int i = 0; i < inputFolders.length; i++) {\n                    String path = inputFolders[i];\n                    JavaPairRDD segRdd = SparkUtil.parseInputPath(path, fs, sc, Text.class, Text.class);\n                    CubeSegment sourceSegment = findSourceSegment(path, cubeInstance);\n                    // re-encode with new dictionaries\n                    JavaPairRDD<Text, Object[]> newEcoddedRdd = segRdd.mapToPair(new ReEncodeCuboidFunction(cubeName,\n                            sourceSegment.getUuid(), cubeSegment.getUuid(), metaUrl, sConf));\n                    mergingSegs.add(newEcoddedRdd);\n                }\n                FileOutputFormat.setOutputPath(job, new Path(outputPath));\n                sc.union(mergingSegs.toArray(new JavaPairRDD[mergingSegs.size()]))\n                        .reduceByKey(reduceFunction, SparkUtil.estimateTotalPartitionNum(cubeStatsReader, envConfig))\n                        .mapToPair(convertTextFunction).saveAsNewAPIHadoopDataset(job.getConfiguration());\n            } else {\n                // merge by layer\n                for (int level = 0; level <= totalLevels; level++) {\n                    List<JavaPairRDD<Text, Object[]>> mergingSegs = Lists.newArrayList();\n                    for (int i = 0; i < inputFolders.length; i++) {\n                        String path = inputFolders[i];\n                        CubeSegment sourceSegment = findSourceSegment(path, cubeInstance);\n                        final String cuboidInputPath = BatchCubingJobBuilder2.getCuboidOutputPathsByLevel(path, level);\n                        JavaPairRDD<Text, Text> segRdd = sc.sequenceFile(cuboidInputPath, Text.class, Text.class);\n                        // re-encode with new dictionaries\n                        JavaPairRDD<Text, Object[]> newEcoddedRdd = segRdd.mapToPair(new ReEncodeCuboidFunction(cubeName,<fim_middle>// class below is blob\n"}