{"text": "<fim_prefix>                          final CatalogProtos.FragmentProto[] fragments) throws PhysicalPlanningException {\n    this(context, plan);\n    mergedInputFragments = new ArrayList<>();\n    for (CatalogProtos.FragmentProto proto : fragments) {\n      FileFragment fragment = FragmentConvertor.convert(context.getConf(), proto);\n      mergedInputFragments.add(new Chunk(inSchema, fragment, scanNode.getTableDesc().getMeta()));\n    }\n  }\n  public ExternalSortExec(final TaskAttemptContext context, final SortNode plan, final PhysicalExec child)\n      throws IOException {\n    this(context, plan);\n    setChild(child);\n  }\n  @Override\n  public void init() throws IOException {\n    if(allocatedCoreNum > 1) {\n      this.executorService = Executors.newFixedThreadPool(this.allocatedCoreNum);\n    }\n    this.sortTmpDir = getExecutorTmpDir();\n    int initialArraySize = context.getQueryContext().getInt(SessionVars.SORT_LIST_SIZE);\n    this.inMemoryTable = new UnSafeTupleList(SchemaUtil.toDataTypes(inSchema), initialArraySize);\n    this.unSafeComparator = new UnSafeComparator(inSchema, sortSpecs);\n    this.primitiveComparator = new PrimitiveComparator(inSchema, sortSpecs);\n    super.init();\n  }\n  public SortNode getPlan() {\n    return this.plan;\n  }\n  private List<UnSafeTuple> sort(UnSafeTupleList tupleBlock) {\n    switch (sortAlgorithm) {\n      case TIM:\n        return OffHeapRowBlockUtils.sort(tupleBlock, unSafeComparator);\n      case MSD_RADIX:\n        return RadixSort.sort(context.getQueryContext(), tupleBlock, inSchema, sortSpecs, unSafeComparator);\n      default:\n        // The below line is not reachable. So, an exception should be thrown if it is executed.\n        throw new TajoRuntimeException(new UnsupportedException(sortAlgorithm.name()));\n    }\n  }\n  /**\n   * Sort a tuple block and store them into a chunk file\n   */\n  private Chunk sortAndStoreChunk(int chunkId, UnSafeTupleList tupleBlock)\n      throws IOException {\n    int rowNum = tupleBlock.size();\n    long sortStart = System.currentTimeMillis();\n    this.sort(tupleBlock);\n    long sortEnd = System.currentTimeMillis();\n    long chunkWriteStart = System.currentTimeMillis();\n    Path outputPath = getChunkPathForWrite(0, chunkId);\n    final DirectRawFileWriter appender =\n        new DirectRawFileWriter(context.getConf(), null, inSchema, intermediateMeta, outputPath);\n    appender.init();\n    for (Tuple t : tupleBlock) {\n      appender.addTuple(t);\n    }\n    appender.close();\n    long chunkWriteEnd = System.currentTimeMillis();\n    info(LOG, \"Chunk #\" + chunkId + \" sort and written (\" +\n        FileUtil.humanReadableByteCount(appender.getOffset(), false) + \" bytes, \" + rowNum + \" rows, \" +\n        \"sort time: \" + (sortEnd - sortStart) + \" msec, \" +\n        \"write time: \" + (chunkWriteEnd - chunkWriteStart) + \" msec)\");\n    FileFragment frag = new FileFragment(\"\", outputPath, 0,\n        new File(localFS.makeQualified(outputPath).toUri()).length());\n    return new Chunk(inSchema, frag, intermediateMeta);\n  }\n  /**\n   * It divides all tuples into a number of chunks, then sort for each chunk.\n   *\n   * @return All paths of chunks\n   * @throws java.io.IOException\n   */\n  private List<Chunk> sortAndStoreAllChunks() throws IOException {\n    Tuple tuple;\n    List<Chunk> chunkPaths = new ArrayList<>();\n    int chunkId = 0;\n    long runStartTime = System.currentTimeMillis();\n    while (!context.isStopped() && (tuple = child.next()) != null) { // partition sort start\n      inMemoryTable.addTuple(tuple);\n      if (inMemoryTable.usedMem() > sortBufferBytesNum) { // if input data exceeds main-memory at least once\n        long runEndTime = System.currentTimeMillis();\n        info(LOG, \"Chunk #\" + chunkId + \" run loading time: \" + (runEndTime - runStartTime) + \" msec\");\n        runStartTime = runEndTime;\n        info(LOG, \"Memory consumption exceeds \" + FileUtil.humanReadableByteCount(inMemoryTable.usedMem(), false));\n        chunkPaths.add(sortAndStoreChunk(chunkId, inMemoryTable));\n        inMemoryTable.clear();\n        chunkId++;\n        // When the volume of sorting data once exceed the size of sort buffer,\n        // the total progress of this external sort is divided into two parts.\n        // In contrast, if the data fits in memory, the progress is only one part.\n        //\n        // When the progress is divided into two parts, the first part sorts tuples on memory and stores them\n        // into a chunk. The second part merges stored chunks into fewer chunks, and it continues until the number\n        // of merged chunks is fewer than the default fanout.\n        //\n        // The fact that the code reach here means that the first chunk has been just stored.\n        // That is, the progress was divided into two parts.\n        // So, it multiply the progress of the children operator and 0.5f.\n        progress = child.getProgress() * 0.5f;\n      }\n    }\n    if(inMemoryTable.size() > 0) { //if there are at least one or more input tuples\n      //store the remain data into a memory chunk.\n      chunkPaths.add(new Chunk(inSchema, inMemoryTable, intermediateMeta));\n    }\n    // get total loaded (or stored) bytes and total row numbers\n    TableStats childTableStats = child.getInputStats();\n    if (childTableStats != null) {\n      inputBytes = childTableStats.getNumBytes();\n    }\n    return chunkPaths;\n  }\n  /**\n   * Get a local path from all temporal paths in round-robin manner.\n   */\n  private synchronized Path getChunkPathForWrite(int level, int chunkId) throws IOException {\n    return localFS.makeQualified(localDirAllocator.getLocalPathForWrite(\n        sortTmpDir + \"/\" + level + \"_\" + chunkId, context.getConf()));\n  }\n  @Override\n  public Tuple next() throws IOException {\n    if (!sorted) { // if not sorted, first sort all data\n      // if input files are given, it starts merging directly.\n      if (mergedInputFragments != null) {\n        try {\n          this.result = externalMergeAndSort(mergedInputFragments);\n          this.inputBytes = result.getInputStats().getNumBytes();\n        } catch (Exception e) {\n          throw new PhysicalPlanningException(e);\n        }\n      } else {\n        // Try to sort all data, and store them as multiple chunks if memory exceeds\n        long startTimeOfChunkSplit = System.currentTimeMillis();\n        List<Chunk> chunks = sortAndStoreAllChunks();\n        long endTimeOfChunkSplit = System.currentTimeMillis();\n        info(LOG, chunks.size() + \" Chunks creation time: \" + (endTimeOfChunkSplit - startTimeOfChunkSplit) + \" msec\");\n        if(chunks.size() == 0) {\n          this.result = new NullScanner(context.getConf(), inSchema, intermediateMeta, null);\n        } else {\n          try {\n            this.result = externalMergeAndSort(chunks);\n          } catch (Exception e) {\n            throw new PhysicalPlanningException(e);\n          }\n        }\n      }\n      sorted = true;\n      result.init();\n      // if loaded and sorted, we assume that it proceeds the half of one entire external sort operation.\n      progress = 0.5f;\n    }\n    return result.next();\n  }\n<fim_suffix>  private int calculateFanout(int remainInputChunks, int inputNum, int outputNum, int startIdx) {\n    int computedFanout = Math.min(remainInputChunks, defaultFanout);\n    // Why should we detect an opportunity for unbalanced merge?\n    //\n    // Assume that a fanout is given by 8 and there are 10 chunks.\n    // If we firstly merge 3 chunks into one chunk, there remain only 8 chunks.\n    // Then, we can just finish the merge phase even though we don't complete merge phase on all chunks.\n    if (checkIfCanBeUnbalancedMerged(inputNum - (startIdx + computedFanout), outputNum + 1)) {\n      int candidateFanout = computedFanout;\n      while (checkIfCanBeUnbalancedMerged(inputNum - (startIdx + candidateFanout), outputNum + 1)) {\n        candidateFanout--;\n      }\n      int beforeFanout = computedFanout;\n      if (computedFanout > candidateFanout + 1) {\n        computedFanout = candidateFanout + 1;\n        info(LOG, \"Fanout reduced for unbalanced merge: \" + beforeFanout + \" -> \" + computedFanout);\n      }\n    }\n    return computedFanout;\n  }<fim_middle>// function below has no smell\n"}