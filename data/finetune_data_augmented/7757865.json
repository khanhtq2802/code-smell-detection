{"text": "<fim_prefix>    }\n    handOffWaitList.removeAll(handoffFinished);\n  }\n  private void publishAndRegisterHandoff(SequenceMetadata<PartitionIdType, SequenceOffsetType> sequenceMetadata)\n  {\n    log.info(\"Publishing segments for sequence [%s]\", sequenceMetadata);\n    final ListenableFuture<SegmentsAndMetadata> publishFuture = Futures.transform(\n        driver.publish(\n            sequenceMetadata.createPublisher(this, toolbox, ioConfig.isUseTransaction()),\n            sequenceMetadata.getCommitterSupplier(this, stream, lastPersistedOffsets).get(),\n            Collections.singletonList(sequenceMetadata.getSequenceName())\n        ),\n        (Function<SegmentsAndMetadata, SegmentsAndMetadata>) publishedSegmentsAndMetadata -> {\n          if (publishedSegmentsAndMetadata == null) {\n            throw new ISE(\n                \"Transaction failure publishing segments for sequence [%s]\",\n                sequenceMetadata\n            );\n          } else {\n            return publishedSegmentsAndMetadata;\n          }\n        }\n    );\n    publishWaitList.add(publishFuture);\n    // Create a handoffFuture for every publishFuture. The created handoffFuture must fail if publishFuture fails.\n    final SettableFuture<SegmentsAndMetadata> handoffFuture = SettableFuture.create();\n    handOffWaitList.add(handoffFuture);\n    Futures.addCallback(\n        publishFuture,\n        new FutureCallback<SegmentsAndMetadata>()\n        {\n          @Override\n          public void onSuccess(SegmentsAndMetadata publishedSegmentsAndMetadata)\n          {\n            log.info(\n                \"Published segments %s with metadata[%s].\",\n                Lists.transform(publishedSegmentsAndMetadata.getSegments(), DataSegment::getId),\n                Preconditions.checkNotNull(publishedSegmentsAndMetadata.getCommitMetadata(), \"commitMetadata\")\n            );\n            sequences.remove(sequenceMetadata);\n            publishingSequences.remove(sequenceMetadata.getSequenceName());\n            try {\n              persistSequences();\n            }\n            catch (IOException e) {\n              log.error(e, \"Unable to persist state, dying\");\n              handoffFuture.setException(e);\n              throw new RuntimeException(e);\n            }\n            Futures.transform(\n                driver.registerHandoff(publishedSegmentsAndMetadata),\n                new Function<SegmentsAndMetadata, Void>()\n                {\n                  @Nullable\n                  @Override\n                  public Void apply(@Nullable SegmentsAndMetadata handoffSegmentsAndMetadata)\n                  {\n                    if (handoffSegmentsAndMetadata == null) {\n                      log.warn(\n                          \"Failed to handoff segments %s\",\n                          Lists.transform(publishedSegmentsAndMetadata.getSegments(), DataSegment::getId)\n                      );\n                    }\n                    handoffFuture.set(handoffSegmentsAndMetadata);\n                    return null;\n                  }\n                }\n            );\n          }\n          @Override\n          public void onFailure(Throwable t)\n          {\n            log.error(t, \"Error while publishing segments for sequenceNumber[%s]\", sequenceMetadata);\n            handoffFuture.setException(t);\n          }\n        }\n    );\n  }\n  private static File getSequencesPersistFile(TaskToolbox toolbox)\n  {\n    return new File(toolbox.getPersistDir(), \"sequences.json\");\n  }\n  private boolean restoreSequences() throws IOException\n  {\n    final File sequencesPersistFile = getSequencesPersistFile(toolbox);\n    if (sequencesPersistFile.exists()) {\n      sequences = new CopyOnWriteArrayList<>(\n          toolbox.getObjectMapper().<List<SequenceMetadata<PartitionIdType, SequenceOffsetType>>>readValue(\n              sequencesPersistFile,\n              getSequenceMetadataTypeReference()\n          )\n      );\n      return true;\n    } else {\n      return false;\n    }\n  }\n  private synchronized void persistSequences() throws IOException\n  {\n    log.info(\"Persisting Sequences Metadata [%s]\", sequences);\n    toolbox.getObjectMapper().writerWithType(\n        getSequenceMetadataTypeReference()\n    ).writeValue(getSequencesPersistFile(toolbox), sequences);\n  }\n  private Map<String, TaskReport> getTaskCompletionReports(@Nullable String errorMsg)\n  {\n    return TaskReport.buildTaskReports(\n        new IngestionStatsAndErrorsTaskReport(\n            task.getId(),\n            new IngestionStatsAndErrorsTaskReportData(\n                ingestionState,\n                getTaskCompletionUnparseableEvents(),\n                getTaskCompletionRowStats(),\n                errorMsg\n            )\n        )\n    );\n  }\n  private Map<String, Object> getTaskCompletionUnparseableEvents()\n  {\n    Map<String, Object> unparseableEventsMap = new HashMap<>();\n    List<String> buildSegmentsParseExceptionMessages = IndexTaskUtils.getMessagesFromSavedParseExceptions(\n        savedParseExceptions\n    );\n    if (buildSegmentsParseExceptionMessages != null) {\n      unparseableEventsMap.put(RowIngestionMeters.BUILD_SEGMENTS, buildSegmentsParseExceptionMessages);\n    }\n    return unparseableEventsMap;\n  }\n  private Map<String, Object> getTaskCompletionRowStats()\n  {\n    Map<String, Object> metrics = new HashMap<>();\n    metrics.put(\n        RowIngestionMeters.BUILD_SEGMENTS,\n        rowIngestionMeters.getTotals()\n    );\n    return metrics;\n  }\n  private void maybePersistAndPublishSequences(Supplier<Committer> committerSupplier)\n      throws InterruptedException\n  {\n    for (SequenceMetadata<PartitionIdType, SequenceOffsetType> sequenceMetadata : sequences) {\n      sequenceMetadata.updateAssignments(currOffsets, this::isMoreToReadBeforeReadingRecord);\n      if (!sequenceMetadata.isOpen() && !publishingSequences.contains(sequenceMetadata.getSequenceName())) {\n        publishingSequences.add(sequenceMetadata.getSequenceName());\n        try {\n          Object result = driver.persist(committerSupplier.get());\n          log.info(\n              \"Persist completed with results: [%s], adding sequence [%s] to publish queue\",\n              result,\n              sequenceMetadata\n          );\n          publishAndRegisterHandoff(sequenceMetadata);\n        }\n        catch (InterruptedException e) {\n          log.warn(\"Interrupted while persisting sequence [%s]\", sequenceMetadata);\n          throw e;\n        }\n      }\n    }\n  }\n  private Set<StreamPartition<PartitionIdType>> assignPartitions(\n      RecordSupplier<PartitionIdType, SequenceOffsetType> recordSupplier\n  )\n  {\n    final Set<StreamPartition<PartitionIdType>> assignment = new HashSet<>();\n    for (Map.Entry<PartitionIdType, SequenceOffsetType> entry : currOffsets.entrySet()) {\n      final PartitionIdType partition = entry.getKey();\n      final SequenceOffsetType currOffset = entry.getValue();\n      final SequenceOffsetType endOffset = endOffsets.get(partition);\n      if (!isRecordAlreadyRead(partition, endOffset) && isMoreToReadBeforeReadingRecord(currOffset, endOffset)) {\n        log.info(\n            \"Adding partition[%s], start[%s] -> end[%s] to assignment.\",\n            partition,\n            currOffset,\n            endOffset\n        );\n        assignment.add(StreamPartition.of(stream, partition));\n      } else {\n        log.info(\"Finished reading partition[%s].\", partition);\n      }\n    }\n    recordSupplier.assign(assignment);\n    return assignment;\n  }\n  private void addSequence(final SequenceMetadata<PartitionIdType, SequenceOffsetType> sequenceMetadata)\n  {\n    // Sanity check that the start of the new sequence matches up with the end of the prior sequence.\n    for (Map.Entry<PartitionIdType, SequenceOffsetType> entry : sequenceMetadata.getStartOffsets().entrySet()) {\n      final PartitionIdType partition = entry.getKey();\n      final SequenceOffsetType startOffset = entry.getValue();\n      if (!sequences.isEmpty()) {\n        final SequenceOffsetType priorOffset = sequences.get(sequences.size() - 1).endOffsets.get(partition);\n        if (!startOffset.equals(priorOffset)) {\n          throw new ISE(\n              \"New sequence startOffset[%s] does not equal expected prior offset[%s]\",\n              startOffset,\n              priorOffset\n          );\n        }\n      }\n    }\n    // Actually do the add.\n    sequences.add(sequenceMetadata);\n  }\n  /**\n   * Returns true if the given record has already been read, based on lastReadOffsets.\n   */\n  private boolean isRecordAlreadyRead(\n      final PartitionIdType recordPartition,\n      final SequenceOffsetType recordSequenceNumber\n  )\n  {\n    final SequenceOffsetType lastReadOffset = lastReadOffsets.get(recordPartition);\n    if (lastReadOffset == null) {\n      return false;\n    } else {\n      return createSequenceNumber(recordSequenceNumber).compareTo(createSequenceNumber(lastReadOffset)) <= 0;\n    }\n  }\n  /**\n   * Returns true if, given that we want to start reading from recordSequenceNumber and end at endSequenceNumber, there\n   * is more left to read. Used in pre-read checks to determine if there is anything left to read.\n   */\n<fim_suffix>  private boolean isMoreToReadBeforeReadingRecord(\n      final SequenceOffsetType recordSequenceNumber,\n      final SequenceOffsetType endSequenceNumber\n  )\n  {\n    final int compareToEnd = createSequenceNumber(recordSequenceNumber)\n        .compareTo(createSequenceNumber(endSequenceNumber));\n    return isEndOffsetExclusive() ? compareToEnd < 0 : compareToEnd <= 0;\n  }<fim_middle>// function below has no smell\n"}