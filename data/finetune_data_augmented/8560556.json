{"text": "<fim_prefix>/**\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.apache.hadoop.hbase.backup.impl;\nimport static org.apache.hadoop.hbase.backup.BackupRestoreConstants.JOB_NAME_CONF_KEY;\nimport java.io.IOException;\nimport java.net.URI;\nimport java.net.URISyntaxException;\nimport java.util.ArrayList;\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Set;\nimport java.util.TreeMap;\nimport org.apache.commons.lang3.StringUtils;\nimport org.apache.hadoop.fs.FileSystem;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.hbase.TableName;\nimport org.apache.hadoop.hbase.backup.BackupCopyJob;\nimport org.apache.hadoop.hbase.backup.BackupInfo.BackupPhase;\nimport org.apache.hadoop.hbase.backup.BackupRequest;\nimport org.apache.hadoop.hbase.backup.BackupRestoreFactory;\nimport org.apache.hadoop.hbase.backup.BackupType;\nimport org.apache.hadoop.hbase.backup.mapreduce.MapReduceBackupCopyJob;\nimport org.apache.hadoop.hbase.backup.util.BackupUtils;\nimport org.apache.hadoop.hbase.client.Admin;\nimport org.apache.hadoop.hbase.client.Connection;\nimport org.apache.hadoop.hbase.mapreduce.WALPlayer;\nimport org.apache.hadoop.hbase.util.Bytes;\nimport org.apache.hadoop.hbase.util.FSUtils;\nimport org.apache.hadoop.hbase.util.HFileArchiveUtil;\nimport org.apache.hadoop.hbase.util.Pair;\nimport org.apache.hadoop.hbase.wal.AbstractFSWALProvider;\nimport org.apache.hadoop.util.Tool;\nimport org.apache.yetus.audience.InterfaceAudience;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n/**\n * Incremental backup implementation.\n * See the {@link #execute() execute} method.\n *\n */\n@InterfaceAudience.Private\npublic class IncrementalTableBackupClient extends TableBackupClient {\n  private static final Logger LOG = LoggerFactory.getLogger(IncrementalTableBackupClient.class);\n  protected IncrementalTableBackupClient() {\n  }\n  public IncrementalTableBackupClient(final Connection conn, final String backupId,\n      BackupRequest request) throws IOException {\n    super(conn, backupId, request);\n  }\n  protected List<String> filterMissingFiles(List<String> incrBackupFileList) throws IOException {\n    List<String> list = new ArrayList<>();\n    for (String file : incrBackupFileList) {\n      Path p = new Path(file);\n      if (fs.exists(p) || isActiveWalPath(p)) {\n        list.add(file);\n      } else {\n        LOG.warn(\"Can't find file: \" + file);\n      }\n    }\n    return list;\n  }\n  /**\n   * Check if a given path is belongs to active WAL directory\n   * @param p path\n   * @return true, if yes\n   */\n  protected boolean isActiveWalPath(Path p) {\n    return !AbstractFSWALProvider.isArchivedLogFile(p);\n  }\n  protected static int getIndex(TableName tbl, List<TableName> sTableList) {\n    if (sTableList == null) {\n      return 0;\n    }\n    for (int i = 0; i < sTableList.size(); i++) {\n      if (tbl.equals(sTableList.get(i))) {\n        return i;\n      }\n    }\n    return -1;\n  }\n  /*\n   * Reads bulk load records from backup table, iterates through the records and forms the paths\n   * for bulk loaded hfiles. Copies the bulk loaded hfiles to backup destination\n   * @param sTableList list of tables to be backed up\n   * @return map of table to List of files\n   */\n<fim_suffix>  @SuppressWarnings(\"unchecked\")\n  protected Map<byte[], List<Path>>[] handleBulkLoad(List<TableName> sTableList)\n          throws IOException {\n    Map<byte[], List<Path>>[] mapForSrc = new Map[sTableList.size()];\n    List<String> activeFiles = new ArrayList<>();\n    List<String> archiveFiles = new ArrayList<>();\n    Pair<Map<TableName, Map<String, Map<String, List<Pair<String, Boolean>>>>>, List<byte[]>> pair =\n            backupManager.readBulkloadRows(sTableList);\n    Map<TableName, Map<String, Map<String, List<Pair<String, Boolean>>>>> map = pair.getFirst();\n    FileSystem tgtFs;\n    try {\n      tgtFs = FileSystem.get(new URI(backupInfo.getBackupRootDir()), conf);\n    } catch (URISyntaxException use) {\n      throw new IOException(\"Unable to get FileSystem\", use);\n    }\n    Path rootdir = FSUtils.getRootDir(conf);\n    Path tgtRoot = new Path(new Path(backupInfo.getBackupRootDir()), backupId);\n    for (Map.Entry<TableName, Map<String, Map<String, List<Pair<String, Boolean>>>>> tblEntry :\n      map.entrySet()) {\n      TableName srcTable = tblEntry.getKey();\n      int srcIdx = getIndex(srcTable, sTableList);\n      if (srcIdx < 0) {\n        LOG.warn(\"Couldn't find \" + srcTable + \" in source table List\");\n        continue;\n      }\n      if (mapForSrc[srcIdx] == null) {\n        mapForSrc[srcIdx] = new TreeMap<>(Bytes.BYTES_COMPARATOR);\n      }\n      Path tblDir = FSUtils.getTableDir(rootdir, srcTable);\n      Path tgtTable = new Path(new Path(tgtRoot, srcTable.getNamespaceAsString()),\n          srcTable.getQualifierAsString());\n      for (Map.Entry<String,Map<String,List<Pair<String, Boolean>>>> regionEntry :\n        tblEntry.getValue().entrySet()){\n        String regionName = regionEntry.getKey();\n        Path regionDir = new Path(tblDir, regionName);\n        // map from family to List of hfiles\n        for (Map.Entry<String,List<Pair<String, Boolean>>> famEntry :\n          regionEntry.getValue().entrySet()) {\n          String fam = famEntry.getKey();\n          Path famDir = new Path(regionDir, fam);\n          List<Path> files;\n          if (!mapForSrc[srcIdx].containsKey(Bytes.toBytes(fam))) {\n            files = new ArrayList<>();\n            mapForSrc[srcIdx].put(Bytes.toBytes(fam), files);\n          } else {\n            files = mapForSrc[srcIdx].get(Bytes.toBytes(fam));\n          }\n          Path archiveDir = HFileArchiveUtil.getStoreArchivePath(conf, srcTable, regionName, fam);\n          String tblName = srcTable.getQualifierAsString();\n          Path tgtFam = new Path(new Path(tgtTable, regionName), fam);\n          if (!tgtFs.mkdirs(tgtFam)) {\n            throw new IOException(\"couldn't create \" + tgtFam);\n          }\n          for (Pair<String, Boolean> fileWithState : famEntry.getValue()) {\n            String file = fileWithState.getFirst();\n            int idx = file.lastIndexOf(\"/\");\n            String filename = file;\n            if (idx > 0) {\n              filename = file.substring(idx+1);\n            }\n            Path p = new Path(famDir, filename);\n            Path tgt = new Path(tgtFam, filename);\n            Path archive = new Path(archiveDir, filename);\n            if (fs.exists(p)) {\n              if (LOG.isTraceEnabled()) {\n                LOG.trace(\"found bulk hfile \" + file + \" in \" + famDir + \" for \" + tblName);\n              }\n              if (LOG.isTraceEnabled()) {\n                LOG.trace(\"copying \" + p + \" to \" + tgt);\n              }\n              activeFiles.add(p.toString());\n            } else if (fs.exists(archive)){\n              LOG.debug(\"copying archive \" + archive + \" to \" + tgt);\n              archiveFiles.add(archive.toString());\n            }\n            files.add(tgt);\n          }\n        }\n      }\n    }\n    copyBulkLoadedFiles(activeFiles, archiveFiles);\n    backupManager.deleteBulkLoadedRows(pair.getSecond());\n    return mapForSrc;\n  }<fim_middle>// function below is long method\n"}