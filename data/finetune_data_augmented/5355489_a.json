{"text": "<fim_prefix>import org.apache.tika.exception.TikaException;\nimport org.apache.tika.io.TikaInputStream;\nimport org.apache.tika.metadata.Metadata;\nimport org.apache.tika.parser.AutoDetectParser;\nimport org.xml.sax.SAXException;\nimport org.xml.sax.helpers.DefaultHandler;\n@InputRequirement(Requirement.INPUT_REQUIRED)\n@Tags({\"media\", \"file\", \"format\", \"metadata\", \"audio\", \"video\", \"image\", \"document\", \"pdf\"})\n@CapabilityDescription(\"Extract the content metadata from flowfiles containing audio, video, image, and other file \"\n        + \"types.  This processor relies on the Apache Tika project for file format detection and parsing.  It \"\n        + \"extracts a long list of metadata types for media files including audio, video, and print media \"\n        + \"formats.\"\n        + \"NOTE: the attribute names and content extracted may vary across upgrades because parsing is performed by \"\n        + \"the external Tika tools which in turn depend on other projects for metadata extraction.  For the more \"\n        + \"details and the list of supported file types, visit the library's website at http://tika.apache.org/.\")\n@WritesAttributes({@WritesAttribute(attribute = \"<Metadata Key Prefix><attribute>\", description = \"The extracted content metadata \"\n        + \"will be inserted with the attribute name \\\"<Metadata Key Prefix><attribute>\\\", or \\\"<attribute>\\\" if \"\n        + \"\\\"Metadata Key Prefix\\\" is not provided.\")})\n@SupportsBatching\npublic class ExtractMediaMetadata extends AbstractProcessor {\n    static final PropertyDescriptor MAX_NUMBER_OF_ATTRIBUTES = new PropertyDescriptor.Builder()\n            .name(\"Max Number of Attributes\")\n            .description(\"Specify the max number of attributes to add to the flowfile. There is no guarantee in what order\"\n                    + \" the tags will be processed. By default it will process all of them.\")\n            .required(false)\n            .defaultValue(\"100\")\n            .addValidator(StandardValidators.NON_NEGATIVE_INTEGER_VALIDATOR)\n            .build();\n    private static final PropertyDescriptor MAX_ATTRIBUTE_LENGTH = new PropertyDescriptor.Builder()\n            .name(\"Max Attribute Length\")\n            .description(\"Specifies the maximum length of a single attribute value.  When a metadata item has multiple\"\n                    + \" values, they will be merged until this length is reached and then \\\", ...\\\" will be added as\"\n                    + \" an indicator that additional values where dropped.  If a single value is longer than this, it\"\n                    + \" will be truncated and \\\"(truncated)\\\" appended to indicate that truncation occurred.\")\n            .required(true)\n            .defaultValue(\"100\")\n            .addValidator(StandardValidators.NON_NEGATIVE_INTEGER_VALIDATOR)\n            .build();\n    static final PropertyDescriptor METADATA_KEY_FILTER = new PropertyDescriptor.Builder()\n            .name(\"Metadata Key Filter\")\n            .description(\"A regular expression identifying which metadata keys received from the parser should be\"\n                    + \" added to the flowfile attributes.  If left blank, all metadata keys parsed will be added to the\"\n                    + \" flowfile attributes.\")\n            .required(false)\n            .addValidator(StandardValidators.REGULAR_EXPRESSION_VALIDATOR)\n            .build();\n    static final PropertyDescriptor METADATA_KEY_PREFIX = new PropertyDescriptor.Builder()\n            .name(\"Metadata Key Prefix\")\n            .description(\"Text to be prefixed to metadata keys as the are added to the flowfile attributes.  It is\"\n                    + \" recommended to end with with a separator character like '.' or '-', this is not automatically \"\n                    + \" added by the processor.\")\n            .required(false)\n            .addValidator(StandardValidators.ATTRIBUTE_KEY_VALIDATOR)\n            .expressionLanguageSupported(ExpressionLanguageScope.FLOWFILE_ATTRIBUTES)\n            .build();\n    static final Relationship SUCCESS = new Relationship.Builder()\n            .name(\"success\")\n            .description(\"Any FlowFile that successfully has media metadata extracted will be routed to success\")\n            .build();\n    static final Relationship FAILURE = new Relationship.Builder()\n            .name(\"failure\")\n            .description(\"Any FlowFile that fails to have media metadata extracted will be routed to failure\")\n            .build();\n    private Set<Relationship> relationships;\n    private List<PropertyDescriptor> properties;\n    private final AtomicReference<Pattern> metadataKeyFilterRef = new AtomicReference<>();\n    private volatile AutoDetectParser autoDetectParser;\n    @Override\n    protected void init(final ProcessorInitializationContext context) {\n        final List<PropertyDescriptor> properties = new ArrayList<>();\n        properties.add(MAX_NUMBER_OF_ATTRIBUTES);\n        properties.add(MAX_ATTRIBUTE_LENGTH);\n        properties.add(METADATA_KEY_FILTER);\n        properties.add(METADATA_KEY_PREFIX);\n        this.properties = Collections.unmodifiableList(properties);\n        final Set<Relationship> relationships = new HashSet<>();\n        relationships.add(SUCCESS);\n        relationships.add(FAILURE);\n        this.relationships = Collections.unmodifiableSet(relationships);\n    }\n    @Override\n    public Set<Relationship> getRelationships() {\n        return this.relationships;\n    }\n    @Override\n    protected List<PropertyDescriptor> getSupportedPropertyDescriptors() {\n        return this.properties;\n    }\n    @SuppressWarnings(\"unused\")\n    @OnScheduled\n    public void onScheduled(ProcessContext context) {\n        String metadataKeyFilterInput = context.getProperty(METADATA_KEY_FILTER).getValue();\n        if (metadataKeyFilterInput != null && metadataKeyFilterInput.length() > 0) {\n            metadataKeyFilterRef.set(Pattern.compile(metadataKeyFilterInput));\n        } else {\n            metadataKeyFilterRef.set(null);\n        }\n        autoDetectParser = new AutoDetectParser();\n    }\n    @Override\n    public void onTrigger(final ProcessContext context, final ProcessSession session) throws ProcessException {\n        FlowFile flowFile = session.get();\n        if (flowFile == null) {\n            return;\n        }\n        final ComponentLog logger = this.getLogger();\n        final AtomicReference<Map<String, String>> value = new AtomicReference<>(null);\n        final Integer maxAttribCount = context.getProperty(MAX_NUMBER_OF_ATTRIBUTES).asInteger();\n        final Integer maxAttribLength = context.getProperty(MAX_ATTRIBUTE_LENGTH).asInteger();\n        final String prefix = context.getProperty(METADATA_KEY_PREFIX).evaluateAttributeExpressions(flowFile).getValue();\n        try {\n            session.read(flowFile, new InputStreamCallback() {\n                @Override\n                public void process(InputStream in) throws IOException {\n                    try {\n                        Map<String, String> results = tika_parse(in, prefix, maxAttribCount, maxAttribLength);\n                        value.set(results);\n                    } catch (SAXException | TikaException e) {\n                        throw new IOException(e);\n                    }\n                }\n            });\n            // Write the results to attributes\n            Map<String, String> results = value.get();\n            if (results != null && !results.isEmpty()) {\n                flowFile = session.putAllAttributes(flowFile, results);\n            }\n            session.transfer(flowFile, SUCCESS);\n            session.getProvenanceReporter().modifyAttributes(flowFile, \"media attributes extracted\");\n        } catch (ProcessException e) {\n            logger.error(\"Failed to extract media metadata from {} due to {}\", new Object[]{flowFile, e});\n            flowFile = session.penalize(flowFile);\n            session.transfer(flowFile, FAILURE);\n        }\n    }\n<fim_suffix>    private Map<String, String> tika_parse(InputStream sourceStream, String prefix, Integer maxAttribs,\n                                           Integer maxAttribLen) throws IOException, TikaException, SAXException {\n        final Metadata metadata = new Metadata();\n        final TikaInputStream tikaInputStream = TikaInputStream.get(sourceStream);\n        try {\n            autoDetectParser.parse(tikaInputStream, new DefaultHandler(), metadata);\n        } finally {\n            tikaInputStream.close();\n        }\n        final Map<String, String> results = new HashMap<>();\n        final Pattern metadataKeyFilter = metadataKeyFilterRef.get();\n        final StringBuilder dataBuilder = new StringBuilder();\n        for (final String key : metadata.names()) {\n            if (metadataKeyFilter != null && !metadataKeyFilter.matcher(key).matches()) {\n                continue;\n            }\n            dataBuilder.setLength(0);\n            if (metadata.isMultiValued(key)) {\n                for (String val : metadata.getValues(key)) {\n                    if (dataBuilder.length() > 1) {\n                        dataBuilder.append(\", \");\n                    }\n                    if (dataBuilder.length() + val.length() < maxAttribLen) {\n                        dataBuilder.append(val);\n                    } else {\n                        dataBuilder.append(\"...\");\n                        break;\n                    }\n                }\n            } else {\n                dataBuilder.append(metadata.get(key));\n            }\n            if (prefix == null) {\n                results.put(key, dataBuilder.toString().trim());\n            } else {\n                results.put(prefix + key, dataBuilder.toString().trim());\n            }\n            // cutoff at max if provided\n            if (maxAttribs != null && results.size() >= maxAttribs) {\n                break;\n            }\n        }\n        return results;\n    }<fim_middle>// function below is feature envy and long method\n"}