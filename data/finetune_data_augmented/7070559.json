{"text": "<fim_prefix>/*\n * Copyright 2019 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\").\n See License in the project root for license information.\n */\npackage com.linkedin.kafka.clients.producer;\nimport com.linkedin.kafka.clients.common.ClusterDescriptor;\nimport com.linkedin.kafka.clients.common.ClusterGroupDescriptor;\nimport com.linkedin.kafka.clients.metadataservice.MetadataServiceClient;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Properties;\nimport java.util.Set;\nimport java.util.UUID;\nimport java.util.concurrent.ConcurrentHashMap;\nimport java.util.concurrent.CountDownLatch;\nimport java.util.concurrent.Future;\nimport java.util.concurrent.TimeUnit;\nimport org.apache.kafka.clients.producer.Callback;\nimport org.apache.kafka.clients.producer.ProducerConfig;\nimport org.apache.kafka.clients.producer.ProducerRecord;\nimport org.apache.kafka.clients.producer.RecordMetadata;\nimport org.apache.kafka.clients.consumer.OffsetAndMetadata;\nimport org.apache.kafka.common.Metric;\nimport org.apache.kafka.common.MetricName;\nimport org.apache.kafka.common.PartitionInfo;\nimport org.apache.kafka.common.TopicPartition;\nimport org.apache.kafka.common.KafkaException;\nimport org.apache.kafka.common.errors.ProducerFencedException;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n/**\n * This is a producer implementation that works with a federated Kafka cluster, which consists of one or more physical\n * Kafka clusters.\n */\npublic class LiKafkaFederatedProducerImpl<K, V> implements LiKafkaProducer<K, V> {\n  private static final Logger LOG = LoggerFactory.getLogger(LiKafkaFederatedProducerImpl.class);\n  // The cluster group this client is talking to\n  private ClusterGroupDescriptor _clusterGroup;\n  // The client for the metadata service which serves cluster and topic metadata\n  private MetadataServiceClient _mdsClient;\n  // Timeout in milliseconds for metadata service requests.\n  private int _mdsRequestTimeoutMs;\n  // The id of this client assigned by the metadata service\n  private UUID _clientId;\n  // Per cluster producers\n  private Map<ClusterDescriptor, LiKafkaProducer<K, V>> _producers;\n  // Producer builder for creating per-cluster LiKafkaProducer\n  private LiKafkaProducerBuilder<K, V> _producerBuilder;\n  // Producer configs common to all clusters\n  private LiKafkaProducerConfig _commonProducerConfigs;\n  public LiKafkaFederatedProducerImpl(Properties props) {\n    this(new LiKafkaProducerConfig(props), null, null);\n  }\n  public LiKafkaFederatedProducerImpl(Properties props, MetadataServiceClient mdsClient,\n      LiKafkaProducerBuilder<K, V> producerBuilder) {\n    this(new LiKafkaProducerConfig(props), mdsClient, producerBuilder);\n  }\n  public LiKafkaFederatedProducerImpl(Map<String, ?> configs) {\n    this(new LiKafkaProducerConfig(configs), null, null);\n  }\n  public LiKafkaFederatedProducerImpl(Map<String, ?> configs, MetadataServiceClient mdsClient,\n      LiKafkaProducerBuilder<K, V> producerBuilder) {\n    this(new LiKafkaProducerConfig(configs), mdsClient, producerBuilder);\n  }\n  @SuppressWarnings(\"unchecked\")\n  private LiKafkaFederatedProducerImpl(LiKafkaProducerConfig configs, MetadataServiceClient mdsClient,\n      LiKafkaProducerBuilder<K, V> producerBuilder) {\n    _commonProducerConfigs = configs;\n    _clusterGroup = new ClusterGroupDescriptor(configs.getString(LiKafkaProducerConfig.CLUSTER_ENVIRONMENT_CONFIG),\n        configs.getString(LiKafkaProducerConfig.CLUSTER_GROUP_CONFIG));\n    // Each per-cluster producer and auditor will be intantiated by the passed-in producer builder when the client\n    // begins to produce to that cluster. If a null builder is passed, create a default one, which builds LiKafkaProducer.\n    _producers = new ConcurrentHashMap<ClusterDescriptor, LiKafkaProducer<K, V>>();\n    _producerBuilder = producerBuilder != null ? producerBuilder : new LiKafkaProducerBuilder<K, V>();\n    _mdsRequestTimeoutMs = configs.getInt(LiKafkaProducerConfig.METADATA_SERVICE_REQUEST_TIMEOUT_MS_CONFIG);\n    try {\n      // Instantiate metadata service client if necessary.\n      _mdsClient = mdsClient != null ? mdsClient :\n          configs.getConfiguredInstance(LiKafkaProducerConfig.METADATA_SERVICE_CLIENT_CLASS_CONFIG, MetadataServiceClient.class);\n      // Register this federated client with the metadata service. The metadata service will assign a UUID to this\n      // client, which will be used for later interaction between the metadata service and the client.\n      //\n      // Registration may also return further information such as the metadata server version and any protocol settings.\n      // We assume that such information will be kept and used by the metadata service client itself.\n      _clientId = _mdsClient.registerFederatedClient(_clusterGroup, configs.originals(), _mdsRequestTimeoutMs);\n    } catch (Exception e) {\n      try {\n        if (_mdsClient != null) {\n          _mdsClient.close(_mdsRequestTimeoutMs);\n        }\n      } catch (Exception e2) {\n        e.addSuppressed(e2);\n      }\n      throw e;\n    }\n  }\n<fim_suffix>  @Override\n  public Future<RecordMetadata> send(ProducerRecord<K, V> producerRecord) {\n    return send(producerRecord, null);\n  }\n  @Override\n  public Future<RecordMetadata> send(ProducerRecord<K, V> producerRecord, Callback callback) {\n    return getOrCreateProducerForTopic(producerRecord.topic()).send(producerRecord, callback);\n  }\n  @Override\n  public void flush() {\n    flush(Integer.MAX_VALUE, TimeUnit.MILLISECONDS);\n  }\n  @Override\n  public void flush(long timeout, TimeUnit timeUnit) {\n    if (_producers.isEmpty()) {\n      LOG.warn(\"No producers to flush for cluster group {}\", _clusterGroup);\n      return;\n    }\n    LOG.info(\"Flushing LiKafkaProducer for cluster group {} in {} {}...\", _clusterGroup, timeout, timeUnit);\n    long startTimeMs = System.currentTimeMillis();\n    long deadlineTimeMs = startTimeMs + timeUnit.toMillis(timeout);\n    CountDownLatch countDownLatch = new CountDownLatch(_producers.entrySet().size());\n    for (Map.Entry<ClusterDescriptor, LiKafkaProducer<K, V>> entry : _producers.entrySet()) {\n      new Thread(() -> {\n          try {\n            entry.getValue().flush(deadlineTimeMs - System.currentTimeMillis(), TimeUnit.MILLISECONDS);\n          } finally {\n            countDownLatch.countDown();\n          }\n        }).start();\n    }\n    try {\n      countDownLatch.await();\n    } catch (InterruptedException e) {\n      throw new KafkaException(\"Fail to flush all producers for cluster group \" + _clusterGroup, e);\n    }\n    LOG.info(\"LiKafkaProducer flush for cluster group {} complete in {} milliseconds\", _clusterGroup,\n        (System.currentTimeMillis() - startTimeMs));\n  }\n  @Override\n  public List<PartitionInfo> partitionsFor(String topic) {\n    return getOrCreateProducerForTopic(topic).partitionsFor(topic);\n  }\n  @Override\n  public Map<String, List<PartitionInfo>> partitionsFor(Set<String> topics) {\n    // TODO: come back here when upstream API settles\n    throw new UnsupportedOperationException(\"Not implemented yet\");\n  }\n  @Override\n  public Map<MetricName, ? extends Metric> metrics() {\n    throw new UnsupportedOperationException(\"Not implemented yet\");\n  }\n  @Override\n  public void close() {\n    close(Integer.MAX_VALUE, TimeUnit.MILLISECONDS);\n  }\n  @Override\n  public void close(long timeout, TimeUnit timeUnit) {\n    if (_producers.isEmpty()) {\n      LOG.warn(\"No producers to close for cluster group {}\", _clusterGroup);\n      return;\n    }\n    LOG.info(\"Closing LiKafkaProducer for cluster group {} in {} {}...\", _clusterGroup, timeout, timeUnit);\n    long startTimeMs = System.currentTimeMillis();\n    long deadlineTimeMs = startTimeMs + timeUnit.toMillis(timeout);\n    CountDownLatch countDownLatch = new CountDownLatch(_producers.entrySet().size());\n    for (Map.Entry<ClusterDescriptor, LiKafkaProducer<K, V>> entry : _producers.entrySet()) {\n      new Thread(() -> {\n          try {\n            entry.getValue().close(deadlineTimeMs - System.currentTimeMillis(), TimeUnit.MILLISECONDS);\n          } finally {\n            countDownLatch.countDown();\n          }\n        }).start();\n    }\n    try {\n      countDownLatch.await();\n    } catch (InterruptedException e) {\n      throw new KafkaException(\"Fail to close all producers for cluster group \" + _clusterGroup, e);\n    }\n    LOG.info(\"LiKafkaProducer close for cluster group {} complete in {} milliseconds\", _clusterGroup,\n        (System.currentTimeMillis() - startTimeMs));\n  }\n  // Transactions are not supported in federated clients.\n  @Override\n  public void initTransactions() {\n    throw new UnsupportedOperationException(\"Not supported\");\n  }\n  @Override\n  public void beginTransaction() throws ProducerFencedException {\n    throw new UnsupportedOperationException(\"Not supported\");\n  }\n  @Override<fim_middle>// function below has no smell\n"}