{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b508155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bắt đầu tải và xử lý dữ liệu...\n",
      "Thông tin bộ dữ liệu:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 4025\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 224\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 224\n",
      "    })\n",
      "})\n",
      "Tải tokenizer và model 'infosys/NT-Java-1.1B' (không quantization)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "603ebb99b50a413ca13f703a14f4d37f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4025 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a1bb39bed474d13b31cadf5019d6431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/224 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0b9b9d072fa4c0c9bbdd4f61d2cd52f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/224 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize dữ liệu hoàn tất.\n",
      "Các lớp được tìm thấy (10 lớp): ['// class below has no smell\\n', '// class below is blob\\n', '// class below is blob and data class\\n', '// class below is data class\\n', '// class below is data class and blob\\n', '// function below has no smell\\n', '// function below is feature envy\\n', '// function below is feature envy and long method\\n', '// function below is long method\\n', '// function below is long method and feature envy\\n']\n",
      "Trọng số class được tính toán: {'// class below has no smell\\n': 0.2810754189944134, '// class below is blob\\n': 1.8896713615023475, '// class below is blob and data class\\n': 9.147727272727273, '// class below is data class\\n': 1.7055084745762712, '// class below is data class and blob\\n': 9.147727272727273, '// function below has no smell\\n': 0.23079128440366972, '// function below is feature envy\\n': 10.320512820512821, '// function below is feature envy and long method\\n': 7.453703703703703, '// function below is long method\\n': 2.4393939393939394, '// function below is long method and feature envy\\n': 7.453703703703703}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1028406/2125370872.py:164: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cấu hình LoRA hoàn tất. Các tham số có thể huấn luyện:\n",
      "trainable params: 2,777,088 || all params: 1,139,984,384 || trainable%: 0.2436\n",
      "\n",
      "Bắt đầu quá trình finetune...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3696' max='8400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3696/8400 4:19:32 < 5:30:29, 0.24 it/s, Epoch 22/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quá trình finetune đã hoàn tất!\n",
      "Model tốt nhất đã được lưu tại: /code-smell-detection/code-smell-finetuned-model-masked-loss/final_best_model\n"
     ]
    }
   ],
   "source": [
    "# QUAN TRỌNG: Nếu gặp lỗi C compiler, hãy chạy lệnh sau trong môi trường của bạn và khởi động lại kernel:\n",
    "# !apt-get update && apt-get install -y build-essential\n",
    "\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CẤU HÌNH & HYPERPARAMETERS\n",
    "# ==============================================================================\n",
    "class Config:\n",
    "    MODEL_ID = \"infosys/NT-Java-1.1B\"\n",
    "    DATA_DIR = \"/code-smell-detection/data/finetune_data_augmented\"\n",
    "    OUTPUT_DIR = \"/code-smell-detection/code-smell-finetuned-model-masked-loss\"\n",
    "    RANDOM_SEED = 42\n",
    "    TEST_SIZE = 0.10\n",
    "    DEV_SIZE_FROM_TEST = 0.50\n",
    "    LORA_R = 4\n",
    "    LORA_ALPHA = 4\n",
    "    LORA_DROPOUT = 0.1\n",
    "    LORA_TARGET_MODULES = [\"c_attn\", \"c_proj\", \"c_fc\"]\n",
    "    NUM_TRAIN_EPOCHS = 50 \n",
    "    EARLY_STOPPING_PATIENCE = 5\n",
    "    PER_DEVICE_TRAIN_BATCH_SIZE = 3\n",
    "    PER_DEVICE_EVAL_BATCH_SIZE = 3\n",
    "    GRADIENT_ACCUMULATION_STEPS = 8\n",
    "    LEARNING_RATE = 1e-3\n",
    "    WEIGHT_DECAY = 0.1\n",
    "    WARMUP_RATIO = 0\n",
    "    FP16 = True\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. TẢI VÀ TIỀN XỬ LÝ DỮ LIỆU\n",
    "# ==============================================================================\n",
    "def load_data_from_json_files(data_dir):\n",
    "    \"\"\"\n",
    "    Tải dữ liệu từ các file .json.\n",
    "    Label được xác định là chuỗi văn bản chính xác nằm sau <fim_middle>.\n",
    "    \"\"\"\n",
    "    json_files = glob.glob(os.path.join(data_dir, \"*.json\"))\n",
    "    data = []\n",
    "    \n",
    "    for file_path in json_files:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = json.load(f)\n",
    "            text = content.get(\"text\", \"\")\n",
    "            \n",
    "            try:\n",
    "                # Trích xuất phần label thô\n",
    "                label_part_raw = text.split(\"<fim_middle>\")[1]\n",
    "                # Thêm dữ liệu vào danh sách.\n",
    "                data.append({\"text\": text, \"label\": label_part_raw})\n",
    "\n",
    "            except IndexError:\n",
    "                print(f\"Cảnh báo: Định dạng FIM không đúng trong file {file_path}. Bỏ qua.\")\n",
    "                continue\n",
    "                \n",
    "    return data\n",
    "\n",
    "def create_datasets(data, config):\n",
    "    train_data, temp_data = train_test_split(\n",
    "        data, test_size=config.TEST_SIZE, random_state=config.RANDOM_SEED, stratify=[d['label'] for d in data]\n",
    "    )\n",
    "    test_data, dev_data = train_test_split(\n",
    "        temp_data, test_size=config.DEV_SIZE_FROM_TEST, random_state=config.RANDOM_SEED, stratify=[d['label'] for d in temp_data]\n",
    "    )\n",
    "    return DatasetDict({\n",
    "        \"train\": Dataset.from_list(train_data),\n",
    "        \"validation\": Dataset.from_list(dev_data),\n",
    "        \"test\": Dataset.from_list(test_data)\n",
    "    })\n",
    "\n",
    "print(\"Bắt đầu tải và xử lý dữ liệu...\")\n",
    "raw_data = load_data_from_json_files(config.DATA_DIR)\n",
    "if not raw_data:\n",
    "    raise ValueError(\"Không có dữ liệu nào được tải. Vui lòng kiểm tra lại đường dẫn DATA_DIR.\")\n",
    "datasets = create_datasets(raw_data, config)\n",
    "print(\"Thông tin bộ dữ liệu:\")\n",
    "print(datasets)\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. THIẾT LẬP TOKENIZER VÀ MODEL\n",
    "# ==============================================================================\n",
    "print(f\"Tải tokenizer và model '{config.MODEL_ID}' (không quantization)...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.MODEL_ID)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "tokenized_datasets = datasets.map(tokenize_function, batched=True, remove_columns=[\"text\", \"label\"])\n",
    "print(\"Tokenize dữ liệu hoàn tất.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. TÍNH TOÁN TRỌNG SỐ CLASS (CLASS WEIGHTS)\n",
    "# ==============================================================================\n",
    "train_labels = datasets['train']['label']\n",
    "class_names = sorted(list(set(train_labels)))\n",
    "class_weights_array = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.array(class_names),\n",
    "    y=train_labels\n",
    ")\n",
    "class_weights_map = {name: weight for name, weight in zip(class_names, class_weights_array)}\n",
    "class_weights_tensor = torch.tensor(list(class_weights_map.values()), dtype=torch.float32).to(\"cuda\")\n",
    "label2id = {name: i for i, name in enumerate(class_names)}\n",
    "\n",
    "print(f\"Các lớp được tìm thấy ({len(class_names)} lớp): {class_names}\")\n",
    "print(f\"Trọng số class được tính toán: {class_weights_map}\")\n",
    "\n",
    "tokenized_train_with_ids = tokenized_datasets['train'].add_column(\"label_id\", [label2id[l] for l in datasets['train']['label']])\n",
    "tokenized_validation_with_ids = tokenized_datasets['validation'].add_column(\"label_id\", [label2id[l] for l in datasets['validation']['label']])\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. THIẾT LẬP DATA COLLATOR VÀ TRAINER TÙY CHỈNH\n",
    "# ==============================================================================\n",
    "\n",
    "class DataCollatorForFIMTask:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.fim_middle_token_id = tokenizer.convert_tokens_to_ids(\"<fim_middle>\")\n",
    "\n",
    "    def __call__(self, features):\n",
    "        label_ids = [feature.pop(\"label_id\") for feature in features]\n",
    "        batch = self.tokenizer.pad(features, return_tensors=\"pt\", padding=True)\n",
    "        batch[\"label_id\"] = torch.tensor(label_ids, dtype=torch.long)\n",
    "        labels = batch[\"input_ids\"].clone()\n",
    "        \n",
    "        for i in range(labels.shape[0]):\n",
    "            input_ids_list = batch[\"input_ids\"][i].tolist()\n",
    "            try:\n",
    "                middle_token_index = input_ids_list.index(self.fim_middle_token_id)\n",
    "                labels[i, : middle_token_index + 1] = -100\n",
    "            except ValueError:\n",
    "                labels[i, :] = -100\n",
    "        \n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "class WeightedLossTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        label_ids = inputs.pop(\"label_id\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        labels = inputs.get(\"labels\")\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.vocab_size), labels.view(-1))\n",
    "        loss_per_sequence = loss.view(labels.shape[0], -1).sum(dim=1)\n",
    "        num_active_tokens = (labels.view(labels.shape[0], -1) != -100).sum(dim=1)\n",
    "        mean_loss_per_sequence = loss_per_sequence / (num_active_tokens + 1e-8)\n",
    "        weights_for_batch = self.class_weights[label_ids].to(mean_loss_per_sequence.device)\n",
    "        weighted_loss_per_sequence = mean_loss_per_sequence * weights_for_batch\n",
    "        final_loss = weighted_loss_per_sequence.mean()\n",
    "        return (final_loss, outputs) if return_outputs else final_loss\n",
    "\n",
    "data_collator = DataCollatorForFIMTask(tokenizer=tokenizer)\n",
    "\n",
    "# ==============================================================================\n",
    "# 6. THIẾT LẬP VÀ CHẠY HUẤN LUYỆN\n",
    "# ==============================================================================\n",
    "peft_config = LoraConfig(\n",
    "    r=config.LORA_R,\n",
    "    lora_alpha=config.LORA_ALPHA,\n",
    "    lora_dropout=config.LORA_DROPOUT,\n",
    "    target_modules=config.LORA_TARGET_MODULES,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "print(\"Cấu hình LoRA hoàn tất. Các tham số có thể huấn luyện:\")\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=config.OUTPUT_DIR,\n",
    "    num_train_epochs=config.NUM_TRAIN_EPOCHS,\n",
    "    per_device_train_batch_size=config.PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=config.PER_DEVICE_EVAL_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=config.GRADIENT_ACCUMULATION_STEPS,\n",
    "    learning_rate=config.LEARNING_RATE,\n",
    "    weight_decay=config.WEIGHT_DECAY,\n",
    "    warmup_ratio=config.WARMUP_RATIO,\n",
    "    fp16=config.FP16,\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "trainer = WeightedLossTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_with_ids,\n",
    "    eval_dataset=tokenized_validation_with_ids,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=config.EARLY_STOPPING_PATIENCE)],\n",
    "    class_weights=class_weights_tensor \n",
    ")\n",
    "\n",
    "print(\"\\nBắt đầu quá trình finetune...\")\n",
    "trainer.train()\n",
    "print(\"Quá trình finetune đã hoàn tất!\")\n",
    "\n",
    "final_model_path = os.path.join(config.OUTPUT_DIR, \"final_best_model\")\n",
    "trainer.save_model(final_model_path)\n",
    "tokenizer.save_pretrained(final_model_path)\n",
    "print(f\"Model tốt nhất đã được lưu tại: {final_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850e6fc3",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e06c645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bắt đầu tải mô hình và tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mô hình đã được tải thành công lên thiết bị: cuda\n",
      "\n",
      "==================================================\n",
      "Đang chạy: Test Case 1: Code\n",
      "==================================================\n",
      "--- INPUT PROMPT ---\n",
      "<fim_prefix>/*\n",
      "* Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
      "*\n",
      "* Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file\n",
      "* except in compliance with the License. A copy of the License is located at\n",
      "*\n",
      "* http://aws.amazon.com/apache2.0/\n",
      "*\n",
      "* or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS,\n",
      "* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for\n",
      "* the specific language governing permissions and limitations under the License.\n",
      "*/\n",
      "\n",
      "package com.amazon.ask.dispatcher.request.handler.impl;\n",
      "\n",
      "import com.amazon.ask.dispatcher.request.handler.HandlerInput;\n",
      "import com.amazon.ask.dispatcher.request.handler.RequestHandler;\n",
      "import com.amazon.ask.model.interfaces.audioplayer.PlaybackNearlyFinishedRequest;\n",
      "import com.amazon.ask.model.Response;\n",
      "\n",
      "import java.util.Optional;\n",
      "\n",
      "/**\n",
      " * Request handler for PlaybackNearlyFinishedRequest requests.\n",
      " */\n",
      "<fim_suffix>public interface PlaybackNearlyFinishedRequestHandler extends RequestHandler {\n",
      "\n",
      "    /**\n",
      "     * Returns true if the handler can dispatch the current request\n",
      "     *\n",
      "     * @param input input to the request handler\n",
      "     * @param playbackNearlyFinishedRequest PlaybackNearlyFinishedRequest request\n",
      "     * @return true if the handler is capable of handling the current request and/or state\n",
      "     */\n",
      "    boolean canHandle(HandlerInput input, PlaybackNearlyFinishedRequest playbackNearlyFinishedRequest);\n",
      "\n",
      "    /**\n",
      "     * Handles the request.\n",
      "     *\n",
      "     * @param input input to the request handler\n",
      "     * @param playbackNearlyFinishedRequest PlaybackNearlyFinishedRequest request\n",
      "     * @return output from the handler.\n",
      "     */\n",
      "    Optional<Response> handle(HandlerInput input, PlaybackNearlyFinishedRequest playbackNearlyFinishedRequest);\n",
      "\n",
      "    @Override\n",
      "    default boolean canHandle(HandlerInput handlerInput) {\n",
      "        if (handlerInput.getRequest() instanceof PlaybackNearlyFinishedRequest) {\n",
      "            return canHandle(handlerInput, (PlaybackNearlyFinishedRequest)handlerInput.getRequest());\n",
      "        }\n",
      "        return false;\n",
      "    }\n",
      "\n",
      "    @Override\n",
      "    default Optional<Response> handle(HandlerInput handlerInput) {\n",
      "        return handle(handlerInput, (PlaybackNearlyFinishedRequest)handlerInput.getRequest());\n",
      "    }\n",
      "\n",
      "}<fim_middle>\n",
      "\n",
      "--- MODEL OUTPUT ---\n",
      "<fim_prefix>/*\n",
      "* Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
      "*\n",
      "* Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file\n",
      "* except in compliance with the License. A copy of the License is located at\n",
      "*\n",
      "* http://aws.amazon.com/apache2.0/\n",
      "*\n",
      "* or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS,\n",
      "* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for\n",
      "* the specific language governing permissions and limitations under the License.\n",
      "*/\n",
      "\n",
      "package com.amazon.ask.dispatcher.request.handler.impl;\n",
      "\n",
      "import com.amazon.ask.dispatcher.request.handler.HandlerInput;\n",
      "import com.amazon.ask.dispatcher.request.handler.RequestHandler;\n",
      "import com.amazon.ask.model.interfaces.audioplayer.PlaybackNearlyFinishedRequest;\n",
      "import com.amazon.ask.model.Response;\n",
      "\n",
      "import java.util.Optional;\n",
      "\n",
      "/**\n",
      " * Request handler for PlaybackNearlyFinishedRequest requests.\n",
      " */\n",
      "<fim_suffix>public interface PlaybackNearlyFinishedRequestHandler extends RequestHandler {\n",
      "\n",
      "    /**\n",
      "     * Returns true if the handler can dispatch the current request\n",
      "     *\n",
      "     * @param input input to the request handler\n",
      "     * @param playbackNearlyFinishedRequest PlaybackNearlyFinishedRequest request\n",
      "     * @return true if the handler is capable of handling the current request and/or state\n",
      "     */\n",
      "    boolean canHandle(HandlerInput input, PlaybackNearlyFinishedRequest playbackNearlyFinishedRequest);\n",
      "\n",
      "    /**\n",
      "     * Handles the request.\n",
      "     *\n",
      "     * @param input input to the request handler\n",
      "     * @param playbackNearlyFinishedRequest PlaybackNearlyFinishedRequest request\n",
      "     * @return output from the handler.\n",
      "     */\n",
      "    Optional<Response> handle(HandlerInput input, PlaybackNearlyFinishedRequest playbackNearlyFinishedRequest);\n",
      "\n",
      "    @Override\n",
      "    default boolean canHandle(HandlerInput handlerInput) {\n",
      "        if (handlerInput.getRequest() instanceof PlaybackNearlyFinishedRequest) {\n",
      "            return canHandle(handlerInput, (PlaybackNearlyFinishedRequest)handlerInput.getRequest());\n",
      "        }\n",
      "        return false;\n",
      "    }\n",
      "\n",
      "    @Override\n",
      "    default Optional<Response> handle(HandlerInput handlerInput) {\n",
      "        return handle(handlerInput, (PlaybackNearlyFinishedRequest)handlerInput.getRequest());\n",
      "    }\n",
      "\n",
      "}<fim_middle>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "////////////////////////////////////////////////////////////////////////////////////\n",
      "==================================================\n",
      "\n",
      "Hoàn tất kiểm tra inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CẤU HÌNH\n",
    "# ==============================================================================\n",
    "# ID của mô hình gốc trên Hugging Face\n",
    "base_model_id = \"infosys/NT-Java-1.1B\"\n",
    "\n",
    "# Đường dẫn đến adapter LoRA đã được finetune và lưu lại\n",
    "adapter_path = \"/code-smell-detection/code-smell-finetuned-model-masked-loss/final_best_model\"\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. TẢI MODEL VÀ TOKENIZER\n",
    "# ==============================================================================\n",
    "print(\"Bắt đầu tải mô hình và tokenizer...\")\n",
    "\n",
    "# Thiết lập thiết bị (sử dụng GPU nếu có)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Tải mô hình gốc với định dạng bfloat16 để tối ưu bộ nhớ\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Tải tokenizer từ thư mục adapter để đảm bảo tính nhất quán\n",
    "tokenizer = AutoTokenizer.from_pretrained(adapter_path)\n",
    "\n",
    "# Tải và hợp nhất adapter LoRA vào mô hình gốc\n",
    "# Thao tác này sẽ tạo ra mô hình đã được finetune hoàn chỉnh\n",
    "model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "\n",
    "# Chuyển mô hình sang thiết bị và đặt ở chế độ đánh giá (evaluation)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Mô hình đã được tải thành công lên thiết bị: {device}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. CHUẨN BỊ DỮ LIỆU TEST\n",
    "# ==============================================================================\n",
    "\n",
    "# Ví dụ 1: Code \"sạch\", không có smell\n",
    "# Mong đợi: model sẽ sinh ra \"// class below has no smell\"\n",
    "input_text = \"<fim_prefix>public class PalindromeChecker {\\n        public static boolean isPalindrome(String str) {\\n          <fim_suffix>return true;\\n      }\\n<fim_middle>\"\n",
    "\n",
    "code_prompt = \"<fim_prefix>/*\\n* Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\\n*\\n* Licensed under the Apache License, Version 2.0 (the \\\"License\\\"). You may not use this file\\n* except in compliance with the License. A copy of the License is located at\\n*\\n* http://aws.amazon.com/apache2.0/\\n*\\n* or in the \\\"license\\\" file accompanying this file. This file is distributed on an \\\"AS IS\\\" BASIS,\\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for\\n* the specific language governing permissions and limitations under the License.\\n*/\\n\\npackage com.amazon.ask.dispatcher.request.handler.impl;\\n\\nimport com.amazon.ask.dispatcher.request.handler.HandlerInput;\\nimport com.amazon.ask.dispatcher.request.handler.RequestHandler;\\nimport com.amazon.ask.model.interfaces.audioplayer.PlaybackNearlyFinishedRequest;\\nimport com.amazon.ask.model.Response;\\n\\nimport java.util.Optional;\\n\\n/**\\n * Request handler for PlaybackNearlyFinishedRequest requests.\\n */\\n<fim_suffix>public interface PlaybackNearlyFinishedRequestHandler extends RequestHandler {\\n\\n    /**\\n     * Returns true if the handler can dispatch the current request\\n     *\\n     * @param input input to the request handler\\n     * @param playbackNearlyFinishedRequest PlaybackNearlyFinishedRequest request\\n     * @return true if the handler is capable of handling the current request and/or state\\n     */\\n    boolean canHandle(HandlerInput input, PlaybackNearlyFinishedRequest playbackNearlyFinishedRequest);\\n\\n    /**\\n     * Handles the request.\\n     *\\n     * @param input input to the request handler\\n     * @param playbackNearlyFinishedRequest PlaybackNearlyFinishedRequest request\\n     * @return output from the handler.\\n     */\\n    Optional<Response> handle(HandlerInput input, PlaybackNearlyFinishedRequest playbackNearlyFinishedRequest);\\n\\n    @Override\\n    default boolean canHandle(HandlerInput handlerInput) {\\n        if (handlerInput.getRequest() instanceof PlaybackNearlyFinishedRequest) {\\n            return canHandle(handlerInput, (PlaybackNearlyFinishedRequest)handlerInput.getRequest());\\n        }\\n        return false;\\n    }\\n\\n    @Override\\n    default Optional<Response> handle(HandlerInput handlerInput) {\\n        return handle(handlerInput, (PlaybackNearlyFinishedRequest)handlerInput.getRequest());\\n    }\\n\\n}<fim_middle>\"\n",
    "\n",
    "# // class below has no smell\\n\n",
    "test_prompts = {\n",
    "    \"Test Case 1: Code\": code_prompt,\n",
    "}\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. CHẠY INFERENCE\n",
    "# ==============================================================================\n",
    "\n",
    "for test_name, prompt in test_prompts.items():\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"Đang chạy: {test_name}\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"--- INPUT PROMPT ---\")\n",
    "    print(prompt)\n",
    "    \n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Sinh ra output từ model\n",
    "    # Chúng ta chỉ cần sinh ra một đoạn text ngắn (label)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=50, \n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    # Decode và hiển thị kết quả\n",
    "    # Kết quả sẽ bao gồm cả prompt đầu vào, chúng ta sẽ in toàn bộ để xem\n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    \n",
    "    print(\"\\n--- MODEL OUTPUT ---\")\n",
    "    print(result)\n",
    "    print(\"=\"*50)\n",
    "\n",
    "print(\"\\nHoàn tất kiểm tra inference.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
